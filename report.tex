\documentclass{report}

\usepackage{preamble}
\usepackage{prealgebra}
\usepackage{premath}
\usepackage{precomb}

\usepackage{biblatex}
\addbibresource{report.bib}

% Remove "chapter" heading
\usepackage{titlesec}
\titleformat{\chapter}{\normalfont\huge\bf}{\thechapter.}{20pt}{\huge\bf}

\newcommand{\diag}[1]{\operatorname{diag}\( #1 \)}
\newcommand{\chiN}{\chi_{N}}
\newcommand{\chiM}{\chi_{M}}
\newcommand{\chiNs}{\chi_{N^*}}
\newcommand{\chiMs}{\chi_{M^*}}
\newcommand{\diagz}{\diag{\chi_0}}
\newcommand{\diagN}{\diag{\chiN}}
\newcommand{\diagM}{\diag{\chiM}}
\newcommand{\diagNs}{\diag{\chiNs}}
\newcommand{\diagMs}{\diag{\chiMs}}
\newcommand{\diagnu}{\diag{\nu}}
\newcommand{\diagmu}{\diag{\mu}}
\newcommand{\vone}{\mathbf{1}}

\title{Cliques in Association Schemes}
\author{
  Andrew Nagarajah \\
  Supervised by: Prof. Mike Newman
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}

  This report is an exploration of some of the topics within \emph{Delsarte
  theory}, which uses ideas from graph theory, algebra, and optimization to
  address questions in coding theory in particular, and combinatorics more
  broadly.  The centre of study is the \emph{association scheme}, which provides
  a setting in which to view various objects, especially \emph{distance-regular
  graphs}.  This perspective enables the computation of various parameters of
  interest, including the eigenvalues of graphs and upper bounds on codes,
  cliques, and independent sets.

\end{abstract}

\tableofcontents

\chapter{Introduction}
  \section{Coding Theory}
    I'm also not sure if I should include a section like this, but seeing as
    coding theory was the original motivation behind Delsarte's LP bound,
    and it remains (presumably?) a strong motivator for this theory,
    I figured it might be interesting to mention this as an application.

    \begin{itemize}
      \item Basics of Coding Theory
      \item Linear Graphs
      \item Finite Vector Spaces
    \end{itemize}

  \section{Hamming Graphs}

  \section{Distance-Regular Graphs}
    I'm not sure if I should maybe merge this section with $P$-polynomial
    section?

    \begin{itemize}
      \item Definition
      \item Basic parameters
      \item Examples?
    \end{itemize}

\chapter{Association Schemes}
  \section{Association Schemes}
    \begin{itemize}
      \item Definition(s)
      \item Examples?
      \item Basic parameters
    \end{itemize}

    \subsection{The Bose-Mesner Algebra}

    \subsection{Duality}

  \section{$P$-Polynomial Schemes}
    \begin{itemize}
      \item Definitions
      \item ``Equivalence" to DRGs
    \end{itemize}

    If we want to focus on the LP bound and translation schemes, I'm not sure
    that this section is necessary, but it is very interesting and provides an
    important class of examples.

    \subsection{$Q$-Polynomial Schemes}

  \section{Automorphisms}
    This section may be merged with the following section.
    \begin{itemize}
      \item Action of a regular group of automorphisms
      \item Cayley graphs
      \item Eigenspaces from characters
    \end{itemize}

  \section{Translation Schemes}
    \begin{itemize}
      \item Equitable partitions of matrices
      \item Group partitions yielding association schemes
      \item Dual schemes? (Interesting, but not particularly necessary for the
        rest of this report)
    \end{itemize}

  \section{The Eigenvalues of the Hamming Scheme}

\chapter{Delsarte's Linear Programming Bound}
  \section{Linear Programming}
    \begin{itemize}
      \item Basics of Linear Programming -- done
      \item Duality -- done
      \item Algorithms?
    \end{itemize}

    The terminology and results from this section,
    except for the adjective \textit{principal} for
    constraints, follows from \cite{matousek}.

    A \textsc{linear programming problem} (or \textsc{linear program})
    is an optimization problem in which one seeks to maximise or minimize
    a linear function of one or more variables, subject to linear constraints.
    That is, fixing a vector $c$, one tries to maximize or minimize
    the linear combinations of the components of $c$:
    $$
      c_1 x_1 + \cdots + c_n x_n = c^T x
    $$
    for some $x$.
    Note that maximizing $c^T x$ is equivalent to minimizing $(-c)^T x$,
    so that for the theory of linear programming,
    it suffices to consider maximization problems without loss of generality.
    As in other optimization problems,
    the function to be maximized ($c^T x$ in this case)
    is called the \textsc{objective (function)}.

    In most cases, there will be contraints on the inputs to the objective
    function, and for the purposes of linear programming these will also have
    to be linear.
    That is, there will be a matrix $A$ and vector $b$
    such that only inputs $x$ satisfying $Ax \leq b$ will be allowed.
    (Note that for \textit{vectors} $a$ and $b$,
    $a \leq b$ will mean that each component $a_i$
    is less than or equal to the corresponding component $b_i$.)
    These are called the \textsc{(principal) constraints},
    and vectors $x$ which satisfy the constraints will be called
    \textsc{feasible (solutions)}.
    (Note that in \cite{delsarte}, the term \textit{program}
    is used to refer to a feasible solution.)

    If there are no constraints on the problem
    (and even in some cases where there are)
    through appropriate choices of feasible solution $x$,
    the objective $c^Tx$ may be made arbitrarily large,
    and such problems are called \textsc{unbounded}.
    Conversely, if no feasible solutions exist,
    then the problem is called \textsc{infeasible}.

    Finally, in most applications of linear programming --
    in particular to the cliques of association schemes --
    the feasible solutions will be further constrained
    to those with all non-negative components (i.e. $x \geq 0$).
    These are called the \textsc{non-negativity constraints},
    in constrast with the \textit{principal constraints}.
    The non-negativity constraints will be required throughout the remainder of
    this report.

    Therefore, for an objective $c^T x$ and constraints $Ax \leq b$,
    the associated linear program will be written in \textsc{standard form}:
    $$
      \max\buildset{
        c^T x
      }{
        A x \leq b,\
        x \geq 0
      }
      \ .
    $$

    \subsection{Duality}

      The most important observation about linear programs
      (for the purposes of this report, at least)
      is that they come in dual pairs.

      Given a linear program $\mathcal{P}$ written in standard form
      $$
        \max\buildset{
          c^T x
        }{
          A x \leq b,\
          x \geq 0
        }
      $$
      its \textsc{dual} program is $\mathcal{P}^*$:
      $$
        \min\buildset{
          b^T y
        }{
          A^T y \geq c,\
          y \geq 0
        }
        \ .
      $$
      Re-writing it in standard form,
      $$
        \max\buildset{
          (-b)^T y
        }{
          -A^T y \leq -c,\
          y \geq 0
        }
      $$
      taking the dual
      $$
        \min\buildset{
          -c^T x
        }{
          -A x \geq -b,\
          x \geq 0
        }
      $$
      and re-writing in standard form
      $$
        \max\buildset{
          c^T x
        }{
          A x \leq b,\
          x \geq 0
        }
      $$
      the original (called \textsc{primal}) linear program is recovered.

      This demonstrates that $\(\mathcal{P}^*\)^* = \mathcal{P}$,
      so that linear programs come in dual pairs.

      \begin{thm}[Weak Duality]\label{weak-duality}
        If $x$ is a feasible solution to a linear program
        $$
          \max\buildset{
            c^T x
          }{
            A x \leq b,\
            x \geq 0
          },
        $$
        and $y$ is a feasible solution to its dual program,
        $$
          \min\buildset{
            b^T y
          }{
            A^T y \geq c,\
            y \geq 0
          },
        $$
        then $c^T x \leq b^T y$.
      \end{thm}

      \begin{proof}
        Let $u, v, w$ be vectors with $u \geq 0$, and $v \leq w$.
        Then for all components $i$,
        $u_i \geq 0$ and $v_i \leq w_i$ implies that $u_i v_i \leq u_i w_i$
        so that
        $$
          u^T v = \sum_i u_i v_i
          \leq \sum_i u_i w_i = u^T w
          \ .
        $$

        In particular, since $y$ is a feasible solution to the dual program,
        and $x \geq 0$,
        $$
          c \leq A^T y 
          \implies x^T c \leq x^T A^T y = y^T A x
          \ .
        $$
        (Here one may take the transpose of the whole expression,
        since the result is a scalar.)
        Similarly, since $x$ is a feasible solution to the primal program,
        and $y \geq 0$,
        $$
          b \geq A x 
          \implies y^T b \geq y^T A x
          \ .
        $$
        By combining the two inequalities,
        $$
          b^T y = y^T b \geq y^T A x \geq x^T c = c^T x
        $$
        which is the desired result.
      \end{proof}

      As a result of the weak duality of linear programs,
      every feasible solution to the dual program
      provides an upper bound on the maximum of the primal,
      and every feasible solution to the primal program
      provides a lower bound on the minimum of the dual.

      In fact the extremal values of dual programs
      (the maximum of the primal, and the minimum of the dual)
      coincide, although this will not be needed for the purposes of this
      report.
      This is referred to as \textit{Strong Duality} of linear programs.

  \section{The LP Bound}

    \begin{defn}
      The \textsc{inner distribution} is TODO.
      I might also put this in the section on association schemes;
      I'm not sure if it belongs better there or here.
    \end{defn}

    \begin{thm}[Delsarte Thm 3.3 \cite{delsarte}]\label{lp-ineq}
      For any inner distribution $y$,
      $$
        Q^T y \geq 0
      $$
      where $Q$ is the matrix of dual eigenvalues.
      (Here, $x \geq 0$ means that each component of
      the vector $x$ is not less than $0$.)
    \end{thm}

    \begin{proof}
      TODO.
      Note that this will require a number of lemmas which I've omitted here for
      brevity, but will include in the final product.
    \end{proof}

    This theorem provides the key inequality that will allow the application of
    linear programming to cliques in association schemes.
    However, because the constraint vector in a primal linear program
    becomes the objective in the dual program,
    this inequality will require some transformation to make it suitable for use
    in linear programming.

    Let $Y$ be an $M$-clique with inner distribution $y$.
    Then $y_i = 0$ for all $i \not\in M$,
    so $Q^T y \geq 0 \iff Q^T \diagM y \geq 0$
    since the action of $\diagM$ acting on the left
    is to zero out the \textit{rows} of $y$
    with index not in $M$.
    Similarly,
    $$
      Q^T \diagM y
      = Q(0)^T y_0 + Q^T \diagMs y
      = \mu + Q^T \diagMs y
    $$
    since the action of $\diagMs$ on the right
    is to zero out the \textit{columns} of $Q^T$
    with index not in $M^*$,
    $y_0 = 1$, and
    $$
      Q^T =
      \begin{bmatrix}
        1 & 1 & \cdots & 1 \\
        \mu_1 & & & \\
        \vdots & & * & \\
        \mu_d & & & \\
      \end{bmatrix}
      \ .
    $$
    Finally, since $y \geq 0$, $Q_0^T y \geq 0$ adds no new constraint,
    so that under the non-negativity constraint
    $Q^T y \geq 0 \iff \diagNs Q^T y \geq 0$.

    Putting all this together,
    $Q^T y \geq 0 \iff \diagNs Q^T \diagMs y \geq -\mu$
    so that Delsarte's LP can be written in standard form:
    \begin{alignat}{2}
      & \max\buildset{
        \vone^T \diagM y
      }{
        Q^T \diagM y \geq 0,\
        y \geq 0,\
        y_0 = 1
      } \label{dlp-primal} \\
      =& \max\buildset{
        \chiMs^T y
      }{
        - \diagNs Q^T \diagMs y \leq \diagNs \mu,\
        y \geq 0
      } + 1 \label{dlp-primal-std}
      \ .
    \end{alignat}

    Taking the dual yields
    \begin{alignat}{2}
      & \min\buildset{
        \mu^T \diagNs z
      }{
        - \diagMs Q \diagNs z \geq \chiMs,\
        z \geq 0
      } + 1 \label{dlp-dual-std} \\
      =& \min\buildset{
        \mu^T z
      }{
        - \diagMs Q \diagNs z \geq \chiMs,\
        z \geq 0,\
        z_0 = 1
      }
      \ .
    \end{alignat}
    Therefore, if $z_0 = 1$ is required,
    recalling that $Q_0 = \vone$ and $\diagMs \vone = \chiMs$, then
    \begin{alignat*}{2}
      & \diagMs Q \diagN z \\
      =& \diagMs \( Q_0 z_0 + Q \diagNs z \) \\
      =& \chiMs + \diagMs Q \diagNs z \\
      \leq& 0
      \ .
    \end{alignat*}
    This equivalence recover's Delsarte's formulation of
    the dual linear program:
    \begin{equation}\label{dlp-dual}
      \min\buildset{
        \mu^T z
      }{
        \diagMs Q z \leq 0,\
        z \geq 0,\
        z_0 = 1
      }
      \ .
    \end{equation}

  \section{The Ratio Bound}
    We will use (\ref{dlp-dual}) frequently.

  \section{The Clique-Coclique Bound}

\chapter{Schrijver's SDP Bound}
  \section{The Terwilliger Algebra of the Hamming Scheme}

  \section{Semi-Definite Programming}

\chapter{Computation}
  I wasn't sure if I ought to mention anything about the code I've written for
  this project (or even if there's anything worth saying that won't be covered
  elsewhere in the report).

  Also, if there are some specific results that would be interesting to show,
  but do not fit naturally into other sections of the report, then perhaps they
  could go here as well.

\appendix

\chapter{Linear Algebra}
  \section{The Spectral Theorem}

  \section{Adjacency Matrices}
    Basic results about the spectra of adjacency matrices, which may be used
    elsewhere in the report.  E.g. the sum of eigenvalues with multiplicity, and
    consequences.

  \section{Positive Semi-Definite Matrices}
    Depending on which proof of the clique-coclique bound I use, and how much
    detail I go into Schrijver's SDP bound, I could make some comments about PSD
    matrices.

\chapter{Group Theory}
  A \textsc{group} is a set $G$ equipped with an associative binary operation
  $G \times G \to G$ satisfying certain axioms.
  Typically this product is written \textit{multiplicatively},
  so that $(g, h) \mapsto gh$.
  To be a group, there must exist an \textsc{identity} element $1 \in G$
  satisfying $1g = g1 = g$ for every $g \in G$.
  When the group is not clear from context,
  the identity element of $G$ will be written $1_G$.

  Furthermore, for every $g \in G$, there must exist $h \in G$
  such that $gh = hg = 1$.
  In this case, $h$ is unique (for $g$) and is denoted $g^{-1}$
  and called the \textsc{inverse} of $g$.

  In the event that the product is commutative,
  then the group is called commutative, or \textsc{abelian}.

  A map $\phi: G \to H$ between groups is a \textsc{homomorphism}
  if $\phi(g g') = \phi(g) \phi(g')$,
  and an \textsc{isomorphism} if it is invertible,
  and its inverse is also a homomorphism.
  Happily, every invertible homomorphism is also an isomorphism.
  Since an isomorphism preserves exactly a group's structure,
  if there exists an isomorphism $G \to H$,
  then the two groups are called \textsc{isomorphic},
  and may be considered ``the same'' for many (but not all) purposes.
  An isomorphism from a group to itself is called an \textsc{automorphism}.

  The preimage of $1_H$ is called the \textsc{kernel} of $\phi$
  and denoted $\ker \phi$.
  An important fact is that a homomorphism is injective if and only if its
  kernel is trivial (contains only the identity of its domain).
  On the other hand, the identity is always contained in the kernel.
  \\

  Given a set $X$, the set of bijections $X \to X$ (denoted $\Sym X$)
  forms a group where the product $fg$ of two bijections
  is their composition $f \circ g$.
  The identity of this group is the identity map $\id_X$,
  and inversion in the group is the same as inversion of the maps.
  This is called the \textsc{symmetric group} on $X$,
  and its elements are called \textsc{permutations}.
  If a subset of a group is also a group with the same product and same
  identity, then the subset is called a \textsc{subgroup}.
  Subgroups of a symmetric group are called \textsc{permutation groups}.

  Historically, group theory began in the study of permutation groups,
  and conversely, by \textit{Cayley's theorem},
  every group is isomorphic to a permutation group.
  The following section formalizes some of the important aspects of this
  correspondence between group elements and permutations.

  \section{Group Actions}
    The material of this section comes primarily from \cite[Section~1.7;
    Chapter~4]{dummit-foote}.

    \begin{defn}[Group Action]\label{group-action}
      Given a group $G$ and a set $X$,
      \textsc{group action} is a homomorphism $G \to \Sym X$,
      where $\Sym X$ is the symmetric group on $X$.
    \end{defn}

    A group action $\phi: G \to \Sym X$ induces a product
    $X \times G \to X$ by mapping $(x, g) \mapsto \phi(g)(x)$.
    When the action is clear from context,
    this will be denoted $x \cdot g$, or simply $xg$.
    This is called a \textsc{right action},
    as $g$ \textit{acts on the right of} $x$
    (the corresponding notion of a \textsc{left action}
    can also be defined.)

    Conversely, given a product $X \times G \to X$,
    the same expression defines a map $G \to \Sym X$.
    If such a product satisfies
    \begin{alignat*}{2}
      &\forall x \in X\ x 1_G = x \\
      \text{and } &\forall x \in X\ \forall g, h \in G\
        (x g) h = x(hg)
    \end{alignat*}
    then the induced map $G \to \Sym X$ will be a homomorphism,
    so that these definitions are equivalent.

    (In \cite{dummit-foote}
    this is taken as the definition of a group action,
    and the homomorphism $G \to \Sym X$ is called its \textsc{permutation
    representation}.  It will be occasionally convenient to adopt each
    perspective.)

    \begin{defn}[Types of Group Actions]\label{group-action-types}
      If if a homomorphism $G \to \Sym X$ is injective,
      then the action is called \textsc{faithful}.
      Note that a group homomorphism is injective
      if and only if it has a trivial kernel.

      Given a group action $G \to \Sym X$,
      $g \in G$ is called \textsc{fixed point-free}
      if $\forall x \in X\ xg \neq x$.
      The group action itself is called \textsc{fixed point-free}
      (or just \textsc{free}) if all its nontrivial elements
      are fixed point-free.

      A group action $G \to \Sym X$ is called \textsc{transitive}
      if $\forall x, y \in X$ there exists some $g \in G$
      such that $xg = y$.

      A group action is called \textsc{regular} if it is simultaneously
      transitive and free.  (This terminology follows \cite{godsil}.)
    \end{defn}

    Note that if $X$ is a structure with automorphisms
    (such as a graph or group), $G$ is a subgroup of $\Aut X$,
    and $G$ acts in the natural way on $X$ (i.e. $xg = g(x)$),
    then this action is faithful.
    That is, $\Aut X \leq \Sym X$,
    so that this action is induced by the identity $G \into \Sym X$,
    which is clearly injective.

    \begin{lem}\label{faithful-transitive-abelian}
      If an abelian group $G$ acts faithfully and transitively on a set $X$,
      then the action is free, and thus also regular. \cite[Section 4.1,
      Exercise 3]{dummit-foote}
    \end{lem}

    \begin{proof}
      Let $g \in G$ be nontrivial, and $x \in X$.
      The goal is to prove that $xg \neq x$.

      Since $g$ is not the identity,
      there exists some $y \in X$ such that $z := yg \neq y$.
      Furthermore, since $G$ acts transitively on $X$,
      there exists some $h \in G$ such that $yh = x \iff y = xh^{-1}$.
      Then,
      \begin{alignat*}{3}
        xg &= (yh) g &\\
        &= y (gh) &\\
        &= y (hg) \quad&\text{ since $G$ is abelian}\\
        &= (yg) h &\\
        &= zh \ .&\\
      \end{alignat*}

      If $zh = x$ then, $z = xh^{-1} = y$,
      but by definition, $z = yg \neq y$,
      so $xg = zh \neq x$.
    \end{proof}

  \section{Character Theory}
    Definition, and basic results used.

\chapter{Notation}
  I've included these sections mostly as an excuse to add the citations to the
  bibliography, though it may be somewhat useful to have a sort of guide to the
  similarities and differences in notation in this report, as well as in the
  references.  (At least, I would find such a thing useful.)

  \section{Godsil}
    See \cite{godsil} Ch. 10.

  \section{Delsarte}
    See \cite{delsarte} Ch. 3.

  \section{Schrijver}
    See \cite{schrijver}.

\printbibliography[heading=bibintoc]

\end{document}
