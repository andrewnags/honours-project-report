\documentclass{report}

\usepackage{preamble}
\usepackage{prealgebra}
\usepackage{premath}
\usepackage{precomb}

\usepackage[style=alphabetic,citestyle=alphabetic]{biblatex}
\addbibresource{report.bib}

% Remove "chapter" heading
\usepackage{titlesec}
\titleformat{\chapter}{\normalfont\huge\bf}{\thechapter.}{20pt}{\huge\bf}

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand{\mysep}{
  \begin{center}
  \scalebox{0.2}{
  \begin{tikzpicture}
    \tikzset{
      dot/.style = {circle, fill, minimum size=4pt,
      inner sep=0pt, outer sep=0pt},
      dot/.default = 4pt % size of the circle diameter
    }

    \node[dot] (-) {};

    \node[dot, above right of = -] (0) {};
    \node[dot, right of = 0] (1) {};
    \draw (0) edge (1);

    \node[dot, below right of = 1] (00) {};
    \node[dot, right of = 00] (01) {};
    \node[dot, above of = 00] (10) {};
    \node[dot, right of = 10] (11) {};
    \draw[-, line width=0.5pt]
      (00) edge (01)
      (00) edge (10)
      (01) edge (11)
      (10) edge (11)
    ;

    \node[dot, right of = 01] (000) {};
    \node[dot, right of = 000] (001) {};
    \node[dot, above of = 000] (010) {};
    \node[dot, right of = 010] (011) {};
    \node[dot, above right of = 000] (100) {};
    \node[dot, right of = 100] (101) {};
    \node[dot, above of = 100] (110) {};
    \node[dot, right of = 110] (111) {};
    \draw[-, line width=0.5pt]
      (001) edge (101)
      (010) edge (011)
      (010) edge (110)
      (000) edge (001)
      (001) edge (011)
      (101) edge (111)
      (100) edge (101)
      (100) edge (110)
      (000) edge (010)
      (000) edge (100)
      (110) edge (111)
      (011) edge (111)
    ;

    \node[dot, right of = 101] (00') {};
    \node[dot, right of = 00'] (01') {};
    \node[dot, above of = 00'] (10') {};
    \node[dot, right of = 10'] (11') {};
    \draw[-, line width=0.5pt]
      (00') edge (01')
      (00') edge (10')
      (01') edge (11')
      (10') edge (11')
    ;

    \node[dot, below right of = 01'] (0') {};
    \node[dot, right of = 0'] (1') {};
    \draw (0') edge (1');

    \node[dot, above right of = 1'] (-') {};
  \end{tikzpicture}
  }
  \end{center}
}

\usepackage{array}
\newcolumntype{C}{>{$\displaystyle}c<{$}}  % math display mode in table
% A new columntype to apply automatic stepping
\newcounter{rowcntr}[table]
\renewcommand{\therowcntr}{\thetable.\arabic{rowcntr}}
\newcolumntype{N}{>{\refstepcounter{rowcntr}\therowcntr}c}
\AtBeginEnvironment{tabular}{\setcounter{rowcntr}{0}}  % reset the rowcntr counter at each new tabular

\newcommand{\AS}{\mathcal{A}}
\newcommand{\BMA}{\mathbb{A}}
\newcommand{\Zq}{\Z_q}
\newcommand{\Zqz}{\Z_q \setminus \set{0}}
\newcommand{\Zqd}{\Z_q^d}
\newcommand{\diag}[1]{\operatorname{diag}\( #1 \)}
\newcommand{\wt}[1]{\abs{ #1 }}
%\newcommand{\wt}[1]{\operatorname{wt}\( #1 \)}
\newcommand{\dist}[2]{\operatorname{dist}\( #1, #2 \)}
\newcommand{\chiD}{\chi_{D}}
\newcommand{\chiC}{\chi_{C}}
\newcommand{\chiDs}{\chi_{D^*}}
\newcommand{\chiCs}{\chi_{C^*}}
\newcommand{\diagz}{\diag{\chi_0}}
\newcommand{\diagD}{\diag{\chiD}}
\newcommand{\diagC}{\diag{\chiC}}
\newcommand{\diagDs}{\diag{\chiDs}}
\newcommand{\diagCs}{\diag{\chiCs}}
\newcommand{\diagnu}{\diag{\nu}}
\newcommand{\diagmu}{\diag{\mu}}
\newcommand{\vone}{\mathbf{1}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\lijt}{\lambda_{i, j}^t}
\newcommand{\lijtu}{\lambda_{i, j}^{t, u}}
\newcommand{\xijt}{x_{i, j}^t}
\newcommand{\Mijt}{M_{i, j}^t}
\newcommand{\chipiY}{\chi_{\pi(Y)}}
\newcommand{\tu}{\tilde{u}}

\title{Cliques, Codes, and Association Schemes}
\author{
  Andrew Nagarajah \\
  Supervised by: Prof. Mike Newman
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}

  This report is an exploration of some of the topics within \emph{Delsarte
  theory}, which uses ideas from graph theory, algebra, and optimization to
  address questions in coding theory in particular, and combinatorics more
  broadly.  The centre of study is the \emph{association scheme}, which provides
  a setting in which to view various objects, especially \emph{distance-regular
  graphs}.  This perspective enables the computation of various parameters of
  interest, including the eigenvalues of graphs and upper bounds on codes,
  cliques, and independent sets.

  This report aims to be mostly self-contained, though background knowledge of
  basic linear algebra and group theory is required.  The important aspects of
  this theory is reviewed in the appendix.

\end{abstract}

\tableofcontents

\chapter{Introduction}\label{ch:intro}
  \section{Coding Theory}\label{sec:intro:coding}
    In most communication, a transmitter in one location must send a message to
    a receiver in another location, possibly by means of a faulty (or
    \textit{noisy}) channel which may introduce errors into the message.

    For example, a spacecraft might send scientific data back to earth from
    another planet by means of a radio signal.  Along the way, other sources of
    electromagnetic radiation might interfere with the signal, such that when it
    arrives at earth, the signal received is slightly different from the one
    sent.
    The scientific data, if successfully received, might needed to be saved in
    some storage medium so that it may be accessed in the future.  Over time
    however, the storage medium may degrade, so that when the data is retrieved,
    it may be slightly different from when it was saved.

    In human speech, if two friends are speaking in a loud environment and one
    says ``I love you'', if the ambient noise is loud enough, the friend might
    hear ``I lave you'' instead.  Not all possible sounds (or combinations of
    letters) are valid messages in English though, so the friend could
    reasonably guess at what was meant.  In this sense, English contains
    \textit{redundancy}.  However, if the environment is so loud that the friend
    hears ``I late you'', then they might not know what the message what
    intended to mean, or incorrectly guess that the true message was ``I hate
    you''.  On the other hand, if the friend hears ``l shgbot gtin'', then they
    will know for sure that the message was corrupted; they will know not to
    misinterpret the message, and may ask that it be re-transmitted.
    \\

    These are the two fundamental goals of coding theory: to encode data in such
    a way that it can be recovered if some errors are introduced; or if it
    cannot be recovered, then at least to detect that the message was corrupted.
    As the above example illustrates, and encoding scheme may be able to correct
    a limited number of errors, or detect some number of errors.  For this
    report, we will focus on the former goal.

    In general, a coding scheme will consist of a set of signals $V$, among
    which a subset $Y$ (called the \textsc{code}) might consist of valid
    messages.  Then, the receiver will need a function $V \times V \to \N$ to
    tell how many errors would be required to change one signal into another.
    We assume that if $s$ errors convert signal $u$ into signal $v$, then $s$
    ``opposite'' errors can convert $v$ into $u$.  Furthermore, if $v$ can be
    converted into $w$ with $t$ errors, then $u$ can be converted into $w$ with
    no more that $s + t$ errors (this is called the \textsc{triangle
    inequality}).  Finally, $u$ will require $0$ errors to be converted into
    itself; conversely for any pair of distinct signals $u$ and $v$, a nonzero
    number of errors should be required to convert one into the other.  Such a
    set of signals, paired with such an error-counting function, is called a
    \textsc{(discrete) metric space}, and its function is typically called a
    \textsc{distance}.  When a signal $u$ arrives at the receiver, they can try
    and find a signal $v$ in the code $Y$ which minimizes the distance between
    $u$ and $v$.  If there is a unique such minimizer $v$, then we decode $u$
    into $v$.

    For example, if we wish to encode the messages $0$ (or `no') and $1$ (or
    `yes'), the set of signals might consist of all binary strings of length
    three, with $000$ and $111$ as the codewords.  If errors consist of
    individual bit flips, then the message $101$ can be decoded into $111$, as
    only one error is required to convert the latter into the former.  However
    if two errors occur and $100$ is received, then this will be decoded into
    $000$, since fewer errors are required to convert between the two.
    Therefore, this code (called a \textit{repetition code}) can correct at most
    $1$ error.

    More generally, if $\delta$ is the \textsc{minimum distance} between any two
    codewords, and $w$ is a signal at distance no more than $\floor{\frac{\delta
    - 1}{2}}$ from a codeword $u$, then its distance to any other codeword $v$
    must be greater.  Otherwise, $u$ could be converted to $v$ with at most
    $\delta - 1$ errors -- a contradiction!

    On the other hand, in the example repetition code given above, there were
    $2^1 = 2$ codewords, and $2^3 = 8$ possible signals.  So, for every message
    we wish to send, three times as many bits need to be transmitted.  This is
    called the \textsc{rate} of the code.

    These two parameters, the minimum distance of the code, and the proportion
    of signals which are codewords, are the two primary measures of the
    effectiveness and efficiency of a code.  A large minimum distance in a code
    will mean that many errors can be corrected, while a large proportion will
    mean that messages are being transmitted efficiently.  The challenge, then,
    is to design codes which have simultaneously a large minimum distance and a
    large proportion of codewords.  One of the aims of this report is to put
    bounds on how efficient a code can be given a target effectiveness; plainly,
    if we want a certain minimum distance code in a set of signals, at most how
    many codewords can there be?  The approach to this question is ultimately
    due to Delsarte \cite{delsarte}, and has be expanded upon by Schrijver
    \cite{schrijver}.

  \section{Linear Codes and Distance-Regular
    Graphs}\label{sec:intro:linear->drg}

    Let $V$ be denote a vector space of dimension $d$ over a finite field of
    order $q$.  Then $Y \subseteq V$ is a \textsc{linear code} if $Y$ is a linear
    subspace of $V$.  One such vector space is the set of length $d$ bitstrings:
    this is a vector space of dimension $d$ over the finite field of two
    elements.  (Here, the addition of vectors corresponds to the bitwise
    exclusive-or of bitstrings.)  A code in such a vector space would be
    immediately applicable to digital data.  This motivates the study of linear
    codes.  One way in which binary data can be corrupted is through bit flips.
    Analogously, if $v = v_1 e_1 + \cdots + v_d e_d$ is a vector written in the
    fixed basis $e_1, \ldots, e_d$, then an error may occur by picking an index
    $i$, and changing the value of $v_i$.  In order to count such errors, we
    define the following distance function on $V$.

    \begin{defn}\label{hamming-distance}
      If $V$ is the vector space of $d$-tuples over a finite field, then the
      \textsc{(Hamming) weight} of a vector $v = (v_1, \ldots, v_d)$ is the number of
      nonzero components $v_i$; this is denoted $\wt{v}$.
      Then the \textsc{(Hamming) distance} between vectors $u, v$ is defined
      $$
        \dist{u}{v} := \wt{u - v} \ .
      $$
    \end{defn}

    In other words, the distance between two vectors is the number of
    coordinates on which they differ.  Since a linear subspace must be closed
    under subtraction, the minimum distance of a linear code is the same as its
    minimum (nonzero) weight.  Meanwhile, the rate of the code is the ratio of
    the dimension of the code to the dimension of its ambient space.
    \\

    While much of coding theory is done in this setting, this report will look
    at codes through a combinatorial lens.  A \textsc{directed graph} on a
    vertex set $V$ is a binary relation $\Gamma \subseteq V \times V$.  If the
    relation is symmetric, then the graph is said to be \textsc{undirected}.
    (Note that this definition will prohibit parallel edges, but not loops.) The
    relation $\Gamma$ is typically denoted $\sim$ and written infix when the
    graph is clear from the context.  For each vector space as described above,
    we can define a corresponding graph and investigate codes in this graph
    instead.

    \begin{defn}\label{hamming-graph}
      The \textsc{Hamming graph} $H(d, q)$ is defined so that its vertex set $V$
      is the set of all $d$-tuples, each entry of which belongs to a fixed set
      of $q$ elements.  Two vertices are said to be adjacent if they have
      hamming distance $1$.

      The graph $H_i(d, q)$ is defined on the same vertex set; however, vertices
      in this graph are adjacent if they have hamming distance $i$.  With this
      definition, $H(d, q) = H_1(d, q)$.
    \end{defn}

    Note that, unlike vector spaces of the form $\GF(q)^d$, the Hamming graph
    $H(d, q)$ is not restricted to prime powers for $q$.  Therefore, we will
    frequently take $\Z_q^d$ to be the vertex set of the Hamming graph.  In the
    case that $q$ is a prime power, we may take $\GF(q)^d$ as the vertex set
    instead.  While these algebraic structures are not isomorphic, their Hamming
    graphs are.

    By generalizing finite vector spaces to Hamming graphs in this way, the
    explicit algebraic structure of the vector space is lost.  However, the
    Hamming graphs have many combinatorial properties which will prove useful in
    this report.  It will also turn out that the Hamming graphs have interesting
    algebraic properties; for example, they have large automorphism groups.
    More importantly however, they are \textit{distance-regular graphs}.

    \begin{defn}\label{defn:drg}
      Let $\Gamma$ be a graph, and $u, v$ be vertices in the graph.  Then the
      distance between $u$ and $v$ in $\Gamma$, denoted $\dist{u}{v}$, is the
      length of a shortest path between $u$ and $v$.  Let $N_i(u)$ denote the
      set of vertices at distance $i$ from $u$ (if $u$ is not incident to a
      loop, then $N_1(u)$ will be the neighbourhood of $u$).

      A graph is said to be \textsc{distance-regular} if for all distances $i$,
      $j$, and $k$, and any two vertices $u, v$ at distance $k$, there is a
      constant $p_{ij}^k$ such that
      $$
        \abs{N_i(u) \cap N_j(v)} = p_{ij}^k \ .
      $$
      In particular, at $u = v$ and $i = j = 1$, if the graph is loopless,
      the above constraint implies that the graph is \textsc{regular} -- that
      the degree of each vertex is constant.  This degree is also called the
      \textsc{valency} of the graph.

      If $\Gamma$ is any graph, then the \textsc{distance graphs} of $\Gamma$
      are the graphs $\Gamma_0, \Gamma_1, \Gamma_2, \ldots$ where each shares
      the same vertex set as $\Gamma$, and vertices are adjacent in $\Gamma_i$
      if and only if they are at distance $i$ in $\Gamma$.  Note that if
      $\Gamma$ is loopless, then $\Gamma = \Gamma_1$.  If $\Gamma_d$ is the last
      non-empty graph in this sequence (i.e. $d$ is the maximum distance in the
      graph), then $d$ is called the \textsc{diameter}.
    \end{defn}

    The Hamming graphs are distance-regular, where the graphs $H_i(d, q)$ are
    the distance graphs of $H(d, q)$, and $d$ is the diameter.  Other examples
    of distance-regular graphs include the Johnson graphs and the Grassman
    graphs.

    For any graph with distance graphs $\Gamma_0, \Gamma_1, \ldots, \Gamma_d$,
    we can define a \textit{code} to be a subset of the vertex set, where the
    distance between vertices is their distance in the graph; that is,
    $\dist{u}{v} = i$ if and only if $u$ and $v$ are adjacent in $\Gamma_i$.
    Then the code has minimum distance $\delta$ if and only if $\delta$ is the
    least positive distance $i$ in $\set{1, \ldots, d}$ for which there exists a
    pair of codewords which are adjacent in $\Gamma_i$.  In part because the
    Hamming graphs belong to this class, we will study coding theory in the
    context of distance-regular graphs for the remainder of this report.
    \\

    Note that the study of codes in graphs is also connected to the study of
    \textit{cocliques} (or \textit{independent sets}).  A \textsc{coclique} in a
    graph is a set of vertices, none of which are adjacent.  Equivalently, a
    coclique is a code in a graph with minimum distance $2$.  On the other hand,
    given distance graphs $\Gamma_0, \Gamma_1, \ldots, \Gamma_d$, then a code of
    minimum distance $\delta$ is an independent set in the graph obtained by
    taking the union of the edgesets of $\Gamma_1, \ldots, \Gamma_\delta$.

  \section{Hamming Graphs}\label{sec:intro:hamming}

    This section will discuss some of the algebraic properties of the Hamming
    graphs.  The fact that the Hamming graphs are distance-regular will be used
    without comment throughout the remainder of the report.  The automorphism
    group of a Hamming graph is helpful in demonstrating this fact.  The
    automorphism group also plays an important role in Schrijver's SDP bound
    (\ref{sec:SDP-bound:SDP-bound}).  Readers who are familiar with the
    automorphism groups of the Hamming graphs, or are willing to accept that
    these graphs are distance-regular, may omit this section.  

    The simplest way to show that the Hamming graphs are distance-regular is to
    show that they satisfy a stronger property, called
    \textit{distance-transitivity}.  To do this, it will be helpful to examine
    the automorphism group of the Hamming graph $H(d, q)$.

    Throughout this section, the vertex set of $H(d, q)$ will be taken to be
    $\Z_q^d$, and $e_i$ will denote the tuple of all zeroes, and a $1$ in the
    $i^\text{th}$ entry.  Then, the neighbours of any vertex $v$ are the
    vertices of the form $v + \alpha e_i$, where $\alpha \in \Z_q \setminus
    \set{0}$.  We will denote by $[d]$ the set $\set{1, \ldots, d}$.
    \\

    The most important property of the automorphism group of a Hamming graph is
    that the action of any automorphism is completely determined by its action
    on a small subset of the graph.  In particular, given any vertex $v$, any
    automorphism is determined by its action on $v$ and the neighbourhood of
    $v$.  

    \begin{lem}\label{lem:hamming-characterized-distance}
      Fix any vertex $v$ in $H(d, q)$.  Then for each vertex $u$, the relation
      $$
        D_v(u) := \buildset{
          (v + \alpha e_i,\ \dist{v + \alpha e_i}{u})
        }{
          \alpha \in \Z_q ,\ 
          i \in [d]
        }
      $$
      completely characterizes the vertex $u$.  (We will call this relation the
      \emph{distance profile} of $u$ with respect to $v$.)
    \end{lem}

    \begin{proof}
      Note that for any two vertices $w$ and $u$, their distance is given by the
      Hamming weight of their difference: $\dist{w}{u} = \wt{w - u}$.

      Then, for any $\alpha \neq 0$, we know that
      $$
        \dist{u}{v + \alpha e_i}
        = \wt{u - (v + \alpha e_i)}
        = \wt{(u - v) - \alpha e_i}
        = \begin{cases}
          > \wt{u - v} & \If u_i = v_i \\
          < \wt{u - v} & \If u_i - v_i = \alpha \\
          = \wt{u - v} & \Else
        \end{cases};
      $$
      precisely; the only case when subtracting $\alpha e_i$ from $u - v$
      lowers the weight of the vector is in the case that the $i^\text{th}$
      component of $u - v$ is $\alpha$.  Therefore, if we are given all the
      weights $\dist{u}{v + \alpha e-i}$, we can reconstruct $u$ as follows:
      $$
        u = v + \sum_{i \in [d]} e_i \sum_{\alpha \in \Z_q \setminus \set{0}}
          \alpha \cdot \mathbbm{1}\[
            \dist{u}{v + \alpha e_i} < \dist{u}{v}
          \]
      $$
      where $\mathbbm{1}$ is the indicator function.  Note that for each $i$,
      there will be \emph{at most one value} of $\alpha$ for which $\dist{u}{v
      + \alpha e_i} < \dist{u}{v}$.
    \end{proof}

    \begin{cor}\label{cor:hamming-aut-characterized-nbhd}
      For any vertex $v$, the action of any automorphism of a Hamming graph is
      completely determined by its action on $\buildset{v + \alpha e_i}{\alpha
      \in \Z_q,\ i \in [d]}$.
    \end{cor}

    \begin{proof}
      Note that every graph automorphism preserves the distances in that graph.

      Fix a vertex $v$ and suppose that $g$ and $g'$ are automorphisms of $H(d,
      q)$ such that $(v + \alpha e_i) g = (v + \alpha e_i) g'$ for all $\alpha$
      and $i$.  Then for any vertex $u$, let $D_v(u) = \set{(v + \alpha e_i,\
      d_{\alpha, i})}_{\alpha, i}$ be the distance profile of $u$ with respect
      to $v$.  Then since automorphisms preserve distances, for all $\alpha$ and
      $i$,
      \begin{alignat*}{2}
        d_{\alpha, i} &= \dist{(v + \alpha e_i) g}{ug} \\
        d_{\alpha, i} &= \dist{(v + \alpha e_i) g'}{ug'} \\
      \end{alignat*}
      and $(v + \alpha e_i)g = (v + \alpha e_i) g'$.  Therefore, the distance
      profiles of $ug$ and $ug'$ with respect to $vg$ are identical, since $vg =
      vg'$, so that $ug = ug'$.
    \end{proof}

    Now, to compute the automorphism group of $H(d, q)$, we will employ a
    strategy following the orbit-stabilizer theorem: we will show that there is
    a single orbit, so that the size of the group is the size of the vertex set
    times the size of stabilizer of a single vertex, say $\vzero$.  Since the
    group is finite, by finding a set of automorphisms which map $\vzero$ to
    each other vertex, and the stabilizer of $\vzero$, taking the compositions
    of all pairs of automorphisms from these two subgroups will generate all
    automorphisms.

    \begin{lem}\label{lem:hamming-shift-automorphisms}
      For any vertex $v$, the map $T_v : u \mapsto u + v$ is an automorphism of
      $H(d, q)$.  On the other hand, every automorphism of $H(d, q)$ is the
      composition of an automorphism of the form $T_v$ for some vertex $v$, and
      an automorphism fixing $\vzero$.
    \end{lem}

    \begin{proof}
      Let $v$ be a vertex of $H(d, q)$.  Then $T_v$ is invertible, since
      $T_v^{-1} = T_{-v}$.  Furthermore, for any vertices $u$ and $w$,
      $$
        \wt{uT_v - wt_v}
        = \wt{(u + v) - (w + v)}
        = \wt{u - w}
      $$
      so $T_v$ preserves adjacency.

      Let $g$ be any automorphism of $H(d, q)$, and let $\vzero g = v$.  Then 
      $\vzero g T_{-v} = \vzero$, so $g = g' T_v$, where $g'$ is an automorphism
      fixing $\vzero$.
    \end{proof}

    Note that this result tells us that the Hamming graph $H(d, q)$ is vertex
    transitive, since for all vertices $u$ and $u'$, $u T_{u' - u} = u + (u' -
    u) = u'$.

    Now let us consider the stabiliser of $\vzero$.  Since automorphisms
    preserve distances, if $g$ fixes $\vzero$, then it will permute the
    neighbours $\alpha e_i$ of $\vzero$.

    \begin{lem}\label{lem:hamming-automorphisms-preserve-component}
      Let $g$ be an automorphism  of $H(d, q)$ which fixes $\vzero$.  Then for
      each $\alpha$ and $e_i$, $g: \alpha e_i \mapsto \beta e_j$ where $\alpha$
      is zero if and only if $\beta$ is.
      Moreover, if $g: \alpha e_i \mapsto \beta e_j$ for some $\alpha$, then for
      all $\alpha'$ there exists a $\beta'$ such that $g: \alpha' e_i \mapsto
      \beta' e_j$.
    \end{lem}

    \begin{proof}
      Note that if $g: \alpha e_i \mapsto \beta e_j$, then $\alpha = 0 \iff \beta
      = 0$ follows directly from the fact that $g$ fixes $\vzero$.  Then, if
      $\alpha$ is nonzero, $\alpha e_i$ will be a neighbour of $\vzero$, so its
      image under an automorphism must also be a neighbour of $\vzero$.  Since
      every neighbour of $\vzero$ is of the form $\beta e_j$, the result
      follows.

      Suppose then that $g: \alpha e_i \mapsto \beta e_j$ with $\alpha \neq 0$.
      If $\alpha' \neq 0$ and $\alpha' \neq \alpha$, then $\alpha e_i$ and
      $\alpha' e_i$ are neighbours, so $\alpha' e_i g$ must be adjacent to
      $\beta e_j$.  By the previous paragraph, $\beta \neq 0$, so all neighbours
      of $\beta e_j$ are of the form $\beta' e_j$.
    \end{proof}

    Now let us consider some examples of automorphisms of $H(d, q)$ that fix
    $\vzero$.  We will then show how all such automorphisms can be constructed
    from the examples we find.

    \begin{lem}\label{lem:hamming-component-permutation}
      Let $\sigma \in \Sym [d]$, and let $\sigma$ act on $H(d, q)$ by mapping
      $(x_1, \ldots, x_d)$ to $(x_{\sigma(1)}, \ldots, x_{\sigma(d)})$.  Then
      $\sigma$ is an automorphism of $H(d, q)$ which fixes $\vzero$.
    \end{lem}

    \begin{proof}
      Note that the action of $\sigma$ on $H(d, q)$ is invertible, with inverse
      given by the action of $\sigma^{-1}$ (as a permutation on $[d]$).
      Furthermore, note that $\sigma$ does not alter the weight the vertices it
      acts on.  Thus, $\vzero \sigma = \vzero$, and since, $v \sigma - u \sigma
      = (v - u) \sigma$, $\sigma$ is an automorphism.
    \end{proof}

    \begin{lem}\label{lem:hamming-coefficient-permutation}
      Let $\tau \in \Sym\( \Z_q \setminus \set{0} \)$; then $\tau$ extends
      naturally to a permutation on $\Z_q$ by fixing $0$.  Fixing any $i \in
      [d]$, $\tau$ acts on $H(d, q)$ by mapping $(x_1, \ldots, x_i, \ldots,
      x_d)$ to $(x_1, \ldots, \tau(x_i), \ldots, x_d)$ .
      For each $\tau$ and $i$, this action is an automorphism on $H(d, q)$
      fixing $\vzero$.
    \end{lem}

    \begin{proof}
      Note that since the action of $\tau$ on $\Z_q$ fixes $0$, the action of
      $\tau$ on $H(d, q)$ fixes $\vzero$.  Furthermore, if $\tau$ acts on the
      $i^\text{th}$ component of vertices in the graph, then this action of
      $\tau$ is invertible by applying the inverse of $\tau$ in $\Sym \Z_q$ to
      the $i^\text{th}$ component.

      Suppose that $u \sim v$ are vertices: then $u - v = \alpha e_j$ for some
      $\alpha \neq 0$ and $j \in [d]$.  For $j' \neq j$, $u_{j'} = v_{j'}$ which
      implies that $\tau(u_{j'}) = \tau(v_{j'}) \implies (u \tau - v \tau)_{j'}
      = 0$.  On the other hand, since $\tau$ is a permutation, $u_j \neq v_j
      \implies \tau(u_j) \neq \tau(v_j)$ so that $(u\tau - v\tau)_j \neq 0$.
    \end{proof}

    \begin{thm}\label{thm:hamming-automorphisms}
      Every automorphism of $H(d, q)$ is of the form $\tau \sigma T_v$ where
      $\tau = \tau_1 \cdots \tau_d$, and each $\tau_i \in \Sym\( \Z_q \setminus
      \set{0} \)$; where $\sigma \in \Sym[d]$; and where $v$ is any vertex.
    \end{thm}

    \begin{proof}
      Let $g$ be any automorphism of $H(d, q)$, and suppose $\vzero g = v$.
      Then by Lemma (\ref{lem:hamming-shift-automorphisms}) $g' := g T_{-v}$ is
      an automorphism which fixes $\vzero$.  Then by Lemma
      (\ref{lem:hamming-automorphisms-preserve-component}) for all $i$ there
      exists a $j$ such that for all nonzero $\alpha$, $g': \alpha e_i \mapsto
      \beta e_j$ for some nonzero $\beta$.  Therefore, let $\sigma^{-1}$ be the
      permutation mapping each $j$ back to $i$, so that $g'' = g' \sigma^{-1}$
      maps $\alpha e_i$ to $\beta e_i$ (Lemma
      \ref{lem:hamming-component-permutation}).  Finally, for each $i$ and
      nonzero $\alpha$, by Lemma
      (\ref{lem:hamming-automorphisms-preserve-component}) $\beta$ is nonzero,
      so define $\tau_i$ to be the permutation on $\Z_q$ mapping $\alpha$ to
      $\beta$, and fixing $0$ (Lemma \ref{lem:hamming-coefficient-permutation}).
      Setting $\tau := \tau_1 \cdots \tau_d$, we see that $g'' \tau^{-1}: \alpha
      e_i \mapsto \alpha e_i$ for all $i$ and all $\alpha$.  Since automorphisms
      of $H(d, q)$ preserve distances in the graph and are completely
      determined by their action on $\buildset{\alpha e_i}{\alpha \in \Z_q,\ i
      \in [d]}$, $g T_v^{-1} \sigma^{-1} \tau^{-1}$ is the identity on $H(d,
      q)$.
    \end{proof}

    \begin{cor}\label{cor:hamming-aut-group-size}
      There are exactly $q!^d d!$ automorphisms of $H(d, q)$.
    \end{cor}

    \begin{proof}
      It is straightforward to see that for each distinct tuple of vertex $v$
      and permutations $\sigma \in \Sym[d]$ and $\tau_1, \ldots, \tau_d \in
      \Sym\( \Z_q \setminus \set{0} \)$ there is a distict automorphism $\tau_1
      \cdots \tau_d \sigma T_v$.  There are $q^d$ vertices $v$, $d!$
      permutations $\sigma$, and $(q - 1)!^d$ permutations $\tau = \tau_1 \cdots
      \tau_d$.
    \end{proof}


    \mysep


    \begin{defn}\label{defn:distance-transitive}
      A graph $\Gamma$ is \textsc{distance-transitive} if for all pairs of
      vertices $u, v$ and $u', v'$ such that $\dist{u}{v} = \dist{u'}{v'}$,
      there is an automorphism of $\Gamma$ mapping $u$ to $u'$ and $v$ to $v'$.
    \end{defn}

    Recall that $N_k(v)$ denotes the set of vertices at distance $k$ from $v$.
    The following result follows almost directly from the characterization of
    the automorphism group of $H(d, q)$.

    % TODO Ask Newman about this language.
    \begin{lem}
      The set $N_k(\vzero)$ is transitive under the action of the stabilizer of
      $\vzero$.  That is, for any two vertices $u, u' \in N_k(\vzero)$, there is
      an automorphism fixing $\vzero$ which maps $u$ to $u'$.
    \end{lem}

    \begin{proof}
      If $u \in N_k(\vzero)$, then there exist coefficients $\alpha_i \in \Z_q$
      -- exactly $k$ of which are nonzero -- such that $u = \sum_i \alpha_i
      e_i$.  Likewise, since $u' \in N_k(\vzero)$, there exists exactly $k$
      nonzero coefficients $\alpha_i' \in \Z_q$ such that $u' = \sum_i \alpha_i'
      e_i$.  Note that each permutation $\sigma \in \Sym[d]$ gives an
      automorphism fixing $\vzero$; so we may choose two permutations to map $u$
      and $u'$ such that the first $k$ values of $\alpha_i$ and $\alpha_i'$ are
      nonzero.  Therefore, assume without loss of generality that
      $$
        u = \sum_{i=0}^k \alpha_i e_i \quad \And \quad
        u' = \sum_{i=0}^k \alpha_i' e_i \ .
      $$
      Then, for all $i$ in $[k]$, $\alpha_i \neq 0$ and $\alpha_i' \neq 0$, so
      there exist permutations $\tau_i$ in $\Sym\( \Z_q \setminus \set{0} \)$
      such that $\tau := \tau_1 \cdots \tau_k$ maps each $\alpha_i$ to
      $\alpha_i'$.
    \end{proof}

    \begin{cor}\label{lem:hamming->distance-transitive}
      The Hamming graph $H(d, q)$ is distance-transitive.
    \end{cor}

    \begin{proof}
      Let $u, u', v, v'$ be vertices such that $\dist{u}{v} = \dist{u'}{v'}$.
      Since automorphisms preserve distance, and the shift automorphisms
      $T_{-u'}$ and $T_{-u}$ map $u'$ and $u$ to $\vzero$, we may assume without
      loss of generality that $u = u' = \vzero$.  Then, for $k :=
      \dist{\vzero}{v} = \dist{\vzero}{v'}$, we find that $v$ and $v'$ are in
      $N_k(\vzero)$.  Since this set is transitive under the action of the
      stabilizer of $\vzero$, there must exist an automorphism fixing $\vzero$
      and mapping $v$ to $v'$.
    \end{proof}

    \begin{lem}\label{lem:distance-transitive->drg}
      Every distance-transitive graph is distance-regular.
    \end{lem}

    \begin{proof}
      Let $i, j$, and $k$ be non-negative integers, and let $u, u', v, v'$ be
      vertices such that $k = \dist{u}{v} = \dist{u'}{v'}$.  It suffices to show
      that the number of vertices $w$ satisfying $\dist{u}{w} = i$ and
      $\dist{w}{v} = j$ equals the number of vertices $w'$ satisfying
      $\dist{u'}{w'} = i$ and $\dist{w'}{v'} = j$.  Since the graph in question
      is distance-transitive, there exists an automorphism mapping $u$ to $u'$
      and $v$ to $v'$.  Then, since automorphisms preserve distances, each $w$
      in $N_i(u) \cap N_j(v)$ will get mapped to an element $w'$ in $N_i(u')
      \cap N_j(v')$.  Since automorphisms are invertible, this provides the
      required bijection.
    \end{proof}

    \begin{cor}
      The Hamming graphs are distance-regular.
    \end{cor}

    By considering the automorphisms of the Hamming graphs $H(d, q)$, we noticed
    that they satisfying the property of \textit{distance-transitivity}.  In
    turn, all distance-transitive graphs satisfy a regularity condition in the
    numbers of vertices at certain distances from other vertices.  It is
    interesting to note that not all distance-regular graphs have large
    automorphism groups like the Hamming graphs.  While this numerical
    regularity may not seem particularly important, by replacing the notion of
    \textit{distance graphs} with any list of graphs, we arrive (roughly) at the
    definition of an association scheme.  It will turn out that these objects
    have many incredible properties, some of which will help us answer our
    original question: what can we say about the size of a code in a graph?
    While this numerical regularity will be sufficient for our answer in
    general, it will be computationally easier when the association schemes have
    ``nice'' automorphism groups.  In particular, the automorphism group of the
    Hamming graph will help us answer our original question with greater
    accuracy.

\chapter{Association Schemes}\label{ch:AS}
  \section{Association Schemes}\label{sec:AS:AS}

    \begin{defn}[Commutative Association Scheme -- Combinatorial
      {{\cite[Section 2.1]{delsarte}}}]
      \label{association-scheme-comb}
      Let $D = \set{0, 1, \ldots, d}$ for some $d \geq 1$.
      A \textsc{commutative association scheme} $\AS$ is a set $V$, called the
      \textsc{vertex set}, together with a set of relations $\set{\Gamma_i}_{i=0}^d$
      satisfying the following axioms:
      \begin{enumerate}
        \item The set of relations $\set{\Gamma_i}_{i=0}^d$ partitions $V \times
          V$;
          \label{cAS-part}
        \item $\Gamma_0$ is the diagonal relation $\buildset{(u, u)}{u \in V}$;
          \label{cAS-diag}
        \item For each $i \in D$, there is an $i' \in D$ such that $\Gamma_{i'}$ is
          the \textit{opposite relation} $\buildset{(v, u)}{(u, v) \in \Gamma_i}$ of
          $\Gamma_i$;
          \label{cAS-sym}
        \item For every triple $i, j, k \in D$, there exists a constant $p_{i,
          j}^k$ such that for all $(u, v) \in \Gamma_k$, there are exactly $p_{i,
          j}^k$ vertices $w$ such that $(u, w) \in \Gamma_i$ and $(w, v) \in
          \Gamma_j$;
          furthermore, $p_{i, j}^k = p_{j, i}^k$.
          \label{cAS-reg}
      \end{enumerate}

      As used above, the elements of $V$ are called \textsc{vertices},
      and vertices $(u, v) \in \Gamma_i$ are called \textsc{$i^\text{th}$ associates}.

      Each relation $\Gamma_i$ is called a \textsc{class} of $\AS$, which has
      \textsc{diameter} $d$ (this will be explained in connection with
      distance-regular graphs in the next section (\ref{sec:AS:PPS})).
      Sometimes, $\AS$ is said to be a $d$-class association scheme (as the
      diagonal relation is discounted).
    \end{defn}

    To every relation $\Gamma \subseteq V \times V$ there exists a $V \times V$ $01$
    matrix $A$, where
    \begin{equation}\label{adjacency-matrix}
      A_{uv} = \begin{cases}
        1 & \If (u, v) \in \Gamma \\
        0 & \If (u, v) \not\in \Gamma \\
      \end{cases};
    \end{equation}
    $A$ is then called the \textsc{adjacency matrix} of $\Gamma$.
    This is a bijective correspondence between relations on $V \times V$,
    and $V \times V$ matrices with each entry either $0$ or $1$.
    (Note that binary relations are precisely directed graphs without parallel
    edges, and this usage of the term \textit{adjacency matrix} agrees with its
    usage in graph theory.)

    By exchanging the relations $\Gamma_i$ for their adjacency matrices $A_i$, the
    combinatorial definition of an association scheme can be reformulated as
    follows:

    \begin{defn}[Commutative Association Scheme -- Algebraic
      {{\cite[Chapter 12]{godsil}}}]
      \label{association-scheme-alg}
      Let $\circ$ denote the \textsc{Schur} product of matrices of the same shape:
      \begin{equation}\label{schur-prod}
        (A \circ B)_{uv} = A_{uv} B_{uv}
        \ .
      \end{equation}
      (This is also called the \textit{Hadamard}, or \textit{entrywise} product.)

      A \textsc{commutative association scheme} is a vertex set $V$ along with
      $V \times V$ matrices $A_0, A_1, \ldots A_d$ such that
      \begin{enumerate}
        \item $\displaystyle \sum_{i=0}^d A_i = J$, the all-ones matrix,
          and $A_i \circ A_j = \delta_{i, j} A_i$;
          \label{aAS-part}
        \item $A_0 = I$, the identity matrix;
          \label{aAS-diag}
        \item For every $i \in D$ there is an $i' \in D$ such that $A_i^T =
          A_{i'}$;
          \label{aAS-sym}
        \item For every $i, j$,
          $$
            A_i A_j = A_j A_i = \sum_{k = 0}^d p_{i, j}^k A_k
            \ .
          $$
          \label{aAS-reg}
      \end{enumerate}
    \end{defn}

    It is clear from the first axiom that the $A_i$ are $01$ matrices.

    The first three equivalences are straightforward translations.  To see the
    last, observe that
    $$
      (A_i A_j)_{uv} = \sum_{z \in V} (A_i)_{uw} (A_j)_{wv}
    $$
    which counts the number of vertices $z \in V$ such that
    $$
      \begin{cases}
        (A_i){uw} = 1 & \iff (u, w) \in \Gamma_i \\
        (A_j){wv} = 1 & \iff (w, v) \in \Gamma_j \\
      \end{cases}\ .
    $$
    Then, $(A_i A_j)_{uv} = p_{ij}^k$ exactly when $(A_k)_{uv} = 1 \iff (u, v)
    \in \Gamma_k$.
    \\


    The requirement in (\hyperref[cAS-reg]{Combinatorial 4}) that $p_{i, j}^k =
    p_{j, i}^k$ corresponds to the requirement that $A_i A_j = A_j A_i$
    (\hyperref[aAS-reg]{Algebraic 4}), which is why such association schemes are
    called \textit{commutative}.

    If the requirement of \textit{commutativity} is dropped, then every finite
    group $G$ is an association scheme in the following way.  Cayley's theorem
    says that every group is isomorphic to the group of permutations on $G$
    given by left multiplication (the same construction will work if right
    multiplication is used everywhere instead of left).  By identifying each
    element of $G$ with the $G \times G$ \textit{permutation matrix}, we obtain
    an association scheme with $G$ as the vertex set, and permutation matrices
    as the classes.  Since the identity element of $G$ will map to the identity
    matrix, the product of two such permutations is another such permutation,
    and the transpose of a permutation matrix is its inverse, axioms
    (\ref{aAS-diag}, \ref{aAS-sym}, \ref{aAS-reg}) are satisfied.  Since for
    every $g, h \in G$, only the element $hg^{-1}$ maps $g$ to $h$, exactly one
    of the permutation matrices will have a $1$ in the $(g, h)$-entry; all the
    others will be $0$.  This association scheme will be commutative if and only
    if the group $G$ is.

    Not every association scheme (commutative or not) arises is this way, but
    many schemes of interest are closely related to a particular group.
    Nevertheless, much of the utility of association schemes, and the elegance
    of their theory, derives from the connection between the combinatorial view
    of schemes as relations or (di)graphs (\ref{association-scheme-comb}), and
    the algebraic view of schemes as matrices (\ref{association-scheme-alg}).
    \\

    A particularly important class of association scheme, called
    \textsc{symmetric}, is one in which every relation is symmetric (i.e. $i =
    i'$ in \hyperref[cAS-sym]{Combinatorial 3}), or equivalently, each adjacency
    matrix is symmetric ($A_i^T = A_i$ in \hyperref[aAS-sym]{Algebraic 3}).
    In this case, the relations $\Gamma_i$ form \textit{undirected} graphs $\Gamma_i$
    with vertex set $V$, and edge set given by
    $$
      u \sim v \iff (u, v) \in \Gamma_i \iff (v, u) \in \Gamma_i
      \ .
    $$
    The most important class of symmetric association scheme (for the purposes
    of this report) arise as the distance graphs of a distance-regular graph
    (\ref{sec:AS:PPS}).

    \subsection{The Bose-Mesner Algebra}\label{sec:AS:AS:BMA}
      For any association scheme $\AS$,
      there are the adjacency matrices
      $A_0, A_1, \ldots, A_d$ (\ref{association-scheme-alg}).
      Since any product of these matrices belongs to their span (\ref{aAS-reg}),
      their span
      \begin{equation}\label{BMA}
        \BMA := \Span\set{A_0, A_1, \ldots, A_d}
      \end{equation}
      is closed under matrix multiplication.  This structure is called the
      \textsc{Bose-Mesner algebra} of $\AS$.

      Note also that from the first axiom of an association scheme
      (\ref{aAS-part}), $\BMA$ is also closed under the \textit{Schur product},
      so that $\BMA$ is actually an algebra with respect to two
      \textit{different} products.
      This \textit{duality} will form an important aspect of the theory of
      association schemes, and will be discussed in detail in this section and
      the next (\ref{sec:AS:AS:duality}).

      Since the adjoint $A_i^* = A_i^T$ belongs to $A_0, A_1, \ldots, A_d$ for
      every $i$, and these matrices all commute, each $A_i$ is a normal operator
      (TODO reference).  From the spectral theorem,
      we can decompose each matrix
      $$
        A_i = \sum_j \theta_{ij} \tilde{F_{ij}}
      $$
      into a linear combination of orthogonal idempotents $\tilde{F_{ij}}$, where
      $$
        I = \sum_j \tilde{F_{ij}}
        \ .
      $$
      Since each $\tilde{F_{ij}}$ is a polynomial in $A_i$, and the $A_i$ all
      commute, so too do the $\tilde{F_{ij}}$.  Therefore, the products
      $\prod_i \tilde{F_{i{k_i}}}$ (for any choices of $k_i$) are also
      orthogonal idempotents, though some may be zero, so that the nonzero
      products are linearly independent.  Furthermore, their sum is the identity
      matrix
      $$
        I = I^{d+1}
        = \prod_i \( \sum_j \tilde{F_{ij}} \)
        = \sum \( \prod_i \tilde{F_{i{k_i}}} \)
      $$
      where the latter sum is taken over all choices of $k_i$.
      Therefore, we can express each $A_i$ as linear combinations of these
      products, as
      \begin{alignat*}{2}
        A_i \prod_{i'} \tilde{F_{i{k_i}}}
        &= A_i \tilde{F_{i{k_i}}} \prod_{i' \neq i} \tilde{F_{i{k_i}}} \\
        &= \theta_{i{k_i}} \tilde{F_{i{k_i}}}
          \prod_{i' \neq i} \tilde{F_{i{k_i}}} \\
        &= \theta_{i{k_i}} \prod_{i'} \tilde{F_{i{k_i}}} \\
        \implies A_i = A_i I
        &= \sum \theta_{i{k_i}} \prod_{i'} \tilde{F_{i{k_i}}} \ .
      \end{alignat*}

      Therefore, the non-zero products $\prod_i \tilde{F_{ik_i}}$ span $\BMA$
      and are linearly independent, so there are precisely $d+1 = \dim \BMA$ of
      them: $F_0, F_1, \ldots, F_d$.  They are orthogonal idempotents, and each
      is self-adjoint since the $\tilde{F_{ij}}$ are self-adjoint and commute.
      These matrices are called the \textsc{principal idempotents} of $\AS$, and
      form an alternative basis to $A_0, A_1, \ldots, A_d$.  (Because the $A_i$
      are orthogonal idempotents under the Schur product, they are sometimes
      called \textsc{Schur idempotents} of $\AS$.)
      Then, one can write
      $$
        A_i = \sum_{j = 0}^d P_i(j) F_j
      $$
      and form the $(d+1) \times (d+1)$ matrix $P$ by $P_{ji} = P_i(j)$
      (note the reversed indices).
      This is called the \textsc{matrix of eigenvalues} of $\AS$.
      \cite[Theorem 12.2.1]{godsil}

      In the case that $\AS$ is symmetric, each $A_i$ is a real, symmetric
      operator, so its eigenvalues $\theta_{ij}$ are real, as are its
      idempotents $\tilde{F_{ij}}$.  Therefore, the idempotents $F_j$ are real,
      as is the matrix of eigenvalues $P$.

    \subsection{Duality and Characterization}\label{sec:AS:AS:duality}
      Already a comparison between the two bases $A_0, A_1, \ldots, A_d$
      and $F_0, F_1, \ldots, F_d$ of $\BMA$ suggests a duality.  For example,
      the $A_i$ are orthogonal idempotents with respect to the Schur product,
      while the $F_j$ are orthogonal idempotents with respect to the usual
      product; $\sum_i A_i = J$, the identity of the Schur product, while
      $\sum_j F_j = I$, the identity of the usual product.
      In this section, additional ``dual'' properties will be discovered by
      examining relations with respect to one basis, and searching for analogous
      ones in the opposite basis.

      Many of these properties will involve two matrices to be introduced, which
      describe how each basis is transformed into the other.  Some will be vital
      to the later analysis of cliques in association schemes.  Others will be
      used to show that either of these matrices, taken in isolation, completely
      characterize the scheme!
      \\

      First, the axioms of an association scheme (\ref{association-scheme-alg})
      require that $A_0 = I$, the identity of the usual product.  As already
      illustrated, $J$ the all-ones matrix acts as the ``dual'' of $I$, so let
      us examine:
      $$
        JF_j = \( \sum_i A_i \) F_j = \( \sum_i P_i(j) \) F_j
        \ .
      $$
      Letting $\gamma_j := \sum_i P_i(j)$, $J F_j = \gamma_j F_j$ demonstrates
      that $\gamma_j$ is an eigenvalue of $J$, with eigenvectors in the column
      space of $F_j$.  Since $J$ is orthogonally diagonalizable,
      with eigenvalues $n = \abs{X}$ of multiplicity $1$,
      and $0$ of multiplicity $n - 1$,
      and the $F_j$ are orthogonal idempotents whose columns are eigenvectors of
      $J$, there is exactly one $j$ with $\gamma_j = n$.
      Since $J F_j = n F_j$, each row of $F_j$ is equal;
      but $F_j$ is self-adjoint the columns are all equal as well.
      Therefore, $F_j = \alpha J$, and $F_j$ is idempotent,
      so that
      $$
        F_j = \alpha J = \alpha^2 J^2 = \alpha^2 n J
        \implies \alpha = \frac{1}{n}
        \ .
      $$
      Since the ordering $F_0, F_1, \ldots, F_d$ is arbitrary,
      by convention an order is taken such that $F_0 = \frac{1}{n}J$.

      Since the $F_j$ are orthogonal, $\gamma_j = 0$ for all $j > 0$,
      so that the row sums of $P$ are known:
      \begin{equation}\label{P-row-sum}
        \sum_{i = 0}^d P_i(j) =
        \begin{cases}
          n & \If j = 0 \\
          0 & \If j > 0
        \end{cases} \ .
      \end{equation}

      Moreover, since $F_0 = \frac{1}{n}J$, the all-ones vector $\vone$ is an
      eigenvector of each $A_i$: let $\nu_i$ be the eigenvalue, so that $A_i
      \vone = \nu_i \vone$.  Note that $\nu_i$ is the (constant) row sum of
      $A_i$.  Since $A_i^T = A_{i'}$, the column sums of $A_i$ are constant as
      well, and if the scheme is symmetric, $A_i^T = A_i$, so the column sums
      are equal to $\nu_i$ as well.  The quantity $\nu_i$ is called the
      \textsc{valency} of $A_i$.  (This usage agrees with its usage for regular
      graphs, where it equals the (constant) degree of vertices.  This is also
      equal to the row sum of the adjacency matrix.)
      \\

      Next, since $F_0, F_1, \ldots, F_d$ is a basis for $\BMA$,
      we can write
      \begin{equation}\label{F-basis}
        A_i = \sum_j P_i(j) F_j
      \end{equation}
      uniquely, and since the $F_j$ are orthogonal idempotents,
      $A_i F_j = P_i(j) F_j$.
      Dually, $A_0, A_1, \ldots, A_d$ is a basis for $\BMA$, so we can write
      \begin{equation}\label{A-basis}
        F_j = \frac{1}{n} \sum_i Q_j(i) A_i
      \end{equation}
      uniquely, and since the $A_i$ are Schur orthogonal idempotents,
      $F_j \circ A_i = \frac{1}{n} Q_i(j) A_i$.
      These coefficients form the \textsc{matrix of dual eigenvalues} $Q$,
      defined such that $Q_{ij} = Q_j(i)$.

      By substituting (\ref{A-basis}) into (\ref{F-basis}), we obtain
      $$
        n A_i = \sum_j P_i(j) \sum_{i'} Q_j(i') A_{i'}
        = \sum_{i'} \( \sum_j P_i(j) Q_j(i') \) A_{i'}
        \ .
      $$
      Because the $A_i$ form a basis,
      $n A_i$ is written uniquely,
      so that
      $$
        \sum_j P_i(j) Q_j(i') = n \delta_{ii'}
        \implies PQ = nI
        \ .
      $$
      Since the first row sum of $P$ is $n$ (\ref{P-row-sum}),
      if we are given $P$ and \textit{no other information},
      then by computing its inverse we can derive $Q$.

      If we were given only $Q$ instead,
      we could calculate $P$ by taking the inverse
      and multiplying by $n$.
      All that remains, then, is to derive $n$ from $Q$.
      In the other direction, this was done by examining the row sums of $P$,
      so we do the same with $Q$.
      The rows of $Q$ correspond to the $A_i$,
      the columns to the $F_j$,
      and $F_j \circ A_i = \frac{1}{n} Q_j(i) A_i$,
      so we fix a row and sum over the columns:
      $$
        \( \sum_j F_j \) \circ A_i = \frac{1}{n} \( \sum_j Q_j(i) \) A_i
        \ .
      $$
      But $\sum_j F_j = I = A_0$ and $A_0 A_i = \delta_{0 i} A_i$
      so
      \begin{equation}\label{Q-row-sum}
        \sum_{j = 0}^d Q_j(i) =
        \begin{cases}
          n & \If i = 0 \\
          0 & \If i > 0
        \end{cases} \ .
      \end{equation}
      \\

      Therefore, given either $P$ or $Q$, and no other information, the other
      can be derived.  However, this process involves matrix inversion, which is
      both expensive to compute and awkward to contemplate.  In fact, if a
      little extra information is known, then there exists an extraordinarily
      simple formula relating the two matrices.  This extra information consists
      of the valencies and their dual concept, the multiplicities, of the scheme.

      We have already encountered the valency $\nu_i$ of $A_i$.  Since by
      definition $\nu_i = P_i(0)$, consider the first row of $Q$ and denote
      $Q_j(0)$ by $\mu_j$.  From the definition of $Q$ we have $F_j \circ A_0 =
      \frac{1}{n} \mu_j A_0$, but $A_0$ is the identity matrix, so that the
      diagonal entries of $F_j$ are identically $\mu_j/n$.  Moreover, $F_j$ is
      an $n \times n$ matrix, so $\trace F_j = \mu_j$, and since $F_j$ is
      idempotent, $\rank F_j = \trace F_j$ (TODO reference).  If we fix $i$ and
      suppose momentarily that the $P_i(j)$ are all distinct, then the column
      space of $F_j$ is the $P_i(j)$-eigenspace of $A_i$, in which case $\mu_j =
      \dim\col F_j$, which is the algebraic multiplicity.  For this reason (even
      if the $P_i(j)$ are not distinct), $\mu_j$ will be called the
      \textsc{multiplicity} of $F_j$.
      (Is it possible that the $P_i(j)$ are not
      all distinct for any $i$?)

      Consider the different products between matrices in one basis
      and matrices in the other: $A_i F_j$ and $A_i \circ F_j$.
      Since $F_j$ is self-adjoint, and commutes with $A_i$,
      $A_i F_j = F_j^* A_i$.
      Writing the product in this way, we might notice that the trace of the
      regular product is actually the (standard) inner product of $A_i$ and
      $F_j$, as is the sum of all entries in the Schur product.
      $$
        \innerprod{A_i}{F_j} = \trace F_j^* A_i = \Sum A_i \circ F_j
      $$
      On one hand,
      $$
        \trace A_i F_j
        = P_i(j) \trace F_j = P_i(j) \mu_j
        \ ,
      $$
      while on the other hand, the $n$ rows of $A_i$ all sum to $\nu_i$, so
      $$
        \Sum A_i \circ F_j
        = \frac{1}{n} Q_j(i) \Sum A_i
        = Q_j(i) \nu_i
        \ .
      $$
      Of course these two expressions must be equal,
      so letting $\nu$ denote the vector of valencies,
      and $\mu$ the vector of multiplicities,
      we see that
      \begin{equation}\label{P-Q-reln}
        P^T \diag{\mu} = \diag{\nu} Q
        \ .
      \end{equation}
      Therefore, given the valencies and multiplicities, $Q$ can be
      computed inexpensively from $P$ and vice versa.  Furthermore, this
      explicit expression will be useful in later chapters when contemplating
      expressions involving $Q$ and vice versa.
      \\

      Briefly, note that in the definition of an association scheme, it was
      required that $A_0 = I$, but it is only by a conventional order that $F_0
      = \frac{1}{n} J$.  If instead we only required that $A_i = I$ for some
      $i$, then $\nu$ can still be identified within the permuted matrix of
      eigenvalues as the only row with nonzero sum.  Similarly, $\mu$ can be
      identified within $Q$ as the only row with nonzero sum, even if we do not
      adhere to our convention.

      \mysep

      For the remainder of this report, the above properties of the Bose-Mesner
      algebra and its duality will suffice.  However, with just a little extra
      work, we show that $\BMA$ is actually characterized by its matrix of
      eigenvalues (or its matrix of dual eigenvalues).
      \\

      The Bose-Mesner algebra has three kinds of structures on it: it is a
      vector space over $\C$, it is a ring with respect to the usual matrix
      product, and it is a ring with respect to the Schur product.
      In order to characterize $\BMA$, we will need to define all three
      structures from $P$ alone.

      First, $P$ is a $(d+1) \times (d+1)$ matrix when $\dim \BMA = d+1$.  Since
      vector spaces are characterized complete by their field and their
      dimension, the shape of $P$ suffices here.

      Next, since we have a basis $F_0, F_1, \ldots, F_d$ in which the usual
      matrix product is ``known'' (that is, they are orthogonal idempotents),
      this structure also will be characterized (vacuously) by $P$.

      Finally, we must characterize the Schur product in $\BMA$.  We also have a
      basis of orthogonal idempotents with respect to this product -- namely
      $A_0, A_1, \ldots, A_d$ -- but since this is a different basis from the
      one used previously, for the two products to be characterized
      simultaneously, we must be able to perform a change of basis.  However,
      $A_i = \sum_j P_i(j) F_j$ and $F_j = \sum_i Q_j(i) A_i$,
      where $Q$ can be derived from $P$.

      In other words, all the information needed to compute the isomorphism
      between $\BMA$ and $\C^{d+1}$ is encapsulated in $P$.  By picking a basis
      $f_0, f_1, \ldots, f_d$ in $\C^{d+1}$, the regular product can be defined
      by $f_i f_j := \delta_{ij} f_i$, and extended bilinearly.  Then, the Schur
      product can be defined by rewriting any element of $\C^{d+1}$ in the basis
      $a_i = \sum_j P_i(j)$ computing $a_i a_j = \delta_{ij} a_i$ and extending
      bilinearly.

      Because $P$ can be derived from $Q$, the Bose-Mesner algebra of $\AS$ is
      also characterized by $Q$.  Moreover, if $\AS$ is symmetric, then $\C$ can
      be replaced by $\R$ throughout the previous discussion.
      \\

      However, more can be said about the characterization of the two products
      on $\BMA$.  In particular, the usual product can be defined in terms of
      the basis of Schur idempotents where
      $$
        A_i A_j = \sum_{k=0}^d p_{ij}^k A_k \ .
      $$
      These $p_{ij}^k$ are called the \textsc{intersection numbers} of $\AS$.
      Similarly, we can define numbers $q_{ij}^k$, called the \textsc{Krein
      parameters}, such that
      $$
        F_i \circ F_j = \frac{1}{n} \sum_{k=0}^d q_{ij}^k F_k \ .
      $$
      These dual families of parameters can also be derived from $P$ or $Q$ in
      the following manner.

      Since the $A_i$ are Schur orthogonal idempotents, we can pick out the
      $p_{ij}^k$ by multiplying $(A_i A_j) \circ A_k = p_{ij}^k A_k$.  Summing
      over all entries of this product, $\Sum p_{ij}^k A_k = p_{ij}^k n \nu_k$.
      However, since the sum of entries in a Schur product is the inner product
      of the factors,
      $$
        \trace A_k^* A_i A_j
        = \trace \(\sum_{r=0}^d P_i(r) P_j(r) \bar{P_k(r)} F_r\)
        = \sum_{r=0}^d P_i(r) P_j(r) \bar{P_k(r)} \mu_r
        \ ,
      $$
      where $\bar{P_k(r)}$ is the complex conjugate of $P_k(r)$.
      (This is done by rewriting each of $A_i, A_j, A_k$ in terms of the $F_r$
      and expanding the product.)
      In the case that $\AS$ is symmetric $P$ is real, so that $\bar{P_k(r)} =
      P_k(r)$.  Then, it is clear that $p_{ij}^k$ remains unchanged when the
      indices $i, j, k$ are permuted.

      Similarly, $(F_i \circ F_j) F_k = q_{ij}^k F_k$, and $F_k$ is
      self-adjoint, so taking the trace $\trace q_{ij}^k F_k = q_{ij}^k \mu_k$.
      However, since the trace of a product is the inner product of the factors,
      $$
        \Sum F_k \circ F_i \circ F_j
        = \Sum \( \frac{1}{n^3}\sum_{r=0}^d Q_i(r) Q_j(r) Q_k(r) A_r \)
        = \frac{1}{n^2}\sum_{r=0}^d Q_i(r) Q_j(r) Q_k(r) \nu_r
        \ .
      $$
      As with the intersection numbers, $q_{ij}^k$ remains unchanged when the
      indices $i, j, k$ are permuted.
      \\

      (TODO? Derive $P$ from the $p_{ij}^k$?)

      \mysep

      Thus far, almost purely algebraic techniques have been used to derive
      results.  There two last dual pairs of results: the first is purely
      algebraic, while the second comes from graph theory.

      \begin{prop}
        The first columns of $P$ and $Q$ are the all-ones vector, $\vone$.
      \end{prop}

      \begin{proof}
        The first column of $P$ holds the eigenvalues of $A_0$, which by
        definition is equal to $I$.  Therefore, $A_0 F_j = P_0(j) F_j = F_j$, so
        $P_0(j) = 1$.

        The first column of $Q$ corresponds to $F_0 = \frac{1}{n} J$.
        Therefore, $F_0 \circ A_i = \frac{1}{n} Q_0(i) A_i$, so $Q_0(j) = 1$.
      \end{proof}

      Note that because $P^T \diag{\mu} = \diag{\nu} Q$, the each of the two
      results above could be derived from the other.

      \begin{prop}
        For all $i, j \in D$, we have $\abs{P_i(j)} \leq \nu_i$.
      \end{prop}

      \begin{proof}
        We know that for each $i$, we have $A_i \vone = \nu_i \vone$.
        Let $x$ be a $\theta$-eigenvector of $A_i$,
        scaled so that its largest component (say, $x_u$)
        has modulus $1$; let $\theta$ have largest modulus among the
        eigenvalues of $A_i$.
        Then $\abs{(A_i x)_u} = \abs{(\theta x)_u} = \abs{\theta}$, and
        $$
          \abs{\theta}
          = \abs{(A_i x)_u}
          \leq \sum_{v} (A_i)_{vu} \abs{x_v}
          \leq \sum_{v} (A_i)_{vu} \abs{x_u}
          = (A_i \vone)_u = \nu_i
          \ .
        $$
        The proposition follows, as each $P_i(j)$ is an eigenvalue of $A_i$.
      \end{proof}

      Note that $A_i \vone$ is the vector of row sums of $A_i$; the sum of
      each row is precisely the out-degrees of the relation (directed graph) of
      $A_i$.  Therefore, this proposition shows that the eigenvalues of a
      directed graph all have modulus less than the maximum out-degree.  In the
      case of a regular graph, this is simply the valency.

      \begin{cor}
        For all $i, j \in D$, we have $\abs{Q_j(i)} \leq \mu_j$.
      \end{cor}

      \begin{proof}
        This follows from the previous proposition, as $Q_j(i) =
        \frac{\mu_j}{\nu_i} P_i(j)$, and both the valencies and multiplicities
        are all positive integers.
      \end{proof}

      This completes the list of standard properties of the Bose-Mesner algebra
      and its duality.  The results are summarized in the table on the next
      page.

      \newpage

      \begin{table}[H]
      \centering
      \caption{Dual Properties of the Bose-Mesner Algebra}
      \label{tab:prop}
      {
      \renewcommand{\arraystretch}{2.5}
      \begin{tabular}{NC|C}
        \label{tab:prop:identity} &
          I = A_0
        &
           \frac{1}{n} J = F_0
        \\
        \label{tab:prop:sum} &
          J = \sum_{i=0}^d A_i
        &
          I = \sum_{j=0}^d F_j
        \\
        \label{tab:prop:idem} &
          A_i \circ A_j = \delta_{ij} A_i
        &
          F_i F_j = \delta_{ij} F_j
        \\ \hline
        \label{tab:prop:basis} &
          A_i = \sum_{j=0}^d P_i(j) F_j
        &
          F_j = \frac{1}{n} \sum_{i=0}^d Q_j(i) A_i
        \\
        \label{tab:prop:eigen} &
          A_i F_j = P_i(j) F_j
        &
          F_j \circ A_i = \frac{1}{n} Q_j(i) A_i
        \\
        \label{tab:prop:first-row} &
          P(0) = \nu^T
        &
          Q(0) = \mu^T
        \\
        \label{tab:prop:first-col} &
          P_0 = \vone
        &
          Q_0 = \vone
        \\
        \label{tab:prop:row-sum} &
          \sum_{i=0}^d P_i(j) = n \delta_{0j}
        &
          \sum_{j=0}^d Q_j(i) = n \delta_{0i}
        \\
        \label{tab:prop:modulus} &
          \abs{P_i(j)} \leq \nu_i
        &
          \abs{Q_j(i)} \leq \mu_j
        \\
        \label{tab:prop:valency-multiplicity} &
          P \diag{\nu}^{-1} P^T = n \diag{\mu}^{-1}
        &
          Q \diag{\mu}^{-1} Q^T = n \diag{\nu}^{-1}
        \\
        \label{tab:prop:inverse} &
          \multicolumn{2}{C}{PQ = nI}
        \\
        \label{tab:prop:similar} &
          \multicolumn{2}{C}{P^T \diag{\mu} = \diag{\nu} Q}
        \\ \hline
        \label{tab:prop:product} &
          A_i A_j = \sum_{k=0}^d p_{ij}^k A_k
        &
          F_i \circ F_j = \frac{1}{n} \sum_{k=0}^d q_{ij}^k F_k
        \\
        \label{tab:prop:intersection-krein} &
          p_{ij}^k = \frac{1}{n\nu_k} \sum_{r=0}^d P_i(r) P_j(r) \bar{P_k(r)}
          \mu_r
        &
          q_{ij}^k = \frac{1}{n^2 \mu_k} \sum_{r=0}^d Q_i(r) Q_j(r) Q_k(r) \nu_r
        \\
      \end{tabular}
      }
      \end{table}

      \newpage


  \section{$P$-Polynomial Schemes}\label{sec:AS:PPS}
    \begin{itemize}
      \item Definitions
      \item ``Equivalence" to DRGs
    \end{itemize}

    Our motivation in this report for the definition of an association scheme
    was the class of distance-regular graphs.  However, the class of association
    schemes -- even symmetric ones -- is much more general.  In this section, we
    will investigate a restricted class of symmetric association schemes: the
    $P$-polynomial schemes.

    Note that in this section, and for the remainder of this report, all
    association schemes will be symmetric unless otherwise specified.

    \begin{defn}\label{p-polynomial}
      An association scheme of adjacency matrices $A_0, A_1, \ldots, A_d$ is
      \textsc{$P$-polynomial} if they can be ordered such that $A_i$ is a
      polynomial in $A_1$ of degree $i$.
    \end{defn}

    It happens that every distance-regular graph forms a $P$-polynomial scheme,
    and that every $P$-polynomial scheme arises in this way.

    To demonstrate that distance-regular graphs generate $P$-polynomial schemes,
    we show that each distance matrix $A_i$ is a degree-$i$ polynomial in $A_1$
    by induction.  In order to do this, we need to be able to write $A_{i+1}$ in
    terms of the matrices at distance $i' \leq i$.

    \begin{lem}
      Let $\Gamma$ be a distance-regular graph with diameter $d$, vertex set
      $V$, and adjacency matrices $A_0, A_1, \ldots, A_d$.
      Then for all $i, j$,
      $$
        A_i A_j = \sum_{k=0}^d p_{ij}^k A_k
      $$
      where $p_{ij}^k = \abs{\buildset{w \in V}{d(u, w) = i \And d(w, v) = j}}$
      for fixed vertices $u$ and $v$ such that $d(u, v) = k$.
    \end{lem}

    \begin{proof}\label{drg-mult-closed}
      Let $\Gamma$ be a distance-regular graph with vertex set $V$, diameter
      $d$, and distance graphs $\Gamma_0, \Gamma_1, \ldots, \Gamma_d$
      with adjacency matrices $A_0, A_1, \ldots, A_d$.
      Then for vertices $u, v$ at distance $k$,
      \begin{alignat*}{2}
        p_{ij}^k &= \abs{N_i(u) \cap N_j(v)} \\
        &= \buildset{w \in V}{d(u, w) = i \And d(w, v) = j} \\
        &= \buildset{w \in V}{(u, w) \in \Gamma_i \And (w, v) \in \Gamma_j}
        \ .
      \end{alignat*}
      Then $A_i A_j = \sum_k p_{ij}^k A_k$ as shown in the equivalence between
      the two definitions of an association scheme
      (\ref{association-scheme-alg}, \ref{association-scheme-comb}).
    \end{proof}

    \begin{lem}\label{drg-mult-restricted}
      Let $\Gamma$ be a distance-regular graph with diameter $d$ and
      adjacency matrices $A_0, A_1, \ldots, A_d$.  Then for all $i$,
      $$
        A_i A_1 = \sum_{k=0}^d p_{i1}^k A_k
      $$
      where $p_{i1}^{i+1} \neq 0$,
      and $p_{i1}^k = 0$ for all $k > i + 1$.
    \end{lem}

    \begin{proof}
      If $\Gamma$ has diameter $d$, then there is a shortest path of length $d$.
      Note that every subpath of a shortest path is also a shortest path, so
      that there exists a shortest path of every length $k \leq d$.

      Let $u, v$ be vertices at distance $i+1$, and consider a shortest path
      between them.  On this path from $u$ to $v$, consider the first vertex $w$
      after $u$.  Then $d(u, w) = 1$, and since the remaining portion of the
      path, from $w$ to $v$ is also a shortest path, $d(w, v) = i$.  Therefore,
      $$
      \abs{\buildset{w \in V}{d(u, w) = 1 \And d(w, v) = i}}
      = p_{1i}^{i+1} = p_{i1}^{i+1} > 0 \ .
      $$

      Now consider $u, v$ at distance any $k$, and $w \in N_1(u) \cap N_i(v)$.
      By the triangle inequality,
      $$
        k = d(u, v) \leq d(u, w) + d(w, v)
        = 1 + i
        \ ,
      $$
      so that if $k > i + 1$, then $p_{i1}^k = 0$.
    \end{proof}

    \begin{thm}
      Every distance-regular graph $\Gamma$ forms a $P$-polynomial association
      scheme $\AS$, with the distance graphs of $\Gamma$ for classes.
      Moreover, the diameter of $\Gamma$ is the diameter of $\AS$.
    \end{thm}

    \begin{proof}
      Let $\Gamma$ be a distance-regular graph with diameter $d$,
      distance graphs $\Gamma_0, \Gamma_1, \ldots, \Gamma_d$,
      and distance matrices $A_0, A_1, \ldots, A_d$.
      From lemma (\ref{drg-mult-closed}), the product of distance matrices
      of a distance-regular graph belongs to the span of distance
      matrices.  Since the distance matrices are symmetric, the rest of the
      axioms follow.  Therefore, $\Gamma$ is an association scheme.

      Note that $A_0 = I$ is a polynomial of degree $0$ in $A_1$, and $A_1$ is a
      polynomial of degree $1$ in itself.  Then, supposing $A_i$ is a polynomial
      of degree $i$ in $A_1$, we have that $A_i A_1$ is a polynomial of degree
      $i+1$.  Then by lemma (\ref{drg-mult-restricted}),
      $$
        A_i A_1 = \sum_{k = 0}^{i+1} p_{i1}^k A_k
        \implies A_{i+1} = \frac{1}{p_{i1}^{i+1}} \(
          A_i A_1 - \sum_{k=0}^i p_{i1}^k A_k
        \)\ ,
      $$
      which demonstrates that $A_{i+1}$ is a polynomial of degree $i+1$ in
      $A_1$.
    \end{proof}

    \mysep

    In fact, the $P$-polynomial schemes correspond exactly with the association
    schemes arising from distance-regular graphs.  In particular, every
    $P$-polynomial scheme contains at least one class which is a
    distance-regular graph, and whose distance graphs form the classes of the
    scheme.  A $P$-polynomial scheme can be generated by more than one class,
    but no more than two classes.
    \\

    There is also an analogous definition of a \textit{$Q$-polynomial scheme}.
    If there exists an idempotent, declared $F_1$ such that each $F_j$ is a
    degree-$j$ polynomial in $F_1$ (using the Schur product in place of the
    regular product), then the scheme is $Q$-polynomial.  There is an additional
    theory of duality with $P$- and $Q$-polynomial translation schemes (see
    Section \ref{sec:AS:translation}), but these will not be discussed in this
    report.


  \section{Automorphisms and Cayley Graphs}

    The material in this section follows mostly from \cite[Chapter~9]{godsil}.
    \\

    This section and the next describes a special class of symmetric graphs
    (respectively, association schemes).  As with many other mathematical
    structures, the \textit{symmetry} of graphs (or schemes) is made precise by
    examining its automorphisms -- those transformations of the object in
    question which leave its structure unchanged.  Graphs (or schemes) with
    certain automorphisms may be classified in this way.

    More importantly for the purposes of this report, the structure revealed by
    the automorphisms of graphs (or schemes) allows one to compute their eigenvalues
    significantly more efficiently than otherwise would be the case, as outlined
    in the previous sections.

    \begin{defn}[Automorphism]\label{auts}
      For graphs $\Gamma, \Gamma'$,
      a map $\phi: V\(\Gamma\) \to V\(\Gamma'\)$ is a \textsc{homomorphism}
      if
      $$
        \forall u, v \in V\(\Gamma\) \
        u \sim_\Gamma v \implies \phi(u) \sim_{\Gamma'} \phi(v)
        \ .
      $$
      An \textsc{isomorphism} is an invertible homomorphism
      whose inverse is also a homomorphism;
      an \textsc{automorphism} is an isomorphism from a graph to itself.
      $\Aut \Gamma$ denotes the set of all automorphisms on $\Gamma$;
      it is the subgroup of $\Sym V\(\Gamma\)$,
      consisting of those permutations which preserve the
      (edge) structure of $\Gamma$.

      An automorphism of an association scheme $\AS$ on vertex set $X$
      is a map $X \to X$ which is simultaneously an automorphism of
      every class in the scheme.
      In other words, $\Aut \AS$ is the intersection of the automorphism groups
      of each class.
    \end{defn}

    \begin{defn}[Cayley Graphs]\label{cayley-graph}
      Given any group $G$ and a subset $C \subseteq G$,
      then $C$ is \textsc{inverse-closed} if for each $g \in C$,
      $g^{-1} \in C$ as well.

      If $C \subseteq G$ is an inverse-closed subset of a group $G$,
      then the \textsc{Cayley graph} of $G$ with respect to $C$ is denoted
      $\Cay(G, C)$, and defined as follows:
      \begin{itemize}
        \item Its vertex set is $G$
        \item $g \sim h$ in $\Cay(G, C)$ if and only if $gh^{-1} \in C$
      \end{itemize}

      Since $C$ is inverse closed, $gh^{-1} \in C \iff hg^{-1} \in C$
      so that $\Cay(G, C)$ is undirected.

      Furthermore, if $1_G \not\in C$, then $g \not\sim g$ so that the graph is
      loopless.  (By definition it already lacks parallel edges.)
    \end{defn}

    Because Cayley graphs are defined from groups using only the group
    structure, it is intuitive that these graphs should be highly symmetric.
    For example, every Cayley graph is \textsc{vertex transitive}: for every
    pair $u, v$ in the vertex set, there is a automorphism taking $u \mapsto v$.
    To see this, note that $G$ acts (\ref{group-action}) on $\Cay(G, C)$ through
    the group operation, since the vertex set is also the group.
    To verify that this is a homomorphism, if $u \sim v$ in $\Cay(G, C)$, then
    $$
      uv^{-1} \in C
      \implies (ug)(vg)^{-1} = ugg^{-1}v^{-1} = u 1 v^{-1} = u v^{-1} \in C
    $$
    so that $ug \sim vg$.
    Then, $\Cay(G, C)$ is clearly vertex transitive if $G$ acts transitively,
    and for any vertices $u, v$, the group element $u^{-1} v$ maps $u$ to $v$.
    Moreover, this action is free, since if $ug = u$, then the group
    cancellation law implies that $g$ is trivial.
    Together, this implies that the action of $G$ is regular,
    which suggests the following lemma which provides a characterization of
    Cayley graphs.

    (This action is also faithful since for $g \neq h \in G$, the vertex $1$
    gets mapped to $g$ and $h$ respectively, which are unequal.  However this
    observation irrelevant for this lemma, since it restricts to the action of
    an automorphism group, which is automatically faithful.)

    \begin{lem}\label{regular-aut-cayley}
      For a graph $\Gamma$, there exists a subgroup $G \leq \Aut \Gamma$
      which acts regularly on $\Gamma$
      if and only if $\Gamma \cong \Cay(G, C)$
      for some inverse-closed $C \subseteq G$.
    \end{lem}

    Since the above argument demonstrates the reverse implication,
    only the forward direction will be shown here.

    Before beginning the proof, it will be worthwhile to note the neighbours
    of $1_G$ in $\Cay(G, C)$: $g \sim 1_G$ precisely when $g1^{-1} = g \in C$.

    \begin{proof}
      Choose a vertex $v \in V(\Gamma)$ to identify with $1_G$.  (This choice
      will not matter in the end, as Cayley graphs are vertex transitive.)
      Since the action is regular, for each $u \in V(\Gamma)$ there exists a
      unique $g_u \in G$ such that $vg_u = u$ (\ref{regular-unique}).

      Then define
      $$
        C := \buildset{g \in G}{vg \sim v}
      $$
      and observe that for $u, w \in V(\Gamma)$,
      $u g_u^{-1} = v$, and $w = v g_w \implies w g_u^{-1} = v g_w g_u^{-1}$.
      So, since $g_u^{-1}$ is an automorphism of $\Gamma$,
      $$
        u \sim w
        \iff u g_u^{-1} \sim w g_u^{-1}
        \iff v \sim v g_w g_u^{-1}
        \iff g_w g_u^{-1} \in C
        \ .
      $$

      Therefore, the map $u \mapsto g_u$ is the desired isomorphism $\Gamma \to
      \Cay(G, C)$.
    \end{proof}

    As promised at the beginning of the section, the next lemma demonstrates
    (for graphs) how automorphisms may be used to derive eigenvalues, and
    moreover, their eigenvectors.
    Naively, computing the eigenvalues of a matrix $A$ involves solving its
    characteristic polynomial, which is generically difficult.
    Then for an eigenvalue $\theta$, finding a $\theta$-eigenvector involves
    computing the kernel of $A - \theta I$, which can be computed in polynomial
    time (though not in linear time), and fast numeric algorithms are typically
    inexact.  (TODO citation)

    However, given the right information about a group, the following result
    finds the eigenvectors and eigenvalues almost instantaneously.

    \begin{lem}\label{cayley-eigen}
      Let $G$ be a finite abelian group,
      let $C \subseteq G \setminus \set{1}$ be inverse-closed,
      and define $\Gamma := \Cay(G, C)$.
      Then the rows of the character table of $G$ provide a complete set of
      eigenvectors for the adjacency matrix $A$ of $\Gamma$.
      Specifically, if $\psi$ is a character of $G$ (equivalently, a row of
      its character table), then $\psi(C)$ is the eigenvalue of $\psi$.
    \end{lem}

    \begin{proof}
      Note first that the neighbours $h \sim g$ of a vertex $g \in G$ consist of
      precisely the set $\buildset{cg}{c \in C} = Cg$ since $h \sim g \iff
      hg^{-1} \in C$, and multiplication by $g$ is invertible.

      As in (\ref{character-vector}), characters are identified with row vectors
      such that $\psi(g) \rightsquigarrow \psi_g$.

      Then
      $$
        (A\psi)_g = \sum_{h \in G} A_{g, h} \psi(h)
        = \sum_{h \sim g} \psi(h) = \sum_{c \in C} \psi(cg)
        = \psi(g) \sum_{c \in C} \psi(c) = \psi_g \psi(C)
        \ .
      $$

      Furthermore, since the rows of the character table are orthogonal,
      the eigenvectors $\psi$ are linearly independent,
      and since $G \cong G^*$ (\ref{character-duality}) implies that the character table is square,
      the rows form a basis of eigenvectors for $A$.
    \end{proof}

    TODO How to get real eigenvectors out of this?

  \section{Partitions and Translation Schemes}\label{sec:AS:translation}

    In a sense, this section generalizes the characterization of Cayley graphs
    from the previous section to the setting of association schemes.
    Throughout this section, a transitive, abelian group of automorphisms will
    replace the regular automorphism group which corresponds to a Cayley graph.
    As per (\ref{faithful-transitive-abelian}) the transitive, abelian group
    will act regularly, so that (\ref{regular-aut-cayley}) still applies.
    This motivates the following definition.

    \begin{defn}[Translation Schemes]\label{translation-scheme}
      A \textsc{translation scheme} is an association scheme whose
      automorphism group contains a transitive, abelian subgroup.
    \end{defn}

    \begin{lem}\label{translation-partition}
      If $\AS$ is a translation scheme,
      and $G$ is a transitive, abelian automorphism group,
      then there is a partition into inverse-closed sets
      $C_0, C_1, \ldots, C_d$ of $G$ where $C_0 = \set{1}$,
      and each graph $\Gamma_i$ in $\AS$ is isomorphic to $\Cay(G, C_i)$.
    \end{lem}

    \begin{proof}
      Since $G$ is abelian and is a transitive subgroup of $\Aut \Gamma_i$ for
      each $i = 0, 1, \ldots, d$, $G$ acts regularly on $\Gamma_i$.
      Therefore, by (\ref{regular-aut-cayley}), there exists an inverse-closed
      set $C_i \subseteq G$ such that $\Gamma_i \cong \Cay(G, C_i)$.

      In particular, since the edges of $\Gamma_0$ are the diagonal relation,
      $C_0 = \set{1}$ generates the graph.

      Otherwise, it suffices to show that $C_0, C_1, \ldots, C_d$ partition $G$.
      Recall from the proof of (\ref{regular-aut-cayley}) that any vertex may be
      chosen to identify with $1_G$, so that the same vertex (say, $v$)
      may be chosen for each graph $\Gamma_i$ without loss of generality,
      in which case $C_i$ consists of the neighbours of $v$.  By the definition
      of an association scheme, for each vertex $u$ there is exactly one
      graph $\Gamma_i$ in which $u \sim v$, so that for each vertex, there is
      exactly one $C_i$ containing it.
    \end{proof}

    In order to characterize the translation schemes in a similar manner to the
    Cayley graphs, an examination of partitions of matrices and groups will be
    required.  This will lead to a simple criterion that distinguishes those
    partitions which generate a translation scheme translation scheme from those
    which do not.  \cite[Section~12.10]{godsil}

    \begin{defn}[Partition Matrix]\label{partition-matrix}
      If $\sigma$ is a partition of a set $X$,
      then the \textsc{partition matrix} of $\sigma$
      is the $01$ matrix
      whose rows are indexed by the elements of $X$,
      and whose columns are indexed by the parts of $\sigma$,
      in which each row -- corresponding to $x \in X$ -- has exactly one $1$,
      in the column corresponding to the part that contains $x$.
    \end{defn}

    Any partition matrix may be obtained from an $X \times X$ identity matrix by
    merging the columns which correspond to elements in the same part.
    Note that this implies that the columns are linearly independent.
    (The rows will \textbf{not} be linearly independent unless the partition
    is induced by the diagonal relation.)

    \begin{defn}[Induced Row Partition]\label{induced-row-partition}
      Given a matrix $H$, if $\sigma$ is a partition of the columns with
      partition matrix $\chi(\sigma)$ then the \textsc{induced row partition}
      $\sigma^*$ is the partition of the rows of $H$ such that two rows are in the
      same part if and only if the corresponding rows in $H\chi(\sigma)$ are
      equal.
    \end{defn}

    In other words, if $f$ is the function which maps each row index
    $i$ of $H$ to the row vector $(H\chi(\sigma))_i$, then $\sigma^*$ is the
    partition given by the fibres of $f$.
    \cite[Section 12.7]{godsil}

    \begin{thm}[Bridges and Mena {{\cite[Theorem~12.10.1]{godsil}}}]
      \label{translation-char}
      Let $G$ be a finite abelian group,
      let $\sigma = \set{C_0, C_1, \ldots, C_d}$ be a partition of $G$
      into inverse-closed parts where $C_0 = \set{1}$, and
      let $\sigma^*$ be the induced row partition
      of the character table $H$ of $G$.

      Then $\abs{\sigma^*} \geq \abs{\sigma}$,
      and the graphs $\Gamma_i := \Cay(G, C_i)$ form the classes of an
      association scheme if and only if $\abs{\sigma^*} = \abs{\sigma}$.
    \end{thm}

    \begin{proof}
      Let $A_i$ be the adjacency matrix of $\Gamma_i$,
      and observe that the set $\set{A_0, A_1, \ldots, A_d}$ is linearly
      independent.  This is because the sets $C_i$ partition $G$, and in each
      $\Gamma_i$ the set $C_i$ consists of the neighbours of $1$.
      The fact that the $C_i$ partition $G$ also implies that $\sum_i A_i = J$,
      and since $C_0 = \set{1}$, $A_0 = I$.

      By (\ref{cayley-eigen}), each character $\psi$ of $G$ (i.e. row of $H$)
      is a common eigenvector of $A_0, A_1, \ldots, A_d$,
      with eigenvalue $\psi(C_i)$ at $A_i$.
      Define $\BMA := \Span\set{A_0, A_1, \ldots, A_d}$.
      Let $\chi_{C_i}$ be the characteristic vector of $C_i$ in $G$,
      and let
      \begin{equation}\label{part-mat-sigma}
        \chi(\sigma) =
        \begin{bmatrix}
          \vertbar   & \vertbar   &        & \vertbar   \\
          \chi_{C_0} & \chi_{C_1} & \cdots & \chi_{C_d} \\
          \vertbar   & \vertbar   &        & \vertbar   \\
        \end{bmatrix}
      \end{equation}
      be the partition matrix of $\sigma$.

      Let $D_0, D_1, \ldots, D_e$ be the parts of $\sigma^*$;
      then $i, k$ (or, their characters $\psi^i, \psi^k$)
      belong to the same part $D_j$ precisely when the rows
      $\psi^i \chi(\sigma), \psi^k \chi(\sigma)$ in
      \begin{equation}
        H \chi(\sigma) =
        \begin{bmatrix}
          \horzbar & \psi^1 & \horzbar \\
                   & \vdots &          \\
          \horzbar & \psi^n & \horzbar \\
        \end{bmatrix}
        \begin{bmatrix}
          \vertbar   & \vertbar   &        & \vertbar   \\
          \chi_{C_0} & \chi_{C_1} & \cdots & \chi_{C_d} \\
          \vertbar   & \vertbar   &        & \vertbar   \\
        \end{bmatrix}
      \end{equation}
      are equal.
      Together, the characters of each $D_j$ span a common eigenspace of the
      $A_i$: let $F_j$ be the orthogonal projection matrix onto this subspace.

      Define $\mathbb{F} := \Span\set{F_0, F_1, \ldots, F_e}$.
      Since the $\col F_j$ are spanned by disjoint sets of characters,
      the subspaces are orthogonal and the $F_j$ are linearly independent;
      since together the characters span $\C^n$ (where $n$ is the order of
      $G$), the (direct) sum of the subspaces is $\C^n$ as well.  Therefore,
      $$
        I = F_0 + F_1 + \cdots + F_e
        \ .
      $$
      Furthermore, since $\col F_j$ is a common eigenspace for the $A_i$,
      for $i = 0, 1, \ldots, d$ and $j = 0, 1, \ldots, e$ there exist constants
      $P_i(j)$ such that
      \begin{equation}\label{ts-P-defn}
        A_i F_j = P_i(j) F_j
        \implies A_i = \sum_{j = 0}^e P_i(j) F_j
        \implies \BMA \leq \mathbb{F}
        \ .
      \end{equation}
      This implies that
      $$
        \abs{\sigma} = d = \dim\BMA \leq \dim\mathbb{F} = e = \abs{\sigma^*}
        \ .
      $$

      Note that the $F_0, F_1, \ldots, F_e$ are orthogonal idempotents,
      so they are closed under the regular matrix product.
      This implies that the algebra they generate is simply $\mathbb{F}$.
      On the other hand, while $A_0, A_1, \ldots, A_d$ are orthogonal
      idempotents with respect to the \textit{Schur product},
      they may generate an algebra with the usual product
      that is strictly larger than $\BMA$ --
      it must, however, be contained in $\mathbb{F}$.
      We will show that these two algebras are actually equal.
      In this case, $e = d$ if and only if $\BMA$ is closed under regular matrix
      multiplication; given the results above, this will then be true if and
      only if $A_0, A_1, \ldots, A_d$ forms an association scheme.
      \\

      From (TODO reference) and (\ref{ts-P-defn}),
      if $g(x)$ is any polynomial, then $g(A_i) = \sum_j g(P_i(j)) F_j$.
      In particular, if $x_0^{s_0} x_1^{s_1} \cdots x_d^{s_d}$ is any monomial,
      $A_i^{s_i} = \sum_j P_i(j)^{s_i} F_j$ as above,
      so that evaluating the monomial at $(A_0, A_1, \ldots, A_d)$ yields
      $$
        A_0^{s_0} A_1^{s_1} \cdots A_d^{s_d}
        = \prod_i \sum_j P_i(j)^{s_i} F_j
        = \sum_j \(\prod_i P_i(j)^{s_i}\) F_j
      $$
      since the $F_j$ are orthogonal idempotents.
      Since any polynomial $g$ in $d+1$ variables is a linear combination of such
      monomials, it follows that
      \begin{alignat*}{2}
        g(A_0, A_1, \ldots, A_d)
        &= \sum_j g(P_0(j), P_1(j), \ldots, P_d(j)) F_j \\
        \implies
        g(A_0, A_1, \ldots, A_d) F_j
        &= g(P_0(j), P_1(j), \ldots, P_d(j)) F_j \\
      \end{alignat*}
      for all $j = 0, 1, \ldots, e$.

      Now let $P$ be the $(e+1) \times (d+1)$ matrix such that $P_{ji} =
      P_i(j)$.  Note that the rows of $P$ are precisely the distinct rows of $H
      \chi(\sigma)$, so that for any two rows $j \neq j'$ of $P$, there exists a
      column $i(j, j')$ such that $P_{i(j, j')}(j) \neq P_{i(j, j')}(j')$.
      This allows for the definition of the polynomials
      $$
        g_j(x_0, x_1, \ldots, x_d) :=
        \prod_{j' \neq j} \( x_{i(j, j')} - P_{i(j, j')}(j') \)
      $$
      so that when applied at $A_0, A_1, \ldots, A_d$,
      \begin{alignat*}{2}
        g_j(A_0, A_1, \ldots, A_d) F_{j''} &:=
        \prod_{j' \neq j} \( P_{i(j, j')}(j'')
          - P_{i(j, j')}(j') \) F_{j''} \\&=
        g_j\(P_0(j''), P_1(j''),
          \ldots, P_d(j'')\) F_{j''}
        \ . \\
      \end{alignat*}
      By construction, if $j'' = j$, then
      $f_j := g_j\(P_0(j''), P_1(j''),
        \ldots, P_d(j'')\)
      $ will be non-zero, but if $j'' \neq j$,
      then there will be some $j' = j''$ at which
      $P_{i(j, j')}(j'') - P_{i(j, j')}(j') = 0$ so that
      $g_j\(P_0(j''), P_1(j''),
        \ldots, P_d(j'')\) = 0
      $.

      This construction demonstrates that for each $j$,
      there exists a polynomial $g_j$ such that
      $$
        g_j(A_0, A_1, \ldots, A_d)
        = \sum_{j'} f_j \delta_{jj'} F_{j'}
        = f_j F_j
      $$
      where $f_j \neq 0$, so that $F_j$ can be written as a polynomial in $A_0,
      A_1, \ldots, A_d$.  This proves that each $F_j$ is contained in the
      algebra generated by $\BMA$, and so all of $\mathbb{F}$ is contained in
      this algebra.  Since the reverse inclusion was already
      shown, this proves that the two algebras are equal, as desired.
    \end{proof}

    It is interesting to note that, while the character table $H$ may in general
    be complex, each of the orthogonal projection matrices $F_j$ is real.  To
    see this, note that the $A_i$ are real, symmetric matrices, so that their
    eigenvalues $P_i(j)$ are real as well.  Then, each of the polynomials $g_j$
    (used to express $F_j$ in the algebra generated by the $A_i$) must also be
    real, since they were defined in terms of the $P_i(j)$.  Therefore, not only
    are the $\C$-algebras of $\BMA$ and $\mathbb{F}$ equal, but so are their
    $\R$-algebras.
    \\

    With this result, translation schemes are characterized by a finite abelian
    group $G$ and partition $\sigma$ satisfying the condition given.  Moreover,
    a finite abelian group $G$ is isomorphic to its groups of characters, $G^*$
    (\ref{character-duality}), and the group of characters is completely
    described by the character table $H$.  Likewise, the group partition
    $\sigma$ is completely described by its partition matrix $\chi(\sigma)$.
    Since the condition in (\ref{translation-char}) depends only on these two
    matrices (both with respect to the same ordering on $G$), if it is
    satisfied, then the matrices completely describe the translation scheme they
    generate.

  \section{The Eigenvalues of the Hamming Scheme}
    The theory just developed in the previous section can be applied immediately
    to the Hamming scheme.  This scheme is of great utility in the setting
    of coding theory, in part because it is a $P$-polynomial scheme, generated
    by the distance-regular Hamming graph.  Moreover, it is also a translation
    scheme, with respect to a particularly nice group, and a simple partition,
    which will allow us to deduce a formula for the eigenvalues of the scheme.
    In the particular case of the Hamming graph, an explicit expression for its
    eigenvalues can be given.

    To this end, let $\Zq$ denote the cyclic group of order $q$ (written additively),
    and consider the direct product $\Zqd$ with subsets
    $$
      C_i := \buildset{x \in \Zqd}{\text{there are exactly $i$ $0$'s in $x$}}
      \ .
    $$
    In particular, $C_0 = \set{0}$ and
    $$
      C_1 = \Zqz \times \set{0}^{d - 1}
      \ \bigsqcup\ \set{0} \times \Zqz \times \set{0}^{d - 2}
      \ \bigsqcup\ \cdots \quad\bigsqcup\
      \set{0}^{d - 1} \times \Zqz
      \ .
    $$

    Then $x, y$ are $i^\text{th}$ associates if and only if $x - y \in C_i$;
    that is, the Hamming distance between $x$ and $y$ is $i$,
    so that this partition yields the Hamming scheme.
    In particular, $H(d, q) \cong \Cay(\Zqd, C_1)$.

    From the proof that $\Z_q^d \cong \(\Z_q^d\)^*$ (\ref{character-duality}), if
    $\omega$ is a $q^\text{th}$ complex root of unity, we see that we can
    identify characters $\psi$ with row vectors of the form
    $$
      \psi \rightsquigarrow
      \begin{bmatrix}
        \omega^{\psi_1} & \cdots & \omega^{\psi_d}
      \end{bmatrix}
    $$
    where $\psi_i \in \set{0, 1, \ldots, q - 1}$, and
    $$
      \psi(x) = \prod_{i=1}^d \omega^{\psi_i x_i} \ .
    $$
    (Note that this notation deviates from (\ref{character-vector}),
    but will be more convenient for this purpose.)
    As in the proof of (\ref{character-duality}), let $e_i$ denote the tuple of
    all zeroes, with a $1$ in the $i^\text{th}$ position, so that each group
    element $x = (x_1, \ldots, x_d)$ can be expressed as $\sum_{i=1}^d x_i e_i$.

    Then, in the $i^\text{th}$-distance Hamming graph,
    $\psi$ is a $\psi(C_i)$-eigenvalue (\ref{cayley-eigen}),
    and for the Hamming graph, it can be computed directly.
    \begin{alignat*}{3}
      \psi(C_1)
      &= \sum_{c \in C_1} \psi(c) \\
      &= \sum_{i=1}^d \sum_{j=1}^{q-1} \psi(j e_i)
      \quad\text{
        here, the outer sum picks which entry of $c$ will be non-zero} \\
      &\qquad\qquad\qquad\qquad\text{
        \ and the inner sum picks the value
      }\\
      &= \sum_{i=1}^d \sum_{j=1}^{q-1} \omega^{\psi_i j}
      = \sum_{i=1}^d \sum_{j=1}^{q-1} \(\omega^{\psi_i}\)^j \\
      &= \sum_{i=1}^d
        \(\frac{1 - \(\omega^{\psi_i}\)^q}{1 - \omega^{\psi_i}} - 1\)
      \quad\text{using the usual formula for geometric sums} \\
      &= \sum_{i=1}^d (q-1)\delta_{0, \psi_i} - d \\
      &= (q-1)\(\text{the number of $i$ with $\psi_i = 0$}\) - d \\
    \end{alignat*}

    Since the number $k$ of indices $i$ for which $\psi_i = 0$
    can vary from $0, 1, \ldots, d$,
    and there are $\binom{d}{k}$ places $i$ at which $\psi_i = 0$
    and $(q - 1)^{d - k}$ choices for the other $\psi_j$,
    the eigenvalues of the Hamming graph are given
    \begin{equation}
      \begin{cases}
        qk - d & k = 0, 1, \ldots, d \\
        \binom{d}{k}(q - 1)^{d - k} & \quad\text{(multiplicity)} \\
      \end{cases} \ .
    \end{equation}

    For the other graphs in the Hamming scheme, their eigenvalues can be
    computed as $\psi(C_i)$, although there may not be a particularly nice
    expression for this sum.

\chapter{Delsarte's Linear Programming Bound}\label{ch:LP-bound}
  \section{Linear Programming}\label{sec:LP-bound:LP}
    \begin{itemize}
      \item Basics of Linear Programming -- done
      \item Duality -- done
      \item Algorithms?
    \end{itemize}

    The terminology and results from this section,
    except for the adjective \textit{principal} for
    constraints, follows from \cite{matousek}.

    A \textsc{linear programming problem} (or \textsc{linear program})
    is an optimization problem in which one seeks to maximize or minimize
    a linear function of one or more variables, subject to linear constraints.
    That is, fixing a vector $c$, one tries to maximize or minimize
    the linear combinations of the components of $c$:
    $$
      c_1 x_1 + \cdots + c_n x_n = c^T x
    $$
    for some $x$.
    Note that maximizing $c^T x$ is equivalent to minimizing $(-c)^T x$,
    so that for the theory of linear programming,
    it suffices to consider maximization problems without loss of generality.
    As in other optimization problems,
    the function to be maximized ($c^T x$ in this case)
    is called the \textsc{objective (function)}.

    In most cases, there will be constraints on the inputs to the objective
    function, and for the purposes of linear programming these will also have
    to be linear.
    That is, there will be a matrix $A$ and vector $b$
    such that only inputs $x$ satisfying $Ax \leq b$ will be allowed.
    (Note that for \textit{vectors} $a$ and $b$,
    $a \leq b$ will mean that each component $a_i$
    is less than or equal to the corresponding component $b_i$.)
    These are called the \textsc{(principal) constraints},
    and vectors $x$ which satisfy the constraints will be called
    \textsc{feasible (solutions)}.
    (Note that in \cite{delsarte}, the term \textit{program}
    is used to refer to a feasible solution.)

    If there are no constraints on the problem
    (and even in some cases where there are)
    through appropriate choices of feasible solution $x$,
    the objective $c^Tx$ may be made arbitrarily large,
    and such problems are called \textsc{unbounded}.
    Conversely, if no feasible solutions exist,
    then the problem is called \textsc{infeasible}.

    Finally, in most applications of linear programming --
    in particular to the cliques of association schemes --
    the feasible solutions will be further constrained
    to those with all non-negative components (i.e. $x \geq 0$).
    These are called the \textsc{non-negativity constraints},
    in contrast with the \textit{principal constraints}.
    The non-negativity constraints will be required throughout the remainder of
    this report.

    Therefore, for an objective $c^T x$ and constraints $Ax \leq b$,
    the associated linear program will be written in \textsc{standard form}:
    $$
      \max\buildset{
        c^T x
      }{
        A x \leq b,\
        x \geq 0
      }
      \ .
    $$

    \subsection{Duality}

      The most important observation about linear programs
      (for the purposes of this report, at least)
      is that they come in dual pairs.

      Given a linear program $\mathcal{P}$ written in standard form
      $$
        \max\buildset{
          c^T x
        }{
          A x \leq b,\
          x \geq 0
        }
      $$
      its \textsc{dual} program is $\mathcal{P}^*$:
      $$
        \min\buildset{
          b^T y
        }{
          A^T y \geq c,\
          y \geq 0
        }
        \ .
      $$
      Re-writing it in standard form,
      $$
        \max\buildset{
          (-b)^T y
        }{
          -A^T y \leq -c,\
          y \geq 0
        }
      $$
      taking the dual
      $$
        \min\buildset{
          -c^T x
        }{
          -A x \geq -b,\
          x \geq 0
        }
      $$
      and re-writing in standard form
      $$
        \max\buildset{
          c^T x
        }{
          A x \leq b,\
          x \geq 0
        }
      $$
      the original (called \textsc{primal}) linear program is recovered.

      This demonstrates that $\(\mathcal{P}^*\)^* = \mathcal{P}$,
      so that linear programs come in dual pairs.

      \begin{thm}[Weak Duality]\label{weak-duality}
        If $x$ is a feasible solution to a linear program
        $$
          \max\buildset{
            c^T x
          }{
            A x \leq b,\
            x \geq 0
          },
        $$
        and $y$ is a feasible solution to its dual program,
        $$
          \min\buildset{
            b^T y
          }{
            A^T y \geq c,\
            y \geq 0
          },
        $$
        then $c^T x \leq b^T y$.
      \end{thm}

      \begin{proof}
        Let $u, v, w$ be vectors with $u \geq 0$, and $v \leq w$.
        Then for all components $i$,
        $u_i \geq 0$ and $v_i \leq w_i$ implies that $u_i v_i \leq u_i w_i$
        so that
        $$
          u^T v = \sum_i u_i v_i
          \leq \sum_i u_i w_i = u^T w
          \ .
        $$

        In particular, since $y$ is a feasible solution to the dual program,
        and $x \geq 0$,
        $$
          c \leq A^T y
          \implies x^T c \leq x^T A^T y = y^T A x
          \ .
        $$
        (Here one may take the transpose of the whole expression,
        since the result is a scalar.)
        Similarly, since $x$ is a feasible solution to the primal program,
        and $y \geq 0$,
        $$
          b \geq A x
          \implies y^T b \geq y^T A x
          \ .
        $$
        By combining the two inequalities,
        $$
          b^T y = y^T b \geq y^T A x \geq x^T c = c^T x
        $$
        which is the desired result.
      \end{proof}

      As a result of the weak duality of linear programs,
      every feasible solution to the dual program
      provides an upper bound on the maximum of the primal,
      and every feasible solution to the primal program
      provides a lower bound on the minimum of the dual.

      \mysep

      In fact the extremal values of dual programs
      (the maximum of the primal, and the minimum of the dual)
      coincide, although this will not be needed for the purposes of this
      report.
      This is referred to as \textit{Strong Duality} of linear programs.

  \section{The LP Bound}\label{sec:LP-bound:LP-bound}

    In this section, we will consider a symmetric association scheme $\AS$ with
    vertex set $V$, relations $\Gamma_0, \Gamma_1, \ldots, \Gamma_d$, adjacency
    matrices $A_0, A_1, \ldots, A_d$, and eigenmatrices $P$ and $Q$.  (Since we
    will assume that $\AS$ is symmetric, this will guarantee that $P$ and $Q$
    are real.)  We will denote by $D$ the set of distances $\set{0, 1, \ldots,
    d}$, and consider a subset $Y$ of the vertex set whose distances (to one
    another) belong to a subset of $D$.  Intuitively, we think of $Y$ as a
    \textit{code} in $V$; if the minimum distance of the code is greater than
    $1$, then this restricts the possible distances between codewords.

    \begin{defn}[Cliques and Cocliques {{\cite[Section 3.3]{delsarte}}}]
      \label{defn:AS-clique}
      Let $Y \subseteq V$, and $C \subseteq D$, where $0 \in C$.
      Then $Y$ is called a \textsc{$C$-clique} if
      $$
        \Gamma_i \cap Y^2 = \emptyset \quad \forall i \in D \setminus C
        \ .
      $$
      Equivalently, $Y$ is a $C$-clique if for all $x, y \in Y$,
      $$
        (x, y) \in \Gamma_i \implies i \in C
        \ .
      $$
      Let $C^* := C \setminus \set{0}$, and $\bar{C} = D \setminus C^*$.
      Then $Y$ is a $C$-clique if and only if it is a
      \textsc{$\bar{C}$-coclique}.

      (Note that such a set $Y$ is called a \textit{$C$-code},
      or \textit{$\bar{C}$-anticode} in \cite{godsil}.)
    \end{defn}

    Then a code of minimum distance $\delta$ corresponds to a $\set{0, \delta,
    \ldots, d}$-clique.  The terminology derives from the fact that, if the
    association scheme is generated by a distance-regular graph, then cliques in
    the graph correspond to $\set{0, 1}$-cliques; cocliques correspond to
    $\set{0, 1}$-cocliques.  (Recall that a \textit{clique} in a graph is a set
    of vertices such that every pair is adjacent; a \textit{coclique}, or
    \textit{independent set}, is a set of vertices such that no pair is
    adjacent.)

    However, the notion of $C$-cliques is a very strict one: that no pair of
    vertices is $i$-associated for $i \not\in C$.  In order to measure arbitrary
    subsets of the vertex set, we will need to consider a more general indicator
    of the distribution of distances within the subset.

    \begin{defn}[Inner Distribution {{\cite[Section
      3.1]{delsarte}}}]\label{defn:inner-distribution}
      The \textsc{inner distribution} of a subset $Y$ of vertices is the vector
      $y$ defined by
      $$
        y_i := \frac{ \Gamma_i \cap Y^2 }{ \abs{Y} }
      $$
      for $i = 0, 1, \ldots, d$.  
    \end{defn}

    Note that if $\varphi_Y$ is the characteristic vector of $Y$ in $V$, then
    for all $i$,
    $$
      y_i = \frac{ \varphi_Y^T A_i \varphi_Y }{ \abs{Y} } \ .
    $$
    Furthermore, $y_0 = 1$ for all subsets, $Y$, and $\sum_{i = 0}^d y_i =
    \abs{Y}$.  

    By restating the above observation, that $\vone^T y = \sum_i y_i = \abs{Y}$,
    we recognize a linear function which gives the size of a code.  For a given
    minimum distance, we generally try to maximize the size of a code with that
    minimum distance.  This setup should be reminiscent of a linear programming
    problem.  All that remains is to find a linear inequality to constrain the
    possible inner distributions $y$.  However, if $y$ is truly the inner
    distribution of a code of minimum distance $\delta$, then we know that $y_i
    = 0$ for $i = 1, \ldots, \delta - 1$.  This will lead to an inequality we
    can use.

    \begin{thm}[Delsarte Theorem 3.3 \cite{delsarte}]\label{thm:lp-ineq}
      For any inner distribution $y$,
      $$
        Q^T y \geq 0
      $$
      where $Q$ is the matrix of dual eigenvalues.
      (Here, $x \geq 0$ means that each component of
      the vector $x$ is not less than $0$.)
    \end{thm}

    We will prove this theorem after discussing how it may be used to bound the
    size of a code in a graph.

    This theorem provides the key inequality that will allow the application of
    linear programming to cliques in association schemes.
    However, because the constraint vector in a primal linear program
    becomes the objective in the dual program,
    this inequality will require some transformation to make it suitable for use
    in linear programming.

    Let $Y$ be an $C$-clique with inner distribution $y$.
    Then $y_i = 0$ for all $i \not\in C$,
    so $Q^T y \geq 0 \iff Q^T \diagC y \geq 0$
    since the action of $\diagC$ acting on the left
    is to zero out the \textit{rows} of $y$
    with index not in $C$.
    Similarly,
    $$
      Q^T \diagC y
      = Q(0)^T y_0 + Q^T \diagCs y
      = \mu + Q^T \diagCs y
    $$
    since the action of $\diagCs$ on the right
    is to zero out the \textit{columns} of $Q^T$
    with index not in $C^*$,
    $y_0 = 1$, and
    $$
      Q^T =
      \begin{bmatrix}
        1 & 1 & \cdots & 1 \\
        \mu_1 & & & \\
        \vdots & & * & \\
        \mu_d & & & \\
      \end{bmatrix}
      \ .
    $$
    Finally, since $y \geq 0$, $Q_0^T y \geq 0$ adds no new constraint,
    so that under the non-negativity constraint
    $Q^T y \geq 0 \iff \diagDs Q^T y \geq 0$.

    Putting all this together,
    $Q^T y \geq 0 \iff \diagDs Q^T \diagCs y \geq -\mu$
    so that Delsarte's LP can be written in standard form:
    \begin{alignat}{2}
      & \max\buildset{
        \vone^T \diagC y
      }{
        Q^T \diagC y \geq 0,\
        y \geq 0,\
        y_0 = 1
      } \label{eqn:dlp-primal} \\
      =& \max\buildset{
        \chiCs^T y
      }{
        - \diagDs Q^T \diagCs y \leq \diagDs \mu,\
        y \geq 0
      } + 1 \label{eqn:dlp-primal-std}
      \ .
    \end{alignat}

    Taking the dual yields
    \begin{alignat}{2}
      & \min\buildset{
        \mu^T \diagDs z
      }{
        - \diagCs Q \diagDs z \geq \chiCs,\
        z \geq 0
      } + 1 \label{dlp-dual-std} \\
      =& \min\buildset{
        \mu^T z
      }{
        - \diagCs Q \diagDs z \geq \chiCs,\
        z \geq 0,\
        z_0 = 1
      }
      \ .
    \end{alignat}
    Therefore, if $z_0 = 1$ is required,
    recalling that $Q_0 = \vone$ and $\diagCs \vone = \chiCs$, then
    \begin{alignat*}{2}
      & \diagCs Q \diagD z \\
      =& \diagCs \( Q_0 z_0 + Q \diagDs z \) \\
      =& \chiCs + \diagCs Q \diagDs z \\
      \leq& 0
      \ .
    \end{alignat*}
    This equivalence recovers Delsarte's formulation of
    the dual linear program:
    \begin{equation}\label{eqn:dlp-dual}
      \min\buildset{
        \mu^T z
      }{
        \diagCs Q z \leq 0,\
        z \geq 0,\
        z_0 = 1
      }
      \ .
    \end{equation}
    \\

    Therefore, given an association scheme with dual matrix of eigenvalues $Q$,
    if we wish to find an upper bound on the size of a code with minimum
    distance $\delta$, then we set $C := \set{0, \delta, \ldots, d}$, and solve
    the linear program in Equation (\ref{eqn:dlp-primal}).  Alternatively, any
    feasible solution to the dual problem (\ref{eqn:dlp-dual}) will provide a
    (possibly worse) upper bound.


    \mysep

    All that remains is to prove Theorem (\ref{thm:lp-ineq}).  To do this, we
    will begin by introduction a different measure of the distribution of
    distances in a subset of vertices.

    \begin{defn}[Outer Distribution {{\cite[Section~3.1]{delsarte}}}]
      \label{defn:outer-distribution}
      The \textsc{outer distribution} of $Y$ is the $V \times D$ matrix $B$
      whose $(v, i)$-entry is defined by
      $$
        B(v, i) := \abs{ \Gamma_i \cap \( \set{v} \times Y \) } \ .
      $$
    \end{defn}

    The outer distribution is also given in block-column form:
    $$
      B = \begin{bmatrix}
        A_0 \varphi_Y & A_1 \varphi_Y & \cdots & A_d \varphi_Y
      \end{bmatrix} \ .
    $$

    \begin{lem}[Delsarte Theorem 3.1 \cite{delsarte}]\label{lem:BTB}
      Let $Y$ be a subset of vertices with inner distribution $y$ and outer
      distribution $B$.  Let $P$ and $Q$ be the eigenmatrices of the association
      scheme.  Then
      $$
        B^T B = \frac{\abs{Y}}{\abs{V}} P^* \diag{Q^T y} P \ .
      $$
    \end{lem}

    \begin{proof}
      Note that the $i^\text{th}$ column of $B$ is given by $A_i \varphi_Y$, so
      we can compute the $(i, j)$-entry of $B^T B$ in the following manner:
      \begin{alignat*}{2}
        (B^T B)_{i, j} &= \(\varphi_Y^T A_j^T\) \(A_i \varphi_Y\) \\
        &= \varphi_Y^T \(A_j^T A_i\) \varphi_Y \\
        &= \varphi_Y^T \(\sum_{k=0}^d p_{ij}^k A_k\) \varphi_Y \\
        &= \sum_{k=0}^d p_{ij}^k \varphi_Y^T A_k \varphi_Y \\
        &= \abs{Y} \sum_{k=0}^d p_{ij}^k y_k \\
      \end{alignat*}
      since by definition, $\abs{Y} y_k = \varphi_Y^T A_k \varphi_Y$.

      Next, we define $\tilde{y}^T := y^T Q$ so that
      $$
        \tilde{y}^T P = y^T QP = y^T \abs{V}
        \implies y_k = \frac{1}{\abs{V}}\sum_{r = 0}^d \tilde{y_r} P_k(r) \ .
      $$
      Substituting this sum for $y_k$ in the previous expression, we obtain
      $$
        (B^T B)_{i, j}
        = \frac{\abs{Y}}{\abs{V}} 
          \sum_{k=0}^d p_{ij}^k \sum_{r=0}^d \tilde{y_r} P_k(r)
        = \frac{\abs{Y}}{\abs{V}} 
          \sum_{r=0}^d \tilde{y_r} \sum_{k=0}^d p_{ij}^k P_k(r) \ .
      $$

      Then, by noticing that combining Equations (\ref{tab:prop:product}) and
      (\ref{tab:prop:eigen}) yields the inner sum above:
      $$
        \begin{rcases*}
          A_i F_j = P_i(j) F_j \\
          A_i A_j = \sum_{k=0}^d p_{ij}^k A_k \\
        \end{rcases*} \implies
        P_i(r) P_j(r) = \sum_{k=0}^d p_{ij}^k P_k(r) \ .
      $$
      Therefore,
      $$
        (B^T B)_{i, j}
        = \frac{\abs{Y}}{\abs{V}} \sum_{r=0}^d \tilde{y_r} P_i(r) P_j(r) \ .
      $$

      Finally, we obtain the desired result by noticing that
      $$
        \sum_{r=0}^d \tilde{y_r} P_i(r) P_j(r)
        = P^* \diag{Q^T y} P \ .
      $$
    \end{proof}

    At first, this result may seem to be unrelated to the theorem which we wish
    to prove.  However, by realizing that $B^T B$ is a positive semi-definite
    matrix, Theorem (\ref{thm:lp-ineq}) follows as a quick corrolary.

    \begin{proof}[Proof of Theorem (\ref{thm:lp-ineq})]
      From the previous lemma (\ref{lem:BTB}), we know that
      $$
        B^T B = \frac{\abs{Y}}{\abs{V}} P^* \diag{Q^T y} P \ .
      $$
      Since $PQ = \abs{V} I$ (\ref{tab:prop:inverse}), we can rewrite the above
      equation as
      $$
        Q^* B^T B Q = \abs{Y}\abs{V} \diag{Q^T y} \ .
      $$
      Then, the $(k, k)$ entry of this matrix is
      $$
        (Q^T y)_k = Q_k^T (B^T B) Q_k \geq 0
      $$
      since $B^T B$ is positive semi-definite, and $Q$ is real.  Therefore, 
      $$
        Q^T y \geq 0 \ .
      $$
    \end{proof}

  \section{The Ratio Bound}\label{sec:LP-bound:ratio-bound}
    
    From the LP bound, we can quickly derive an eigenvalue bound on the size of
    a clique in a distance-regular graph.  We do this by finding a feasible
    solution to the dual problem (\ref{eqn:dlp-dual}).

    \begin{thm}[Ratio Bound for Cliques
      {{\cite[Section~3.3.2]{delsarte}}}]\label{thm:ratio-clique}
      Let $Y$ be a clique in a distance-regular graph.  Let $r$ be the greatest
      and $\tau$ the least eigenvalue of the adjacency matrix of the graph.
      Then
      $$
        \abs{Y} \leq 1 - \frac{r}{\tau} \ .
      $$
    \end{thm}

    \begin{proof}
      Let $P$ and $Q$ be the eigenmatrices of the association scheme generated
      by the graph, and let $\nu$ and $\mu$ be the vectors of valencies and
      multiplicities respectively.  Note that the $1^\text{th}$ column of $P$,
      $P_1$ contains the eigenvalues of the adjacency matrix.  We will sort the
      rows of $P$ such that $P_1$ is sorted in descending order.  Note also that
      the valency $\nu_1$ is the largest eigenvalue (\ref{tab:prop:modulus}), so
      $\nu_1 = r$.  Therefore, we can write
      $$
        P^T = \begin{bmatrix}
          1      & \cdots & 1    \\
          \nu_1  & \cdots & \tau \\
          \vdots & *      &      \\
          \nu_d  &        &      \\
        \end{bmatrix} \ .
      $$

      Define the vector
      $$
        z := \begin{bmatrix}
          1 & 0 & \cdots & \frac{-r}{\mu_d \tau}
        \end{bmatrix}^T \ .
      $$
      Note that since the trace of an adjacency matrix is zero, and that the
      trace of any diagonalizable matrix is the sum of its eigenvalues, since
      $r$ is positive, $\tau$ must be negative.  Therefore, $z \geq 0$ and $z_0
      = 1$.  So if $z$ satisfies $\diagCs Q z \leq 0$, then it will be a
      feasible dual solution; furthermore, $\mu^T z = 1 - r / \tau$, which is
      the desired bound.  Since we are looking for an upper bound on the size of
      a clique, we set $C := \set{0, 1}$.

      Using Equation (\ref{tab:prop:similar}), we compute
      \begin{alignat*}{2}
        \diagCs Q z &= \diagCs \diagnu^{-1} P^T \diagmu z \\
        &= \begin{bmatrix}
          \frac{\nu_1}{\nu_1} & \cdots & \frac{\tau}{r}
        \end{bmatrix}
        \begin{bmatrix}
          1 \\ \vdots \\ \frac{-r}{\mu_d \tau}
        \end{bmatrix} \\
        &= 1 - 1 = 0 \leq 0
      \end{alignat*}
      as desired.
    \end{proof}

  \section{The Clique-Coclique Bound}\label{sec:LP-bound:clique-coclique-bound}

    Similar to the bound found in the previous section, by finding a feasible
    solution to the dual linear program, we will obtain an upper bound on the
    size of a $C$-clique.

    \begin{thm}[Clique-Coclique Bound
      {{\cite[Theorem~3.9]{delsarte}}}]\label{thm:clique-coclique}
      Let $Y$ be a $C$-clique and $Z$ be a $C$-coclique (i.e. a
      $\bar{C}$-clique) in a symmetric association scheme with vertex set $V$.
      Then
      $$
        \abs{Y} \cdot \abs{Z} \leq \abs{V} \ .
      $$
    \end{thm}

    \begin{proof}
      Let $y$ and $z$ be the inner distributions of $Y$ and $Z$ respectively,
      let $\mu$ be the vector of multiplicities,
      and for each $k \in D$ define
      $$
        \zeta_k := \frac{1}{\mu_k  \abs{Z}} \sum_j z_j Q_k(j) \ .
      $$
      We will look for an upper bound on $\abs{Y}$ by demonstrating that $\zeta$
      is a feasible solution to the dual linear program \textit{for $Y$}.

      Then $\zeta = \frac{1}{\abs{Z}} \diagmu^{-1} Q^T z$, and $Q^T z \geq 0$ so
      $\zeta \geq 0$ as well.  At $k = 0$, $\mu_0 = 1$ and $Q_0 = \vone$, so
      $\zeta_0 = \frac{1}{\abs{Z}} \vone z = 1$.

      To prove that $\zeta$ is a feasible dual solution, it remains to show that
      $\diagCs Q \zeta \leq 0$.  However,
      $$
        Q \zeta = \frac{1}{\abs{Z}} Q \diagmu^{-1} Q^T z
        = \frac{\abs{V}}{\abs{Z}} \diagnu^{-1} z
      $$
      using Equation (\ref{tab:prop:valency-multiplicity}).  Since $z_i = 0$ for
      all $i \in C$ (since $Z$ is a $C$-coclique), it follows that $\diagCs Q
      \zeta = 0 \leq 0$ as desired.

      Then to compute the upper bound, we compute
      $$
        \mu^T \zeta = Q(0) \zeta
        = \( \frac{\abs{V}}{\abs{Z}} \diagnu^{-1} z \)_0
        = \frac{\abs{V}}{\abs{Z}}
      $$
      which implies that $\abs{Y} \leq \abs{V} / \abs{Z}$ as desired.
    \end{proof}

\chapter{Schrijver's Semi-Definite Programming Bound}\label{ch:SDP-bound}

  Delsarte's linear programming bound results from the discovery of an object
  which depends on a given code (the inner distribution) in such a way that the
  size of the code can be computed from the object.  On the other hand, all of
  these objects resulting from such codes satisfied a certain property (an
  inequality, derived from the fact that a certain matrix is positive
  semi-definite).  Therefore, we may put an upper bound on the size of certain
  codes by maximizing the object in question, subject to its constraints.

  In this chapter, we will investigate an improved bound on the size of a code
  in a binary Hamming graph $H(d, 2)$ by following the outline above.  Where
  previously the objects were non-negative vectors satisfying a linear
  inequality, now the objects will be positive-definite matrices satisfying
  linear equalities.  In both cases, the objective will be to maximize a linear
  function of the objects in question.

  \section{Semi-Definite Programming}\label{sec:SDP-bound:SDP}

    For a given distance-regular graph, and a minimum distance, Delsarte's LP
    bound is the result of a maximization problem based on the input parameters.
    There is no general solution to this problem, and so there is no explicit
    formula for the bound for all distance-regular graphs and minimum distances.
    While some bounds can be found be specifying a feasible dual solution, as
    done in Sections \ref{sec:LP-bound:ratio-bound} and
    \ref{sec:LP-bound:clique-coclique-bound}, these are not guaranteed to be
    optimal.

    Similarly, Schrijver's SDP bound is computed by maximizing a semi-definite
    program built from the input graph and minimum distance.  To this end, this
    section will discuss some aspects of semi-definite programming.
    \\

    We will use the notation $S \succeq 0$ to assert that the matrix $S$ is
    positive semi-definite, and $S \succeq T$ to assert that $S - T \succeq 0$.
    Given an objective matrix $C$, constraint matrices $K_i$, and constraint
    values $b_i$ for $i = 1, \ldots, m$, the semi-definite programming problem
    is to maximize the following constrained problem
    $$
      \max\buildset{\innerprod{C}{X}}{
        \innerprod{K_i}{X} = b_i \ ,
        X \succeq 0
      } \ .
    $$

    In fact, semi-definite programs are a strict generalization of linear
    programs.  Given a linear program,
    $$
      \max\buildset{
        c^T x
      }{
        kx \leq b,\
        x \geq 0
      }
    $$
    the inequality constraint can be re-expressed as an equality constraint $kx'
    = b$ by adding an extra variable to $x$ for each row of $k$ (these variables
    are called \textit{slack} variables).  If each new variable $s$ is
    constrained to be non-negative, then for each row $k_i$ in $k$,
    $$
      k_i x + s = b_i \iff kx \leq b_i
    $$
    so that the constraints are equivalent.  By extending the objective $c$ with
    zeroes to match the length of $x'$, the optimal value will also be equal.
    Then, an equivalent semi-definite program can be constructed by associating
    each vector with the diagonal matrix formed by that vector (including the
    rows of $k$):
    \begin{alignat*}{2}
      c           &\leftrightsquigarrow \diag{c} \\
      x \geq 0    &\leftrightsquigarrow \diag{x} \succeq 0 \\
      c^Tx        &\leftrightsquigarrow \innerprod{\diag{c}}{\diag{x}} \\
      k_i         &\leftrightsquigarrow \diag{k_i} \\
      k_i x = b_i &\leftrightsquigarrow \innerprod{\diag{k_i}}{\diag{x}} = b_i\ .
    \end{alignat*}

    As with linear programming, each semi-definite program is equipped with a
    dual program:
    $$
      \min\buildset{
        - \sum_{i=1}^m y_i b_i
      }{
        -C - \sum_{i=1}^m y_i K_i \succeq 0
      } \ .
    $$
    Furthermore, the weak duality theorem also holds true for semi-definite
    programs.

    \begin{thm}[Weak Duality
      {{\cite[Proposition~5.1]{ocw}}}]\label{thm:SDP-weak-duality}
      If feasible and bounded, the maximum value of the primal semi-definite
      program is at mos the minimum value of the dual program.
    \end{thm}

    \begin{proof}
      Let $X$ be a feasible solution to the primal program, and $y$ a feasible
      solution to the dual.  Then define
      $$
        S := -C - \sum_{i=1}^m y_i K_i \succeq 0 \ ,
      $$
      and compute
      $$
        \innerprod{S}{X}
        = -\innerprod{C}{X} - \sum_i y_i \innerprod{K_i}{X}
        = -\innerprod{C}{X} - \sum_i y_i b_i \ .
      $$
      Then, to prove $\innerprod{S}{X} \geq 0$ is equivalent to proving that
      $-\sum_i y_i b_i \geq \innerprod{C}{X}$ as desired.

      Since both $S$ and $X$ are symmetric, positive semi-definite matrices,
      each can be orthogonally diagonalized, and their eigenvalues must be
      non-negative.  Write
      \begin{alignat*}{2}
        S &= U_S \Delta_S U_S^T \\
        X &= U_X \Delta_X U_X^T \\
      \end{alignat*}
      where $U_S, U_X$ are orthogonal matrices, and $\Delta_S, \Delta_X$ are
      diagonal and non-negative.

      Using the cyclicity of trace, we can compute
      \begin{alignat*}{2}
        \innerprod{S}{X} &= \trace SX \\
        &= \trace U_S \Delta_S U_S^T U_X \Delta_X U_X^T \\
        &= \trace \Delta_S U_S^T U_X \Delta_X U_X^T U_S \\
        &= \sum_i (\Delta_S)_i (U_S^T U_X \Delta_X U_X^T U_S)_i \ .
      \end{alignat*}
      Since $\Delta_S$ and $\Delta_X$ are positive semi-definite, and $U_X^T
      U_S$ is invertible, the conjugated matrix $U_S^T U_X \Delta_X U_X^T U_S$
      is also positive semi-definite, so the sum is too.  This proves the claim.
    \end{proof}

    Unlike with linear programs, the difference between the minimum of the dual
    program and the maximum of the primal may be zero, finite, or infinite.

  \section{The Terwilliger Algebra of the Hamming
    Scheme}\label{sec:SDP-bound:terwilliger}

    This section follows \cite[Subsection I. A]{schrijver} closely.

    Where Delsarte's LP bound used properties of the Bose-Mesner algebra of an
    association scheme, Schrijver's SDP bound will use a \textit{Terwilliger
    algebra}, which is a non-commutative superset of the Bose-Mesner algebra.

    Recall that for any Hamming graph $H(d, 2)$, we typically take the vertex
    set to be the group $\Z_2^d$.  However, this group also forms a ring, with
    entrywise multiplication: in fact, this is an example of a \textit{boolean
    ring}.  One way to view the set $\Z_2^d$ is as the set of functions $[d] \to
    \set{0, 1}$, which has a natural interpretation as the powerset of $[d]$.
    In this interpretation, each function is identified with its support (the
    set of elements in $[d]$ which map to $1$).  Then, the addition in the ring
    corresponds to taking the symmetric difference of sets; the multiplication
    in the ring corresponds to the intersection of sets; the additive identity
    corresponds to the empty set; and the multiplicative identity corresponds to
    the entire set $[d]$.  The union of sets $u$ and $v$ can also be computed as
    $$
      u \cup v = u + v + uv \ .
    $$
    Furthermore, the Hamming weight of a vertex is precisely its cardinality as
    a set.  Both of these interpretations of the vertex set of $H(d, 2)$ -- as a
    ring, and as the powerset of $[d]$ -- will be used throughout the following
    two sections.

    \begin{defn}[Terwilliger Algebra]
      Let $H(d, 2)$ be a binary Hamming graph with vertex set $V$.  For all
      integers $i, j, t \in D$, define the $V \times V$ matrix $\Mijt$ such that
      for all vertices $u$ and $v$,
      $$
        (\Mijt)_{u, v} :=
        \begin{cases}
          1 & \If \wt{u} = i,\ \wt{v} = j,\ \And \wt{uv} = t \\
          0 & \Else
        \end{cases} \ .
      $$
      The algebra generated by the $\Mijt$ is called the \textsc{Terwilliger
      algebra} of $H(d, 2)$.  (In fact, it is the Terwilliger algebra with
      respect to $\vzero$, but this is has no bearing on this report.)
    \end{defn}

    From the definition of the Bose-Mesner algebra, the adjacency matrix $A_k$
    is $1$ wherever $\wt{u+v} = k$, and $0$ elsewhere.  For all $u$ and $v$,
    such that $\wt{u} = i$, $\wt{v} = j$, and $\wt{uv} = t$, then $\wt{u+v} =
    \wt{u} + \wt{v} - 2\wt{uv} = i + j - 2t$, so the set of entries at which
    $\Mijt$ is $1$ is a subset of the entries at which $A_{i+j-2t}$ is $1$.  In
    particular, each $A_k$ is a sum of some $\Mijt$, so that the Terwilliger
    algebra does indeed contain the Bose-Mesner algebra.

    Like the Bose-Mesner algebra, the Terwilliger algebra is closed under
    transposition, since for all $i, j, t$, $(\Mijt)^T = M_{j, i}^t$.  Like the
    adjacency matrices $A_k$, the $\Mijt$ are closed under matrix
    multiplication.  To see this, note that when $j \neq j'$, the product $\Mijt
    M_{j', k}^s$ is zero, and when $j = j'$, and $u$ and $w$ are vertices
    satisfying $\wt{u} = i$, $\wt{w} = k$, and $\wt{uw} = r$,
    \begin{alignat*}{2}
      (\Mijt M_{j', l}^s)_{u, w} &=
        \abs{\buildset{v \in V}{\wt{v} = j,\ \wt{uv} = t,\ \wt{vw} = s}} \\
      &= \sum_{l=0}^r \binom{r}{l} \binom{i-r}{t-l} \binom{k-r}{s-l}
        \binom{d-i-k+r}{j-t-s+l} \ .
    \end{alignat*}
    Since this depends only on the values of $\wt{u}$, $\wt{w}$, and $\wt{uw}$,
    it must be a linear combination of the $M_{i, k}^p$.  However, unlike the
    Bose-Mesner algebra, the Terwilliger algebra is not generally commutative.

    \begin{figure}[h]
      \centering
      \begin{tikzpicture}[
          thick,
          set/.style = {draw, circle, minimum size=5cm},
        ]

        \node[draw,rectangle, minimum width=10cm, minimum height=8.5cm] () at (0:0cm) {};

        \node[set, label={135:$u$}, fill=black!15] (u) at (145:1.7cm) {};
        \node[set, label={ 45:$w$}, fill=black!15] (w) at ( 35:1.7cm) {};
        \node[set, label={-90:$v$}] (v) at (-90:1.0cm) {};

        \draw[      thick] (145:1.7cm) circle(2.5cm);
        \draw[      thick] ( 35:1.7cm) circle(2.5cm);
        \draw[ultra thick] (-90:1.0cm) circle(2.5cm);

        \node () at ( 90:0.3cm) {$\displaystyle \binom{r}{l}$};
        \node () at (200:1.6cm) {$\displaystyle \binom{i-r}{t-l}$};
        \node () at (-20:1.6cm) {$\displaystyle \binom{k-r}{s-l}$};
        \node () at (-90:2.3cm) {$\displaystyle \binom{d-i-k+r}{j-t-s+l}$};
      \end{tikzpicture}
      \caption{
        A term in the $(u, w)$ entry of $\Mijt M_{j, k}^s$.
      }
    \end{figure}

    Like the adjacency matrices, the non-zero $\Mijt$ of the Terwilliger algebra
    are linearly independent, since for any pair of vertices $u, v$, and weights
    $i' := \wt{u}$, $j' := \wt{v}$, and $t' := \wt{uv}$, there will be exactly
    one of the $\Mijt$ which is non-zero.  On the other hand, the number of
    triples $(i, j, t)$ such that $\Mijt$ is non-zero is equal to the number of
    triples $(a, b, t)$ such that there exist disjoint subsets $u', v', w'$ of
    $[d]$ with cardinalities $a$, $b$, and $t$.  This is straightforward to see,
    as every pair of vertices $u$ and $v$ with $\wt{u} = i$, $\wt{v} = j$, and
    $\wt{uv} = t$ can be decomposed into three disjoint sets $u \setminus v$, $v
    \setminus u$, and $uv$, and vice versa.  There exist disjoint subsets $u'$,
    $v'$ and $w'$ for every $a$, $b$, and $t$ as long as $a + b + t \leq d$, and
    the number of solutions to this in non-negative integers is
    $\binom{d+3}{3}$.  Therefore, this is the dimension of the Terwilliger
    algebra.

    In fact, there exists an explicit diagonalization of the Terwilliger
    algebra, such that computations in the algebra may be done with objects of
    dimension $\binom{d+3}{3}$.

  \FloatBarrier
  \section{The SDP Bound}\label{sec:SDP-bound:SDP-bound}

    This section follows \cite[Subsection I. B]{schrijver} closely.

    In order to place an upper bound on the sizes of codes in a binary Hamming
    graph following the recipe outlined at the beginning of this chapter, for
    each code we require a positive semi-definite matrix from which we can
    compute the size of the code.  Furthermore, these objects must satisfy some
    linear (in)equality constraints.

    Therefore, let $Y$ be a code, and let $\lijt$ denote the number of triples
    $(u, v, w)$ in $Y^3$ such that $\wt{u + v} = i$, $\wt{u + w} = j$, and
    $\wt{(u+v)(u+w)} = t$.  Using the multinomial notation
    $$
      \binom{d}{i, j, t} := \frac{d!}{i! j! t! (d - i - j - t)!},
    $$
    we define
    \begin{equation}
      \xijt := \frac{\lijt}{\abs{Y} \binom{d}{i - t, j - t, t}}
    \end{equation}
    when $\binom{d}{i-t, j-t, t}$ is non-zero, and $\xijt := 0$ when it is zero.
    (Here, $i, j, t$ run over $D = \set{0, 1, \ldots, d}$, where $i + j - t \leq
    d$.)

    With this notation, we can define the ``objects'' of our code $Y$:
    \begin{equation}\label{eqn:defn-R} \begin{aligned}
      R  &:= \sum_{i, j, t} \xijt \Mijt \\
      R' &:= \frac{\abs{Y}}{2^d - \abs{Y}} 
        \sum_{i, j, t} \( x_{i + j - 2t, 0}^0 - \xijt \) \Mijt \ .
    \end{aligned} \end{equation}
    It will turn out that these matrices are both positive semi-definite.
    However, it will be easier to compute $\abs{Y}$, as well as to place
    constraints on the numbers $\xijt$ directly.  For any valid triple $i', j',
    t'$ we can find a pair of vertices $u, v$ satisying $\wt{u} = i'$, $\wt{v} =
    j'$, and $\wt{uv} = t'$ and create the matrix $M$ which is $1$ in the $(u,
    v)$ entry, and $0$ elsewhere.  Then,
    $$
      \innerprod{M}{R}
      = \sum_{i, j, t} \xijt \innerprod{M}{\Mijt}
      = x_{i', j'}^{t'}
    $$
    So any linear (in)equality constraint on the $\xijt$ can be translated into
    a linear (in)equality constraint on $R$ (or $R'$).  Similarly, any linear
    combination of the $\xijt$ can be expressed as a linear function of $R$.

    Below, we discuss the objective function on the ``objects'' whose
    maximum is an upper bound on the size of the code; the positive
    semi-definiteness of the ``objects''; and the constraints on the
    ``objects''.  We do each of these in terms of the values $\xijt$ instead of
    $R$ or $R'$ for simplicity.
    \\

    \begin{thm}[Objective]
      The size of a code $Y$ can be computed from its matrix $R$.  In
      particular,
      $$
        \abs{Y} = \sum_{i = 0}^d \binom{d}{i} x_{i, 0}^0 \ .
      $$
    \end{thm}

    \begin{proof}
      To begin,
      $$
        \sum_{i = 0}^d \binom{d}{i} x_{i, 0}^0
        = \sum_{i=0}^d \binom{d}{i} \frac{\lambda_{i, 0}^0}{\abs{Y}\binom{d}{i}}
        = \frac{1}{\abs{Y}} \sum_{i=0}^d \lambda_{i, 0}^0 \ ,
      $$
      so it is sufficient to show that $\sum_{i=0}^d \lambda_{i, 0}^0 =
      \abs{Y}^2$.

      However, $\lambda_{i, 0}^0$ counts the number of triples $(u, v, w) \in
      Y^3$ such that $\wt{u + v} = i$ and $\wt{u + w} = \wt{(u+v)(u+w)} = 0$.
      However, $\wt{u+w} = 0$ implies that $u = w$ and $\wt{(u+v)(u+w)} = 0$, so
      $\lambda_{i, 0}^0$ counts the number of pairs $(u, v) \in Y^2$ such that
      $\wt{u + v} = i$.  Summing over all $i$, there are $\abs{Y}^2$ such pairs.
    \end{proof}

    \begin{thm}[Positive Semi-Definiteness]\label{thm:R-psd}
      For any code $Y$, its matrices $R$ and $R'$ are positive semi-definite.
    \end{thm}

    \begin{proof}

      Recall from Section (\ref{sec:intro:hamming}) the automorphism group of
      the binary Hamming graph, $H(d, 2)$.  Let $\Pi$ be the set of
      automorphisms $\pi$ such that $\vzero \in \pi(Y)$; let $\Pi'$ be the set of
      automorphisms $\pi$ such that $\vzero \not\in \pi(Y)$.  For each
      automorphism $\pi$, let $\chipiY$ be the characteristic vector of the
      image of $Y$ under $\pi$.  Then we define
      \begin{equation}\label{eqn:alt-defn-R} \begin{aligned}
        R  &:= \frac{1}{\abs{\Pi}} \sum_{\pi \in \Pi} \chipiY \chipiY^T \\
        R' &:= \frac{1}{\abs{\Pi'}} \sum_{\pi \in \Pi'} \chipiY \chipiY^T \ .
      \end{aligned} \end{equation}
      It suffices to show that these definitions equal those given in Equation
      (\ref{eqn:defn-R}) since every matrix of the form $BB^T$ is positive
      semi-definite, and a non-negative linear combination of positive
      semi-definite matrices is also positive semi-definite.

      Fix a vertex $u$, and let $\Pi_u$ be the set of automorphisms such that
      $\pi(u) = \vzero$.  Then as seen in Corollary
      (\ref{cor:hamming-aut-group-size}), there are $d! (2-1)!^d = d!$
      automorphisms in $\Pi_u$.  Let $\lijtu$ denote the number of pairs $(v, w)
      \in Y^2$ such that $\wt{u + v} = i$, $\wt{u + w} = j$, $\wt{(u+v)(u+w)} =
      t$, define
      $$
        R_u := \frac{1}{\abs{\Pi_u}} \sum_{\pi \in \Pi_u} \chipiY \chipiY^T
      $$
      and claim that
      \begin{equation}\label{eqn:R-u}
        R_u = \sum_{i, j, t} \binom{d}{i-t, j-t, t}^{-1} \lijtu \Mijt \ .
      \end{equation}

      Since $\abs{\Pi_u} = d!$, the above claim is equivalent to the statement
      that
      $$
        \sum_{\pi \in \Pi_u} \chipiY \chipiY^T =
        \sum_{i, j, t} (i-t)!\ (j-t)!\ t!\ (n-i-j+t)!\ \lijtu \Mijt \ ,
      $$
      which is demonstrated in the following lemma (\ref{lem:R-u}).

      Now, note that $\Pi$ is the disjoint union of $\Pi_u$, as $u$ runs over
      $Y$, while $\Pi'$ is the disjoint union of $\Pi_u$ where $u$ is in the
      complement of $Y$.  Since there are $2^d$ total vertices, and $\abs{\Pi} =
      \abs{Y} d! = \abs{Y} \abs{\Pi_u}$ for any $u$, $\abs{\Pi'} = (2^d -
      \abs{Y}) \abs{\Pi_u}$.  Therefore,
      $$
        R =
          \sum_{u \in Y} \sum_{\pi \in \Pi_u} \frac{1}{\abs{Y} \abs{\Pi_u}}
          \chipiY \chipiY^T
          = \frac{1}{\abs{Y}} \sum_{u \in Y} R_u \ ,
      $$
      and similarly,
      $$
        R' = \frac{1}{2^d - \abs{Y}} \sum_{u \not\in Y} R_u \ .
      $$

      Substituting Equation \ref{eqn:R-u} into the equations above and
      simplifying using the following facts proves the claim.
      \begin{alignat}{2}
        \sum_{u \in Y} \lambda_{i, j}^{t, u} &= \lijt \\
        \sum_{u \not\in Y} \lambda_{i, j}^{t, u} &=
          \binom{i + j - 2t}{i - t} \binom{d - i - j + 2t}{t} \lambda_{i + j -
          2t, 0}^0 - \lijt \\
        \binom{d}{i+j-2t}^{-1} &=
          \binom{d}{i-t, j-t, t}^{-1}\binom{i+j-2t}{i-t}\binom{d-i-j+2t}{t} \\
      \end{alignat}
      The first fact above follows directly from the definition of $\lijt$,
      while the third fact can be show by expanding each binomial and
      multinomial coefficient, and cancelling the appropriate terms.  The second
      fact above is shown in Lemma \ref{lem:lijt-outside-Y}.

      Following the proofs of Lemma \ref{lem:R-u} and Lemma
      \ref{lem:lijt-outside-Y}, this completes the proof of the theorem.

    \end{proof}

    \begin{lem}\label{lem:R-u}
      For any code $Y$, and any vertex $u$ (not necessarily in the code), the
      following equality holds:
      $$
        \sum_{\pi \in \Pi_u} \chipiY \chipiY^T =
        \sum_{i, j, t} (i-t)!\ (j-t)!\ t!\ (n-i-j+t)!\ \lijtu \Mijt \ .
      $$
      (Here, $\lambda_{i, j}^{t, u}$ is used as in the proof of Theorem
      \ref{thm:R-psd}.)
    \end{lem}

    \begin{proof}

      Note that the $(v, w)$-entry of $\chipiY \chipiY^T$ is $1$ if $v$ and $w$
      both belong to $\pi(Y)$, and is $0$ otherwise.  Therefore, the $(v,
      w)$-entry of the sum $\sum_{\pi \in \Pi_u} \chipiY \chipiY^T$ counts the
      number of automorphisms $\pi$ such that $\pi(u) = \vzero$ and $v, w \in
      \pi(Y)$.  By taking the inverse of each such automorphism, we see this is
      equal to the number of automorphisms $\pi$ such that $\pi(\vzero) = u$,
      and $\pi(v), \pi(w) \in Y$.  Using the automorphism decomposition from
      Theorem (\ref{thm:hamming-automorphisms}), we see that such an
      automorphism $\pi = \sigma T_u$ (since $\tau = \id$ for all automorphisms
      of $H(d, 2)$).  Therefore, the number of automorphisms $\pi$ satisfying
      the required property is precisely the number of permutations $\sigma \in
      \Sym[d]$ such that $v\sigma + u$ and $w\sigma +u$ belong to $Y$.

      Recall that the vertex set $\Z_2^d$ can be viewed as the powerset of
      $[d]$, where each vertex is identified with its support.  Then, each
      permutation $\sigma \in \Sym[d]$ acts on subsets of $[d]$ by mapping them
      to their image under $\sigma$.

      Suppose that $\wt{v} = i$, $\wt{w} = j$, and $\wt{vw} = t$.  Then
      $\lambda_{i, j}^{t, u}$ gives the number of pairs $(v', w') \in C^2$ such
      that $\wt{u+v'} = i$, $\wt{u+w'} = j$, and $\wt{(u+v')(u+w')} = t$.  For
      each such pair $(v', w')$, there are $(i-t)!\ (j-t)!\ t!\ (n-i-j+t)!$
      permutations $\sigma$ mapping $v$ to $u + v'$, and $w$ to $u + w'$.  To
      see this, note that each such permutation $\sigma$ can be decomposed into
      a permutation fixing the sets $v \setminus w$, $w \setminus v$, $vw$, and
      $v \cup w$, followed by a permutation mapping
      $$
      \begin{cases}
        v \setminus w &\mapsto (u+v') \setminus (u+w') \\
        w \setminus v &\mapsto (u+w') \setminus (u+v') \\
        vw            &\mapsto (u+v')(u+w') \\
        v \cup w      &\mapsto (u+v') \cup (u+w') \\
      \end{cases} \ .
      $$
      (This is possible since each of the sets above is the same size as its
      image, and together the sets partition the powerset of $[d]$.)

    \end{proof}

    \begin{lem}\label{lem:lijt-outside-Y}
      For all $i$, $j$, and $t$,
      $$
        \sum_{u \not\in Y} \lambda_{i, j}^{t, u} =
          \binom{i + j - 2t}{i - t} \binom{d - i - j + 2t}{t} \lambda_{i + j -
          2t, 0}^0 - \lijt \ .
      $$
      (Here, $\lambda_{i, j}^{t, u}$ is used as in the proof of Theorem
      \ref{thm:R-psd}.)
    \end{lem}

    \begin{proof}
      Note that it suffices to show that
      $$
        \sum_u \lambda_{i, j}^{t, u} =
          \binom{i + j - 2t}{i - t} \binom{d - i - j + 2t}{t} \lambda_{i + j -
          2t, 0}^0 \ .
      $$

      Furthermore, $\lambda_{i+j-2t, 0}^0$ counts the number of pairs $(u', v',
      w') \in C^3$ such that $\wt{v'+w'} = i + j - 2t$ and $\wt{u'+w'} =
      \wt{(v'+w')(u'+w')} = 0$.  (Note that here, the roles of $u'$ and $w'$
      have been swapped with respect to their usual roles.)  For each such pair
      $(v', w')$, define $v := v'$ and $w := w'$.  Then, there will be $\binom{i
      + j - 2t}{i - t} \binom{d - i - j + 2t}{t}$ vertices $u$ such that
      $\wt{u+v} = i$, $\wt{u+w}=j$, and $\wt{(u+v)(u+w)} = t$.

      To see this, we instead count $\tu := u+v$, since $v$ is fixed, and $u$
      can be recovered as $u = \tu + v$.  We can choose $\tu$ directly (as a
      subset of [d]) so that $\wt{u+v} = i$ by selecting a set $a$ of $i-t$
      elements from $\binom{i+j-2t}{i-t}$ choices in $v+w$, and selecting a set
      $b$ of $t$ elements from $\binom{d-i-j+2t}{t}$ choices in the complement
      of $v+w$.  In this way we can write $\tu = a \sqcup b$, where $a \subseteq
      v+w$ and $b \cap (v+w) = \emptyset$.

      Then, we can write $u+w = v+z \sqcup b \setminus a$, from which we can
      compute
      $$
        \wt{u+w} = (i+j-2t) + (t) - (i-t) = j
      $$
      as desired.  Furthermore, we can compute
      \begin{alignat*}{2}
        \wt{v+w} &= \wt{u+v} + \wt{u+w} - 2\wt{(u+v)(u+w)} \\
        \implies \wt{(u+v)(u+w)} &= -\frac{1}{2} \( (i+j-2t) - i - j \) = t \ ,
      \end{alignat*}
      which proves the claim.

    \end{proof}

    \begin{figure}[h]
      \centering
      \begin{tikzpicture}[
          thick,
          set/.style = {draw, circle, minimum size=4cm},
        ]

        \node[draw,rectangle, minimum width=10cm, minimum height=8cm] () at (0:0cm) {};

        \node[set, label={135:$u$}] (u) at (135:1.5cm) {};
        \node[set, label={ 45:$w$}, fill=black!15] (w) at ( 45:1.5cm) {};
        \node[set, label={-90:$v$}, fill=black!15] (v) at (-90:0.8cm) {};

        \begin{scope}
          \clip ( 45:1.5cm) circle(2cm);
          \clip (-90:0.8cm) circle(2cm);
          \fill[white] (-45:1cm) circle(4cm);
        \end{scope}

        \draw[ultra thick] (135:1.5cm) circle(2cm);
        \draw[      thick] ( 45:1.5cm) circle(2cm);
        \draw[ultra thick] (-90:0.8cm) circle(2cm);

        \node at (-90:1.7cm) {$a$};
        \node at ( 90:1.7cm) {$a$};

        \node at (-15:1.3cm) {$b$};
        \node at (135:2.3cm) {$b$};
      \end{tikzpicture}
      \caption{
        In the proof of Lemma \ref{lem:lijt-outside-Y}, we construct a set $u+v$
        given sets $v$ and $w$ as the disjoint union of
        $a = u (v + w)$ and 
        $b = u \setminus (v + w)$.
      }
    \end{figure}

    \begin{thm}[Constraints]
      Let $Y$ be a code of minimum distance $\delta$.  Then the following
      constraints hold.
      \begin{enumerate}[(i)]
        \item $x_{0, 0}^0 = 1$.
        \item For all $i, j, t$,
          $$
            0 \leq \xijt \leq x_{i, 0}^0 \leq 1 % TODO Ask Newman if the `\leq 1` part is valid.
            \quad\And\quad
            x_{i, 0}^0 + x_{j, 0}^0 \leq 1 + \xijt \ .
          $$
        \item For all $i, j, t$, if the tuple $(i, j, i + j - 2t)$ is a
          permutation of $(i', j', i' + j' - 2t')$, then $\xijt = x_{i',
          j'}^{t'}$.
        \item For all $i, j, t$,
          $$
            \set{i, j, i + j - 2t} \cap \set{1, \ldots, \delta - 1} \neq
            \emptyset \implies \xijt = 0 \ .
          $$
      \end{enumerate}
    \end{thm}

    \begin{proof} \begin{enumerate}[(i)]
      \item Note that $\lambda_{0, 0}^0$ is the number of triples $(u, v, w)
        \in Y^3$ such that $\wt{u + v} = \wt{u + w} = \wt{(u + v)(u + w)} =
        0$.  Since the weight of any vertex is $0$ if and only if that vertex
        is itself the zero vertex, we conclude that $u = v = w$.  (This
        follows from the fact that in $\Z_2^d$, each element is its own
        additive inverse.)  Since there are $\abs{Y}$ choices of vertex in
        $Y$, and $\binom{d}{0, 0, 0} = 1$, we see that $x_{0, 0}^0 = \abs{Y} /
        \abs{Y} = 1$.

      \item Consider the two equivalent definitions of $R$ (Equation
        \ref{eqn:defn-R} and Equation \ref{eqn:alt-defn-R}):
        $$
          \frac{1}{\abs{\Pi}} \sum_{\pi \in \Pi} \chipiY \chipiY^T
          = \sum_{i, j, t} \xijt \Mijt \ .
        $$
        For any vertex $u$, if $\wt{u} = i$, then the diagonal element of $R$ is
        $x_{i, 0}^0$, since $(\Mijt)_{u, v}$ is non-zero when $\wt{u} = i$,
        $\wt{v} = j$, and $\wt{u + v} = t$.  Therefore, it will suffice to prove
        that for each row of $R$, its diagonal element is at least as large as
        any other.

        However, the $(u, v)$-entry in $\chipiY \chipiY^T$ is $1$ whenever $u$
        and $v$ both belong to $\pi(Y)$, and is $0$ elsewhere.  However, if $u$
        is in $\pi(Y)$, then the $(u, u)$ entry will certainly be non-zero,
        whereas the $(u, v)$-entry may or may not be zero.  Therefore, when
        summing over $\pi \in \Pi$, the diagonal entry is counted at least as
        many times as any other entry in its row.  Therefore, $\xijt \leq x_{i,
        0}^0$.  

        Further, let $\Pi(u)$ be the set of permutations $\pi$ such that $u \in
        \pi(Y)$.  Since $\Pi(u) \subseteq \Pi$, it follows that $x_{i, 0}^0 =
        \abs{\Pi(u)} / \abs{\Pi} \leq 1$.  So, for each triple $(i, j, t)$, let
        $u$ and $v$ be vertices such that $\wt{u} = i$, $\wt{v} = j$, and $\wt{u
        + v} = t$, so that the $(u, v)$-entry of $R$ is $\xijt$.  Then, $\xijt$
        counts the number of permutations $\pi$ for which $u \in \pi(Y)$ and $v
        \in \pi(Y)$, and since
        $$
          \abs{\Pi(u)} + \abs{\Pi(v)}
          = \abs{\Pi(u) \cap \Pi(v)}
          + \abs{\Pi(u) \cup \Pi(v)}
          \leq \abs{\Pi(u) \cap \Pi(v)}
          + \abs{\Pi}
        $$
        by dividing the above relation by $\abs{\Pi}$, we see that $x_{i, 0}^0 +
        x_{j, 0}^t \leq 1 + \xijt$.

      \item Recall that the vertex set $\Z_2^d$ of a binary Hamming graph may
        also be viewed as the powerset of $[d]$, by identifying each vertex with
        its support.  Furthermore, recall that $\lijt$ counts the number of
        triples $(u, v, w) \in Y^3$ such that $\wt{u + v} = i$, $\wt{u + w} =
        j$, and $\wt{(u + v)(u + w)} = t$.  Since $(u + v) + (u + w) = v + w$
        we can see that
        $$
          \wt{v + w} = \wt{u + v} + \wt{u + w} - 2\wt{(u + v)(u + w)}
        $$
        so that $\wt{v + w} = i + j - 2t$ if and only if $\wt{(u + v)(u + w)} =
        t$.  Therefore, $\lijt$ also counts the number of triples $(u, v, w) \in
        Y^3$ such that $\wt{u + v} = i$, $\wt{u + w} = j$, and $\wt{v + w} = i +
        j - 2t$.  Since the roles of $u$, $v$, and $w$ are symmetric in this
        definition, a permutation of the values $i$, $j$, and $i + j - 2t$ must
        leave $\lijt$ unchanged.

      \item Recall from Definition (\ref{defn:AS-clique}) that $Y$ is a
        $C$-clique if the distance between any pair of vertices is in $C$.  By
        this definition, $Y$ is a code of minimum distance $\delta$ if and only
        if it is a $C$-clique with $C = \set{0, \delta, \ldots, d}$.  Following
        the previous proof, then for all $(u, v, w) \in Y^3$, their distances
        $i := \wt{u + v}$, $j := \wt{u + w}$, and $i + j - 2t := \wt{v + w}$
        must belong to $C$.  If any of $i$, $j$, or $i + j - 2t$ belong to $D
        \setminus C$, then $\lijt = 0$, as no such triple of vertices $(u, v,
        w)$ may all belong to $Y$.

    \end{enumerate} \end{proof}

    \mysep

    % TODO ?
    \begin{thm}
      Schrijver's SDP bound is at least as strong as Delsarte's LP bound.
    \end{thm}

\chapter{Conclusion}

  Motivated by the desire to transmit messages in such a way that they are
  robust to errors which may occur during transmission, algebraic coding theory
  typically seeks a subspace of a finite vector space in which to encode
  messages.  To correct as many errors as possible, it is beneficial for the
  subspace to have a large minimum distance.  However, for the purposes of
  efficiency, it is desirable that the subspace should be large.  Due to this
  tradeoff, given a fixed minimum distance, we seek upper bounds on the size of
  a code.

  This question transfers naturally from the subspaces of a finite vector space,
  to the subsets of a Hamming graph -- or more generally, a distance-regular
  graph.  In this setting, a code of a certain minimum distance is equivalent to
  a coclique (or independent set) in a related graph.  Then, to find upper
  bounds on the codes, cocliques, and cliques as well, we notice that
  distance-regular graphs form a structure called an association scheme.

  Association schemes have a number of useful algebraic properties that allow us
  to treat codes, cocliques, and cliques uniformly, and to compute upper bounds
  on their size, given a specific graph and minimum distance.  This computation
  relies specifically on the change of basis matrices $P$ and $Q$ in the
  association scheme.  While for arbitrary association schemes computing these
  matrices involves diagonalizing all the classes in the scheme, for the
  $P$-polynomial schemes (those arising from a distance-regular graph) there
  exists an adjacency matrix $A_1$ such that every other adjacency matrix $A_i$
  is a polynomial in $A_1$.  Therefore, any eigenvector of $A_1$ is
  automatically an eigenvector of each $A_i$, which is significantly more
  efficient.  Even more efficient than this, when an association scheme contains
  a transitive abelian subgroup of its automorphism group, the eigenvectors of
  the entire scheme can be read directly from the character table of the group.

  For the Hamming scheme in particular, in large part because it has such a
  large automorphism group, it forms a translation scheme.  Furthermore, there
  exists a stronger bound for the codes of the Hamming scheme, although this
  requires working in the larger Terwilliger algebra.  In both cases, the bounds
  are formulated as convex optimization problems: the first is expressed as a
  linear programming problem with constraint matrix $Q$; the other is posed as a
  semi-definite programming problem.

  The LP bound is generally more efficient, and linear programs are easier to
  solve than semi-definite ones, and moreover, the objects in question in the LP
  bound (the matrix of dual eigenvalues $Q$) are of order $(d+1) \times (d+1)$.
  On the other hand, the dimension of the Terwilliger algebra used in the SDP
  bound is $\binom{d+3}{3}$, which is cubic in $d$, the diameter of the scheme.
  For the Hamming scheme in particular, the $Q$ matrix can be directly computed,
  and the Terwilliger algebra has an explicit diagonalization, but more
  generally, the orders of objects in the Terwilliger algebra is exponential in
  $d$, as are objects such as the character table of the transitive, abelian
  automorphism group.  Therefore, when computing such bounds, the explicit
  diagonalization of the Terwilliger algebra is necessary; the explicit formula
  for $Q$ helps, but as character tables may be generated on-the-fly the LP
  bound may be formulated efficiently for translation schemes.

  However, despite the fact that the LP bound is generally more efficient to
  compute, and its simplicity allows for explicit upper bounds given by feasible
  dual solutions, the SDP bound is more powerful.  An example of the LP and SDP
  upper bounds on a Hamming $H(d, 2)$ scheme for some diameters $d$ and minimum
  distances $\delta$ is shown in Figure \ref{fig:results}.

  \begin{figure}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
      \hline
      $d$ & $\delta$ & SDP   & LP \\
      \hline
      19  & 6        & 1280  & 1289 \\
      23  & 6        & 13766 & 13775 \\
      25  & 6        & 47998 & 48138 \\
      \hline
      19  & 8        & 142   & 145 \\
      20  & 8        & 274   & 290 \\
      25  & 8        & 5477  & 5557 \\
      27  & 8        & 17768 & 18189 \\
      28  & 8        & 32151 & 32206 \\
      \hline
      22  & 10       & 87    & 95 \\
      25  & 10       & 503   & 551 \\
      26  & 10       & 886   & 1040 \\
      \hline
    \end{tabular}
    \caption{Upper bounds for binary Hamming codes of diameter $d$ and minimun
      distance $\delta$, from \cite[Table~I]{schrijver}}
    \label{fig:results}
  \end{figure}

  In the process of computing these bounds, we learned much about the
  automorphisms and eigenvalues of the Hamming scheme, as well as about
  association schemes in general.  This topic uses a unique blend of ideas from
  graph theory, group theory, linear algebra, and optimization.  There are also
  a number of additional uses of association schemes to combinatorial design,
  representation theory, and statistics that were not covered in this report.
  Furthermore, not all aspects of the theory of association schemes were
  covered.  More information may be found in the sources used to compile this
  report; see the bibliography for more.

\appendix

\chapter{Linear Algebra}

  The reader of this report is expected to be familier with the standard notions
  of finite-dimensional vector spaces over $\R$ or $\C$.  Some topics which are
  more important to this report, and less commonly emphasized elsewhere, are
  included here for reference.

  Throughout the report, the vector spaces in question will typically be $\C^n$
  or $\R^n$, for various values of $n$, with the standard inner product.  Given
  a matrix (or vector) $M$, its adjoint will be written $M^*$.  If $M$ is real,
  then this is simply the transpose; if $M$ is complex, then it is the conjugate
  transpose.

  \section{The Spectral Theorem}
    \subsection{Orthogonal Projection}

      Let $X$ be an inner product space, let $W$ be a subspace of $X$, and let
      the columns of a matrix $U$ be a basis for $W$.  Then the matrix $UU^*$,
      which maps all vectors into $W$, is the \textit{orthogonal projection}
      onto $W$.  It is a \textit{projection} in that it is idempotent ($(UU^*)^2
      = UU^*$), and it is \textit{orthogonal} because it maps vectors $x$ to the
      closest vector $w$ in $W$ to $x$ (then $x - w$ is orthogonal to $W$).

      Recall that the \textit{trace} of a matrix is the sum of its diagonal
      entries.  If the matrix is diagonalizable, then it is also the sum of its
      eigenvalues (with multiplicity).  The following facts will be importing in
      the course of the report.

      \begin{lem}
        The eigenvalues of an idempotent matrix are all in $\set{0, 1}$.
      \end{lem}

      \begin{proof}
        Let $F$ be an idempotent matrix, so that $F^2 - F = 0$.  If $v$ is an
        eigenvector with eigenvalue $\lambda$, then
        $$
          0 =(F^2 - F)v = F^2v - Fv = \lambda^2 v - \lambda v = (\lambda^2 -
          \lambda)v \ .
        $$
        Since $v$ cannot be zero, this implies that $\lambda(\lambda - 1) = 0$.
      \end{proof}

      \begin{prop}
        The trace of diagonalizable idempotent matrix is its rank.
      \end{prop}

      \begin{proof}
        The trace of a matrix is the sum of its eigenvalues, and the eigenvalues
        of an idempotent matrix are all in $\set{0, 1}$.  The trace is the
        number of $1$s in the diagonal representation of the matrix, which is
        equal to the rank.
      \end{proof}

    \subsection{Spectral Decomposition}

      \begin{defn}
        A linear operator is called \textsc{normal} if it commutes with its
        adjoint.
      \end{defn}

      The following result regarding normal operators of a finite-dimensional
      inner product space is central to this report.

      \begin{thm}
        If $M$ is a normal operator, then $M$ is unitarily diagonalizable.  That
        is, there exists a unitary matrix $U$ and a diagonal matrix $\Delta$
        such that $M = UDU^*$.
      \end{thm}

      While this is the way that the spectral theorem is normally presented, it
      will be useful to look closer at this result.

      Let $M$ be a normal matrix with distinct eigenvalues $\theta$, and
      eigenspaces $E_\theta$.  Let $F_\theta$ be the orthogonal projection onto
      $E_\theta$.  Then,
      $$
        M = \sum_\theta \theta F_\theta \quad\And\quad
        I = \sum_\theta F_\theta \ .
      $$
      This \textit{spectral decomposition} will be the form used throughout this
      report.

      There are a number of important facts about this decomposition which will
      also be useful.  First, the $F_\theta$ are orthogonal idempotents, in that
      $F_\theta F_{\theta'} = \delta_{\theta \theta'} F_\theta$.  This implies
      the following fact.  If $f$ is any polynomial, then $f(M) = \sum_\theta
      f(\theta) F_\theta$.  Since the eigenvalues $\theta$ are distinct, for
      each eigenvalue $\theta$, there exists a polynomial -- for example,
      $\prod_{\theta' \neq \theta} (x - \theta')$ -- which is nonzero on exactly
      $\theta$ and no other eigenvalues.  This allows one to express each
      $F_\theta$ as a polynomial in $M$.

  \section{Positive Semi-Definite Matrices}
    Depending on which proof of the clique-coclique bound I use, and how much
    detail I go into Schrijver's SDP bound, I could make some comments about PSD
    matrices.

    In this section, we will restrict the discussion to real, symmetric
    matrices.

    \begin{defn}
      A real, symmetric matrix $F$ is \textsc{positive semi-definite} if for all
      vectors $x$,
      $$
        x^T F x \geq 0 \ .
      $$
    \end{defn}

    This is the most common definition of a positive semi-definite matrix, but
    in fact there are a number of other equivalent conditions.

    \begin{thm}
      The following are equivalent:
      \begin{itemize}
        \item The matrix $F$ is positive semi-definite.
        \item There exists a (possibly non-square) matrix $B$ such that $F = B^T
          B$.
        \item All the eigenvalues of $F$ are non-negative.
      \end{itemize}
    \end{thm}

    These conditions will be used interchangeably in this report.

    It also follows directly from the definition, that any non-negative linear
    combination of positive semi-definite matrices is also positive
    semi-definite.  That is, if $\alpha_0, \alpha_1, \ldots, \alpha_d$ are
    non-negative real numbers, and $F_0, F_1, \ldots, F_d$ are positive
    semi-definite matrices, then $\alpha_0 F_0 + \alpha_1 F_1 + \cdots +
    \alpha_d F_d$ is also positive semi-definite.

    Furthermore, it is important to note that if $U$ is any invertible operator,
    then $F$ is positive semi-definite, if and only if $U^TFU$ is also positive
    semi-definite.  This is because for any vector $x$, there exists a unique
    pre-image $y$ of $x$, such that $x^T F x = (Uy)^T F (Uy) \geq 0$.

\chapter{Group Theory}
  \section{Group Actions}
    The material of this section comes primarily from \cite[Section~1.7;
    Chapter~4]{dummit-foote}.

    \begin{defn}[Group Action]\label{group-action}
      Given a group $G$ and a set $X$,
      \textsc{group action} is a homomorphism $G \to \Sym X$,
      where $\Sym X$ is the symmetric group on $X$.
    \end{defn}

    A group action $\phi: G \to \Sym X$ induces a product
    $X \times G \to X$ by mapping $(x, g) \mapsto \phi(g)(x)$.
    When the action is clear from context,
    this will be denoted $x \cdot g$, or simply $xg$.
    This is called a \textsc{right action},
    as $g$ \textit{acts on the right of} $x$
    (the corresponding notion of a \textsc{left action}
    can also be defined.)

    Conversely, given a product $X \times G \to X$,
    the same expression defines a map $G \to \Sym X$.
    If such a product satisfies
    \begin{alignat*}{2}
      &\forall x \in X\ x 1_G = x \\
      \text{and } &\forall x \in X\ \forall g, h \in G\
        (x g) h = x(gh)
    \end{alignat*}
    then the induced map $G \to \Sym X$ will be a homomorphism,
    so that these definitions are equivalent.

    (In \cite{dummit-foote}
    this is taken as the definition of a group action,
    and the homomorphism $G \to \Sym X$ is called its \textsc{permutation
    representation}.  It will be occasionally convenient to adopt each
    perspective.)

    \begin{defn}[Types of Group Actions]\label{group-action-types}
      If if a homomorphism $G \to \Sym X$ is injective,
      then the action is called \textsc{faithful}.
      Note that a group homomorphism is injective
      if and only if it has a trivial kernel.

      Given a group action $G \to \Sym X$,
      $g \in G$ is called \textsc{fixed point-free}
      if $\forall x \in X\ xg \neq x$.
      The group action itself is called \textsc{fixed point-free}
      (or just \textsc{free}) if all its nontrivial elements
      are fixed point-free.

      A group action $G \to \Sym X$ is called \textsc{transitive}
      if $\forall x, y \in X$ there exists some $g \in G$
      such that $xg = y$.

      A group action is called \textsc{regular} if it is simultaneously
      transitive and free.  (This terminology follows \cite{godsil}.)
    \end{defn}

    Note that if $X$ is a structure with automorphisms
    (such as a graph or group), $G$ is a subgroup of $\Aut X$,
    and $G$ acts in the natural way on $X$ (i.e. $xg = g(x)$),
    then this action is faithful.
    That is, $\Aut X \leq \Sym X$,
    so that this action is induced by the identity $G \into \Sym X$,
    which is clearly injective.

    \begin{lem}\label{faithful-transitive-abelian}
      If an abelian group $G$ acts faithfully and transitively on a set $X$,
      then the action is free, and thus also regular. \cite[Section 4.1,
      Exercise 3]{dummit-foote}
    \end{lem}

    \begin{proof}
      Let $g \in G$ be nontrivial, and $x \in X$.
      The goal is to prove that $xg \neq x$.

      Since $g$ is not the identity,
      there exists some $y \in X$ such that $z := yg \neq y$.
      Furthermore, since $G$ acts transitively on $X$,
      there exists some $h \in G$ such that $yh = x \iff y = xh^{-1}$.
      Then,
      \begin{alignat*}{3}
        xg &= (yh) g &\\
        &= y (hg) &\\
        &= y (gh) \quad&\text{ since $G$ is abelian}\\
        &= (yg) h &\\
        &= zh \ .&\\
      \end{alignat*}

      If $zh = x$ then, $z = xh^{-1} = y$,
      but by definition, $z = yg \neq y$,
      so $xg = zh \neq x$.
    \end{proof}

    An alternate characterization of regular actions will be useful in this
    report.  To see this, note that for a pair $x, y \in X$, there exists a $g
    \in G$ such that $xg = y$ by transitivity; for any $g' \in G$ satisfying
    $xg' = y$,
    $$
      xg = xg' \implies x = x g'g^{-1}
    $$
    so that $g'g^{-1} = 1$ since the action is free, and so $g' = g$.
    Conversely, if for each $x, y \in X$ there existed a unique $g \in G$
    satisfying $xg = y$, then the action would clearly be transitive; since $x 1
    = x$, $1$ is the unique group element fixing any point, so the action must
    be free.

    \begin{lem}\label{regular-unique}
      A group action $G$ on $X$ is regular if and only if
      for all $x, y \in X$, there exists a unique $g \in G$
      such that $xg = y$.
    \end{lem}

  \section{The Structure of Finite Abelian Groups}
    TODO I need a citation for this.
    I'm also not sure if this section shouldn't go before the section on
    Character Theory, as it will be used in (\ref{character-duality}).

    \begin{thm}[Structure Theorem]\label{thm:abelian-structure}
      If $G$ is a finitely generated abelian group,
      then $G$ is isomorphic to a direct product of cyclic groups.
      Specifically,
      $$
        G \cong
        \Z_{q_1}^{d_1} \times \cdots \times \Z_{q_t}^{d_t}
        \times \Z^f
      $$
      where the $q_i$ are distinct prime powers.
      Moreover, this decomposition is unique up to the ordering of its factors.
    \end{thm}

    This is a well-known and well-used result.  It comes as a direct result of
    the Structure Theorem for Finitely Generated Modules over PIDs, using the
    language of rings and modules, but this is out of the scope of this report.

  \section{Character Theory}
    \begin{defn}[Characters]\label{character}
      Given a group $G$, a \textsc{character} of the group $G$
      is a homomorphism $G \to \C^\times$,
      the group of non-zero complex numbers under multiplication.
      Then $G^*$ will denote the set of characters of $G$.
      \cite[Chapter 8]{godsil}

      (For abelian groups, this corresponds to irreducible degree $1$ characters
      over $\C$ in \cite[Section 18.3]{dummit-foote}.)
    \end{defn}

    TODO Maybe use $\circ$ for this product to mimic the usage for the Schur
    product?

    On $G^*$ a product of characters can be defined by setting
    $$
      \phi \psi: g \mapsto \phi(g) \psi(g)
    $$
    under which the character taking each $g \in G$ identically to $1$
    acts as identity.

    Furthermore, for any character $\psi \in G^*$, the map $g \mapsto
    \psi\(g^{-1}\)$ is a homomorphism since
    $$
      gh \longmapsto \psi\((gh)^{-1}\)
      = \psi\( h^{-1} g^{-1} \)
      = \psi\(h^{-1}\) \psi\(g^{-1}\)
      = \psi\(g^{-1}\) \psi\(h^{-1}\)
    $$
    and for any $g \in G$,
    $$
      \psi(g) \psi\(g^{-1}\)
      = \psi\(g^{-1}\) \psi(g)
      = \psi(1) = 1
    $$
    so that this homomorphism is an inverse for $\psi$.

    Therefore, $G^*$ forms a group under the above product of characters.
    \\

    For the remainder of this section (and the rest of this report),
    discussion of characters will be restricted
    to the case of finite abelian groups.
    Throughout this section, $G$ will denote
    a finite abelian group of order $n$.

    In this case, by Lagrange's theorem, $g^n = 1_G$ for every $g \in G$,
    and so for any character $\psi \in G^*$
    $$
      1 = \psi(1) = \psi\(g^n\) = \psi(g)^n
    $$
    -- that is, the image of each character is contained in
    the set of $n^\text{th}$ roots of unity.

    Since the inverse of a complex number with modulus $1$ is also its complex
    conjugate, looking at the inversion in $G^*$,
    $$
      \psi\(g^{-1}\) = \psi(g)^{-1} = \bar{\psi(g)}
    $$
    so that the inverse of $\psi \in G^*$ is $\bar{\psi}: g \mapsto
    \bar{\psi(g)}$.

    \begin{thm}\label{character-duality}
      For all finite abelian groups $G$,
      $$
        G \cong G^*
        \ .
      $$
    \end{thm}

    \begin{proof}
      Let
      $$
        G = \Z_{q_1} \times \cdots \times \Z_{q_d}
      $$
      for $q_i \in \N_{>0}$ (\ref{thm:abelian-structure}).  Since all finite
      abelian groups are isomorphic to a group of this form, it suffices to
      prove that each group of this form is isomorphic to its group of
      characters.

      For each $i$, let $e_i$ be the tuple of all zeroes, with a $1$ in the
      $i^\text{th}$ spot.  Then, every element $x$ of $G$ is of the form $x =
      (x_1, \ldots, x_d) = \sum_{i=1}^d x_i e_i$, where each $x_i$ is some
      element of $\Z_{q_i}$.  Furthermore, for all $i$, let $\omega_i$ be
      \textit{any} primitive $q_i^\text{th}$ complex root of unity, so that
      $\omega_i^0, \omega_i^1, \ldots, \omega_i^{q_i - 1}$ are all distinct.

      First, we claim that every character $\psi$ is completely determined by
      its values on $e_1, \ldots, e_d$.  To see this, take any group element
      $z$, and compute
      $$
        \psi(z)
        = \prod_{i=1}^d \psi(z_i e_i)
        = \prod_{i=1}^d \psi\(\sum_{j=1}^{z_i} e_i\)
        = \prod_{i=1}^d \prod_{j=1}^{z_i} \psi(e_i)
        = \prod_{i=1}^d \psi(e_i)^{z_i}
        \ .
      $$
      Furthermore, note that $\psi(e_i)$ is a $q_i^\text{th}$ complex root of
      unity, since $q_i \cdot e_i = 0$ and $\psi(q_i \cdot e_i) =
      \psi(e_i)^{q_i}$, while $\psi(0) = 1$.

      So, define the map $^*: G \to G^*$ by
      $$
        x^* :
          z \mapsto \prod_{i=1}^d \omega_i^{x_iz_i}
        \ .
      $$
      Then $x^*$ is a character of $G$ since $x^*(0) = \prod_{i=1}^d \omega_i^0
      = 1$, and for group elements $z$ and $\tilde{z}$,
      $$
        x^*(z + \tilde{z})
        = \prod_{i=1}^d \omega_i^{x_i(z_i + \tilde{z_i})}
        = \prod_{i=1}^d \omega_i^{x_iz_i} \omega_i^{x_i\tilde{z_i}}
        = \prod_{i=1}^d \omega_i^{x_iz_i} \prod_{i=1}^d \omega_i^{x_i\tilde{z_i}}
        = x^*(z) x^*(\tilde{z})
        \ .
      $$
      Therefore, this map is well-defined.

      Next, we claim that this map is a homomorphism.  To see this, note that
      for any group element $z$, $0^*(x) = \prod_{i=1}^d \omega_i^0 = 1$.  Then,
      for any three group elements $x$, $y$, and $z$,
      $$
        (x + y)^*(z)
        = \prod_{i=1}^d \omega_i^{(x_i + y_i)z_i}
        = \prod_{i=1}^d \omega_i^{x_i z_i} \omega_i^{y_i z_i}
        = \prod_{i=1}^d \omega_i^{x_i z_i} \prod_{i=1}^d \omega_i^{y_i z_i}
        = x^*(z) y^*(z)
      $$
      which demonstrates that $(x + y)^* = x^* y^*$.

      Finally, we show that this map is bijective.  First, note that if a
      character maps each $e_i$ to $1$, then that character is the identity.
      Since, $x^*(e_i) = \omega_i^{x_i}$, which is $1$ if and only if $x_i = 0$,
      $x^*$ is the identity if and only if $x = 0$.  Furthermore, if $\psi$ is
      any character, then $\psi$ is completely determined by the values
      $\psi(e_1), \ldots, \psi(e_d)$.  Since each $\psi(e_i)$ is a
      $q_i^\text{th}$ root of unity, $\psi(e_i) = \omega_i^{x_i}$ for some $x_i$
      in $\Z_{q_i}$.  Therefore, $\psi = (x_1, \ldots, x_d)^*$.
    \end{proof}

    While by transitivity this shows that $G^{**} \cong G$,
    this can be seen more directly via the isomorphism
    $$
      g \mapsto \( \psi \mapsto \psi(g) \) \ .
    $$
    \\

    Given an ordering $G = \set{g_1, \ldots, g_n}$,
    the character $\psi \in G^*$ can be identified with the row vector
    \begin{equation}\label{character-vector}
      \psi \rightsquigarrow
      \begin{bmatrix}
        \psi(g_1) & \cdots & \psi(g_n) \\
      \end{bmatrix}
      \ .
    \end{equation}
    With this identification, the product of characters becomes the entrywise
    product of vectors (the \textit{Schur product} of $n \times 1$ matrices),
    and the inverse of a character in $G^*$ is the entrywise inversion of the
    vector.

    Furthermore, given an ordering $G^* = \set{\psi^1, \ldots, \psi^n}$,
    the matrix whose rows consist of the characters of $G^*$ is called the
    \textsc{character table} of $G$:
    \begin{equation}\label{character-table}
      H =
      \begin{bmatrix}
        \horzbar & \psi^1 & \horzbar \\
                 & \vdots &          \\
        \horzbar & \psi^n & \horzbar \\
      \end{bmatrix}
      \ .
    \end{equation}
    Remarkably, this matrix turns out to be (almost) unitary.
    \\

    For any subset $C \subseteq G$, define
    $$
      \psi(C) := \sum_{g \in C} \psi(g)
      = \psi \chi_C
    $$
    where $\chi_C$ is the characteristic vector of $C$ in $G$
    (with the same ordering).
    So, given characters $\psi, \phi$, their inner product can be written
    $$
      \psi \phi^* = \sum_{g \in G} \psi(g) \bar{\phi}(g)
      = \sum_{g \in G} \(\psi \bar{\phi}\) (g)
      = (\psi \bar{\phi}) \chi_G
      = (\psi \bar{\phi}) (G)
      \ .
    $$
    (Note here that $\phi^*$ denotes the conjugate transpose of $\phi$,
    and $\chi_G$ is also the all-ones column vector $\vone$.)

    \begin{lem}\label{characters-orthogonal}
      For any $\psi \in G^*$
      $$
        \psi(G) =
        \begin{cases}
          \abs{G} & \text{ if $\psi$ is the identity of $G^*$} \\
          0 & \text{ else.} \\
        \end{cases}
      $$
    \end{lem}

    \begin{proof}
      For any $h \in G$, since $g \mapsto hg$ is an automorphism of $G$,
      $hG = G$, so that
      $$
        \psi(G) = \sum_{g \in G} \psi(g)
        = \sum_{g \in G} \psi(hg)
        = \sum_{g \in G} \psi(h) \psi(g)
        = \psi(h) \sum_{g \in G} \psi(g)
        = \psi(h) \psi(G)
      $$
      which implies that either $\psi(h) = 1$ or $\psi(G) = 0$.

      But this holds for arbitrary $h \in G$, so that either
      $$
        \forall h \in G\ \psi(h) = 1
        \implies \psi = 1_{G^*}
        \quad\text{and}\quad
        \psi(G) = \sum_{g \in G} 1 = \abs{G}
      $$
      or else
      $$
        \exists h \in G\ \psi(h) \neq 1
        \implies \psi(G) = 0
        \ .
      $$
    \end{proof}

    \begin{cor}\label{character-table-unitary}
      If $H$ is the character table of a finite abelian group $G$ of order $n$,
      then $H H^* = nI$, where $H^*$ is the conjugate transpose of $H$.
    \end{cor}

    \begin{proof}
      If $\psi, \phi$ are characters of $G$, and $\psi \neq \phi$,
      then letting $\theta = \psi \bar{\phi}$, the $(\psi, \phi)$-entry of $HH^*$
      is given by $\psi \phi^* = \theta(G) = 0$, since $\theta$ is not the
      identity of $G^*$.

      However, for the diagonal, $(\psi, \psi)$-entries of $HH^*$,
      $\psi \psi^* = 1_{G^*}(G) = n$, which proves the claim.
    \end{proof}

\chapter{Notation}
  I've included these sections mostly as an excuse to add the citations to the
  bibliography, though it may be somewhat useful to have a sort of guide to the
  similarities and differences in notation in this report, as well as in the
  references.  (At least, I would find such a thing useful.)

  \section{Godsil}
    See \cite{godsil} Ch. 10.

  \section{Delsarte}
    See \cite{delsarte} Ch. 3.

  \section{Schrijver}
    See \cite{schrijver}.

\printbibliography[heading=bibintoc]

\end{document}
