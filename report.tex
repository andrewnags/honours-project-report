\documentclass{report}

\usepackage{preamble}
\usepackage{prealgebra}
\usepackage{premath}
\usepackage{precomb}

\usepackage{biblatex}
\addbibresource{report.bib}

% Remove "chapter" heading
\usepackage{titlesec}
\titleformat{\chapter}{\normalfont\huge\bf}{\thechapter.}{20pt}{\huge\bf}

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\newcommand{\AS}{\mathcal{A}}
\newcommand{\Zq}{\Z_q}
\newcommand{\Zqz}{\Z_q \setminus \set{0}}
\newcommand{\Zqd}{\Z_q^d}
\newcommand{\diag}[1]{\operatorname{diag}\( #1 \)}
\newcommand{\chiN}{\chi_{N}}
\newcommand{\chiM}{\chi_{M}}
\newcommand{\chiNs}{\chi_{N^*}}
\newcommand{\chiMs}{\chi_{M^*}}
\newcommand{\diagz}{\diag{\chi_0}}
\newcommand{\diagN}{\diag{\chiN}}
\newcommand{\diagM}{\diag{\chiM}}
\newcommand{\diagNs}{\diag{\chiNs}}
\newcommand{\diagMs}{\diag{\chiMs}}
\newcommand{\diagnu}{\diag{\nu}}
\newcommand{\diagmu}{\diag{\mu}}
\newcommand{\vone}{\mathbf{1}}

\title{Cliques in Association Schemes}
\author{
  Andrew Nagarajah \\
  Supervised by: Prof. Mike Newman
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}

  This report is an exploration of some of the topics within \emph{Delsarte
  theory}, which uses ideas from graph theory, algebra, and optimization to
  address questions in coding theory in particular, and combinatorics more
  broadly.  The centre of study is the \emph{association scheme}, which provides
  a setting in which to view various objects, especially \emph{distance-regular
  graphs}.  This perspective enables the computation of various parameters of
  interest, including the eigenvalues of graphs and upper bounds on codes,
  cliques, and independent sets.

  This report aims to be mostly self-contained, though background knowledge of
  basic linear algebra and group theory is required.  The important aspects of
  this theory is reviewed in the appendix.

\end{abstract}

\tableofcontents

\chapter{Introduction}
  \section{Coding Theory}
    I'm also not sure if I should include a section like this, but seeing as
    coding theory was the original motivation behind Delsarte's LP bound,
    and it remains (presumably?) a strong motivator for this theory,
    I figured it might be interesting to mention this as an application.

    \begin{itemize}
      \item Basics of Coding Theory
      \item Linear Graphs
      \item Finite Vector Spaces
    \end{itemize}

  \section{Hamming Graphs}

  \section{Distance-Regular Graphs}
    I'm not sure if I should maybe merge this section with $P$-polynomial
    section?

    \begin{itemize}
      \item Definition
      \item Basic parameters
      \item Examples?
    \end{itemize}

\chapter{Association Schemes}
  \section{Association Schemes}
    \begin{itemize}
      \item Definition(s)
      \item Examples?
      \item Basic parameters
    \end{itemize}

    \subsection{The Bose-Mesner Algebra}

    \subsection{Duality}

  \section{$P$-Polynomial Schemes}
    \begin{itemize}
      \item Definitions
      \item ``Equivalence" to DRGs
    \end{itemize}

    If we want to focus on the LP bound and translation schemes, I'm not sure
    that this section is necessary, but it is very interesting and provides an
    important class of examples.

    \subsection{$Q$-Polynomial Schemes}

  \section{Automorphisms and Cayley Graphs}
    This section may be merged with the following section.
    \begin{itemize}
      \item Action of a regular group of automorphisms
      \item Cayley graphs
      \item Eigenspaces from characters
    \end{itemize}
    
    The material in this section follows mostly from \cite[Chapter~9]{godsil}.
    \\

    This section and the next describes a special class of symmetric graphs
    (respectively, association schemes).  As with many other mathematical
    structures, the \textit{symmetry} of graphs (or schemes) is made precise by
    examining its automorphisms -- those transformations of the object in
    question which leave its structure unchanged.  Graphs (or schemes) with
    certain automorphisms may be classified in this way.

    More importantly for the purposes of this report, the structure revealed by
    the automorphisms of graphs (or schemes) allows one to compute their eigenvalues
    significantly more efficiently than otherwise would be the case, as outlined
    in the previous sections.

    \begin{defn}[Automorphism]\label{auts}
      For graphs $\Gamma, \Gamma'$,
      a map $\phi: V\(\Gamma\) \to V\(\Gamma'\)$ is a \textsc{homomorphism}
      if
      $$
        \forall u, v \in V\(\Gamma\) \
        u \sim_\Gamma v \implies \phi(u) \sim_{\Gamma'} \phi(v)
        \ .
      $$
      An \textsc{isomorphism} is an invertible homomorphism
      whose inverse is also a homomorphism;
      an \textsc{automorphism} is an isomorphism from a graph to itself.
      $\Aut \Gamma$ denotes the set of all automorphisms on $\Gamma$;
      it is the subgroup of $\Sym V\(\Gamma\)$,
      consisting of those permutations which preserve the
      (edge) structure of $\Gamma$.

      An automorphism of an association scheme $\AS$ on vertex set $X$
      is a map $X \to X$ which is simultaneously an automorphism of
      every class in the scheme.
      In other words, $\Aut \AS$ is the intersection of the automorphism groups
      of each class.
    \end{defn}

    \begin{defn}[Cayley Graphs]\label{cayley-graph}
      Given any group $G$ and a subset $C \subseteq G$,
      then $C$ is \textsc{inverse-closed} if for each $g \in C$,
      $g^{-1} \in C$ as well.

      If $C \subseteq G$ is an inverse-closed subset of a group $G$,
      then the \textsc{Cayley graph} of $G$ with respect to $C$ is denoted
      $\Cay(G, C)$, and defined as follows:
      \begin{itemize}
        \item Its vertex set is $G$
        \item $g \sim h$ in $\Cay(G, C)$ if and only if $gh^{-1} \in C$
      \end{itemize}

      Since $C$ is inverse closed, $gh^{-1} \in C \iff hg^{-1} \in C$
      so that $\Cay(G, C)$ is undirected.

      Furthermore, if $1_G \not\in C$, then $g \not\sim g$ so that the graph is
      loopless.  (By definition it already lacks parallel edges.)
    \end{defn}

    Because Cayley graphs are defined from groups using only the group
    structure, it is intuitive that these graphs should be highly symmetric.
    For example, every Cayley graph is \textsc{vertex transitive}: for every
    pair $u, v$ in the vertex set, there is a automorphism taking $u \mapsto v$.
    To see this, note that $G$ acts (\ref{group-action}) on $\Cay(G, C)$ through
    the group operation, since the vertex set is also the group.
    To verify that this is a homomorphism, if $u \sim v$ in $\Cay(G, C)$, then 
    $$
      uv^{-1} \in C
      \implies (ug)(vg)^{-1} = ugg^{-1}v^{-1} = u 1 v^{-1} = u v^{-1} \in C
    $$
    so that $ug \sim vg$.
    Then, $\Cay(G, C)$ is clearly vertex transitive if $G$ acts transitively,
    and for any vertices $u, v$, the group element $u^{-1} v$ maps $u$ to $v$.
    Moreover, this action is free, since if $ug = u$, then the group
    cancellation law implies that $g$ is trivial.
    Together, this implies that the action of $G$ is regular,
    which suggests the following lemma which provides a characterization of
    Cayley graphs.

    (This action is also faithful since for $g \neq h \in G$, the vertex $1$
    gets mapped to $g$ and $h$ respectively, which are unequal.  However this
    observation irrelevant for this lemma, since it restricts to the action of
    an automorphism group, which is automatically faithful.)

    \begin{lem}\label{regular-aut-cayley}
      For a graph $\Gamma$, there exists a subgroup $G \leq \Aut \Gamma$
      which acts regularly on $\Gamma$
      if and only if $\Gamma \cong \Cay(G, C)$
      for some inverse-closed $C \subseteq G$.
    \end{lem}

    Since the above argument demonstrates the reverse implication,
    only the forward direction will be shown here.

    Before beginning the proof, it will be worthwhile to note the neighbours
    of $1_G$ in $\Cay(G, C)$: $g \sim 1_G$ precisely when $g1^{-1} = g \in C$.

    \begin{proof}
      Choose a vertex $v \in V(\Gamma)$ to identify with $1_G$.  (This choice
      will not matter in the end, as Cayley graphs are vertex transitive.)
      Since the action is regular, for each $u \in V(\Gamma)$ there exists a
      unique $g_u \in G$ such that $vg_u = u$ (\ref{regular-unique}).

      Then define
      $$
        C := \buildset{g \in G}{vg \sim v}
      $$
      and observe that for $u, w \in V(\Gamma)$,
      $u g_u^{-1} = v$, and $w = v g_w \implies w g_u^{-1} = v g_w g_u^{-1}$.
      So, since $g_u^{-1}$ is an automorphism of $\Gamma$,
      $$
        u \sim w
        \iff u g_u^{-1} \sim w g_u^{-1}
        \iff v \sim v g_w g_u^{-1}
        \iff g_w g_u^{-1} \in C
        \ .
      $$

      Therefore, the map $u \mapsto g_u$ is the desired isomorphism $\Gamma \to
      \Cay(G, C)$.
    \end{proof}

    As promised at the beginning of the section, the next lemma demonstrates
    (for graphs) how automorphisms may be used to derive eigenvalues, and
    moreover, their eigenvectors.
    Naively, computing the eigenvalues of a matrix $A$ involves solving its
    characteristic polynomial, which is generically difficult.
    Then for an eigenvalue $\theta$, finding a $\theta$-eigenvector involves
    computing the kernel of $A - \theta I$, which can be computed in polynomial
    time (though not in linear time), and fast numeric algorithms are typically
    inexact.  (TODO citation)

    However, given the right information about a group, the following result
    finds the eigenvectors and eigenvalues almost instantaneously.

    \begin{lem}\label{cayley-eigen}
      Let $G$ be a finite abelian group,
      let $C \subseteq G \setminus \set{1}$ be inverse-closed,
      and define $\Gamma := \Cay(G, C)$.
      Then the rows of the character table of $G$ provide a complete set of
      eigenvectors for the adjacency matrix $A$ of $\Gamma$.
      Specifically, if $\psi$ is a character of $G$ (equivalently, a row of
      its character table), then $\psi(C)$ is the eigenvalue of $\psi$.
    \end{lem}

    \begin{proof}
      Note first that the neighbours $h \sim g$ of a vertex $g \in G$ consist of
      precisely the set $\buildset{cg}{c \in C} = Cg$ since $h \sim g \iff
      hg^{-1} \in C$, and multiplication by $g$ is invertible.

      As in (\ref{character-vector}), characters are identified with row vectors
      such that $\psi(g) \rightsquigarrow \psi_g$.

      Then
      $$
        (A\psi)_g = \sum_{h \in G} A_{g, h} \psi(h)
        = \sum_{h \sim g} \psi(h) = \sum_{c \in C} \psi(cg)
        = \psi(g) \sum_{c \in C} \psi(c) = \psi_g \psi(C)
        \ .
      $$

      Furthermore, since the rows of the character table are orthogonal,
      the eigenvectors $\psi$ are linearly independent,
      and since $G \cong G^*$ (\ref{character-duality}) implies that the character table is square,
      the rows form a basis of eigenvectors for $A$.
    \end{proof}

    TODO How to get real eigenvectors out of this?

  \section{Partitions and Translation Schemes}
    \begin{itemize}
      \item Equitable partitions of matrices
      \item Group partitions yielding association schemes
      \item Dual schemes? (Interesting, but not particularly necessary for the
        rest of this report)
    \end{itemize}

    In a sense, this section generalizes the characterization of Cayley graphs
    from the previous section to the setting of association schemes.
    Throughout this section, a transitive, abelian group of automorphisms will
    replace the regular automorphism group which corresponds to a Cayley graph.
    As per (\ref{faithful-transitive-abelian}) the transitive, abelian group
    will act regularly, so that (\ref{regular-aut-cayley}) still applies.
    This motivates the following definition.

    \begin{defn}[Translation Schemes]\label{translation-scheme}
      A \textsc{translation scheme} is an association scheme whose
      automorphism group contains a transitive, abelian subgroup.
    \end{defn}

    \begin{lem}\label{translation-partition}
      If $\AS$ is a translation scheme,
      and $G$ is a transitive, abelian automorphism group,
      then there is a partition into inverse-closed sets
      $C_0, C_1, \ldots, C_d$ of $G$ where $C_0 = \set{1}$,
      and each graph $\Gamma_i$ in $\AS$ is isomorphic to $\Cay(G, C_i)$.
    \end{lem}

    \begin{proof}
      Since $G$ is abelian and is a transitive subgroup of $\Aut \Gamma_i$ for
      each $i = 0, 1, \ldots, d$, $G$ acts regularly on $\Gamma_i$.
      Therefore, by (\ref{regular-aut-cayley}), there exists an inverse-closed
      set $C_i \subseteq G$ such that $\Gamma_i \cong \Cay(G, C_i)$.

      In particular, since the edges of $\Gamma_0$ are the diagonal relation,
      $C_0 = \set{1}$ generates the graph.

      Otherwise, it suffices to show that $C_0, C_1, \ldots, C_d$ partition $G$.
      Recall from the proof of (\ref{regular-aut-cayley}) that any vertex may be
      chosen to identify with $1_G$, so that the same vertex (say, $v$)
      may be chosen for each graph $\Gamma_i$ without loss of generality,
      in which case $C_i$ consists of the neighbours of $v$.  By the definition
      of an association scheme, for each vertex $u$ there is exactly one
      graph $\Gamma_i$ in which $u \sim v$, so that for each vertex, there is
      exactly one $C_i$ containing it.
    \end{proof}

    In order to characterize the translation schemes in a similar manner to the
    Cayley graphs, an examination of partitions of matrices and groups will be
    required.  This will lead to a simple criterion that distinguishes those
    partitions which generate a translation scheme translation scheme from those
    which do not.  \cite[Section~12.10]{godsil}

    \begin{defn}[Partition Matrix]\label{partition-matrix}
      If $\sigma$ is a partition of a set $X$,
      then the \textsc{partition matrix} of $\sigma$
      is the $01$ matrix
      whose rows are indexed by the elements of $X$,
      and whose columns are indexed by the parts of $\sigma$,
      in which each row -- corresponding to $x \in X$ -- has exactly one $1$,
      in the column corresponding to the part that contains $x$.
    \end{defn}

    Any partition matrix may be obtained from an $X \times X$ identity matrix by
    merging the columns which correspond to elements in the same part.
    Note that this implies that the columns are linearly independent.
    (The rows will \textbf{not} be linearly independent unless the partition
    is induced by the diagonal relation.)

    Given any matrix $H$, if $\sigma$ is a partition of the columns with
    partition matrix $\chi(\sigma)$ then the \textsc{induced row partition}
    $\sigma^*$ is the partition of the rows of $H$ such that two rows are in the
    same part if and only if the corresponding rows in $H\chi(\sigma)$ are
    equal.  In other words, if $f$ is the function which maps each row index
    $i$ of $H$ to the row vector $(H\chi(\sigma))_i$, then $\sigma^*$ is the
    partition given by the fibres of $f$.
    \cite[Section 12.7]{godsil}

    \begin{lem}[{{\cite[Section 12.7]{godsil}}}] \label{larger-row-partition}
      If $\sigma$ is a column partition of a matrix $H$ with full column-rank,
      and $\sigma^*$ is the induced row partition,
      then $\abs{\sigma^*} \geq \abs{\sigma}$.
    \end{lem}

    \begin{proof}
      Let $S$ be the partition matrix of $\sigma$.
      If $HSx = 0$ then since the columns of $H$ are linearly independent,
      $Sx = 0$, and since the columns of $S$ are linearly independent,
      $x = 0$.  Therefore the columns of $HS$ are linearly independent.
      Since there are $\abs{\sigma}$ columns of $HS$,
      $\rank HS = \abs{\sigma}$.

      Similarly, if $R$ is the partition matrix of $\sigma*$,
      then $\rank R = \abs{\sigma^*}$.  Therefore, to prove $\abs{\sigma} \leq
      \abs{\sigma^*}$ it suffices to show that $\col HS \subseteq \col R$.

      However, $\sigma^*$ is defined so that whenever two rows of $HS$ are equal,
      the rows will belong to the same part of $\sigma^*$,
      and thus the rows will be equal in $R$
      (this should be clear upon contemplating the definition of $R$).  
      More importantly for this proof, the converse holds:
      whenever two rows of $R$ are equal, the corresponding rows of $HS$ must
      also be equal.
      Therefore, for any vector in the column space of $HS$,
      where $R$ has equal rows the vector is guaranteed to have equal
      components (since those rows of $HS$ will be equal)
      so that the vector will also belong to the column space of $R$.
    \end{proof}

    Note in fact that the same argument will apply to any refinement of
    $\sigma*$, and will show that $\col HS \not\subseteq \col R$ for any
    partition which is not a refinement of $\sigma^*$.
    \cite[Section 12.7]{godsil}

    TODO

  \section{The Eigenvalues of the Hamming Scheme}
    Let $\Zq$ denote the cyclic group of order $q$ (written additively),
    and consider the direct product $\Zqd$ with subsets
    $$
      C_i := \buildset{x \in \Zqd}{\text{there are exactly $i$ $0$'s in $x$}}
      \ .
    $$
    In particular, $C_0 = \set{0}$ and
    $$
      C_1 = \Zqz \times \set{0}^{d - 1}
      \ \bigsqcup\ \set{0} \times \Zqz \times \set{0}^{d - 2}
      \ \bigsqcup\ \cdots \quad\bigsqcup\ 
      \set{0}^{d - 1} \times \Zqz 
      \ .
    $$

    Then $x, y$ are $i^\text{th}$ associates if and only if $x - y \in C_i$;
    that is, the Hamming distance between $x$ and $y$ is $i$,
    so that this partition yields the Hamming scheme.
    In particular, $H(d, q) \cong \Cay(\Zqd, C_1)$.

    For a character of the group $\psi \in (\Zqd)^*$ (\ref{character}) and
    $x = (x_1, \ldots, x_d) = \sum_{i=1}^d x_i e_i$ in the group
    (here $e_i$ is the tuple of all zeroes, and a $1$ in the $i^\text{th}$
    spot), then since $\psi$ is a homomorphism,
    $$
      \psi(x)
      = \prod_{i=1}^d \psi(x_i e_i)
      = \prod_{i=1}^d \psi\(\sum_{j=1}^{x_i} e_i\)
      = \prod_{i=1}^d \prod_{j=1}^{x_i} \psi(e_i)
      = \prod_{i=1}^d \psi(e_i)^{x_i}
    $$
    so that $\psi$ is completely determined by its values on $e_1, \ldots, e_d$.

    Let $\omega$ be a primitive $q^\text{th}$ root of unity,
    so that $1 = \omega^0, \omega^1, \ldots, \omega^{q-1}$ are distinct.
    Then every choice in  $\set{\omega^0, \omega^1, \ldots, \omega^{q-1}}^d$
    will yield a distinct character
    by assigning the $i^\text{th}$ entry to $\psi(e_i)$
    (identifying $\psi$ with the tuple as in (\ref{character-vector})),
    and defining the value of $\psi$ at all other $x = (x_1, \ldots, x_d)$ by
    $$
      \psi(x) := \prod_{i=1}^d \psi(e_i)^{x_i} \ .
    $$
    Note that this is a homomorphism since
    $$
      \psi(x + y)
      = \prod_{i=1}^d \psi(e_i)^{x_i + y_i}
      = \prod_{i=1}^d \psi(e_i)^{x_i} \psi(e_i)^{y_i}
      = \prod_{i=1}^d \psi(e_i)^{x_i} \prod_{i=1}^d \psi(e_i)^{y_i}
      = \psi(x) \psi(y) \ .
    $$

    Therefore, $(\Zqd)^* \cong \set{\omega^0, \ldots, \omega^{q-1}}^n$
    (taking entrywise multiplication as the group product on the right),
    so that the characters $\psi$ will be identified with row vectors
    $$
      \psi \rightsquigarrow
      \begin{bmatrix}
        \omega^{\psi_1} & \cdots & \omega^{\psi_d}
      \end{bmatrix}
    $$
    where $\psi_i \in \set{0, 1, \ldots, q - 1}$.

    (Note that this notation deviates from (\ref{character-vector}),
    but will be more convenient for this purpose.
    In fact, this shows that $(\Zqd)^* \cong \Zqd$ directly,
    confirming (\ref{character-duality}).)

    Then, in the $i^\text{th}$-distance Hamming graph,
    $\psi$ is a $\psi(C_i)$-eigenvalue (\ref{cayley-eigen}),
    and for the Hamming graph, it can be computed directly.
    \begin{alignat*}{3}
      \psi(C_1)
      &= \sum_{c \in C_1} \psi(c) \\
      &= \sum_{i=1}^d \sum_{j=1}^{q-1} \psi(j e_i)
      \quad\text{
        here, the outer sum picks which entry of $c$ will be non-zero} \\
      &\qquad\qquad\qquad\qquad\text{
        \ and the inner sum picks the value
      }\\
      &= \sum_{i=1}^d \sum_{j=1}^{q-1} \psi(e_i)^j
      = \sum_{i=1}^d \sum_{j=1}^{q-1} \(\omega^{\psi_i}\)^j \\
      &= \sum_{i=1}^d
        \(\frac{1 - \(\omega^{\psi_i}\)^q}{1 - \omega^{\psi_i}} - 1\)
      \quad\text{using the usual formula for geometric sums} \\
      &= \sum_{i=1}^d (q-1)\delta_{0, \psi_i} - d \\
      &= (q-1)\(\text{the number of $i$ with $\psi_i = 0$}\) - d \\
    \end{alignat*}

    Since the number $k$ of indices $i$ for which $\psi_i = 0$
    can vary from $0, 1, \ldots, d$,
    and there are $\binom{d}{k}$ places $i$ at which $\psi_i = 0$
    and $(q - 1)^{d - k}$ choices for the other $\psi_j$,
    the eigenvalues of the Hamming graph are given
    \begin{equation}
      \begin{cases}
        qk - d & k = 0, 1, \ldots, d \\
        \binom{d}{k}(q - 1)^{d - k} & \quad\text{(multiplicy)} \\
      \end{cases} \ .
    \end{equation}

\chapter{Delsarte's Linear Programming Bound}
  \section{Linear Programming}
    \begin{itemize}
      \item Basics of Linear Programming -- done
      \item Duality -- done
      \item Algorithms?
    \end{itemize}

    The terminology and results from this section,
    except for the adjective \textit{principal} for
    constraints, follows from \cite{matousek}.

    A \textsc{linear programming problem} (or \textsc{linear program})
    is an optimization problem in which one seeks to maximise or minimize
    a linear function of one or more variables, subject to linear constraints.
    That is, fixing a vector $c$, one tries to maximize or minimize
    the linear combinations of the components of $c$:
    $$
      c_1 x_1 + \cdots + c_n x_n = c^T x
    $$
    for some $x$.
    Note that maximizing $c^T x$ is equivalent to minimizing $(-c)^T x$,
    so that for the theory of linear programming,
    it suffices to consider maximization problems without loss of generality.
    As in other optimization problems,
    the function to be maximized ($c^T x$ in this case)
    is called the \textsc{objective (function)}.

    In most cases, there will be contraints on the inputs to the objective
    function, and for the purposes of linear programming these will also have
    to be linear.
    That is, there will be a matrix $A$ and vector $b$
    such that only inputs $x$ satisfying $Ax \leq b$ will be allowed.
    (Note that for \textit{vectors} $a$ and $b$,
    $a \leq b$ will mean that each component $a_i$
    is less than or equal to the corresponding component $b_i$.)
    These are called the \textsc{(principal) constraints},
    and vectors $x$ which satisfy the constraints will be called
    \textsc{feasible (solutions)}.
    (Note that in \cite{delsarte}, the term \textit{program}
    is used to refer to a feasible solution.)

    If there are no constraints on the problem
    (and even in some cases where there are)
    through appropriate choices of feasible solution $x$,
    the objective $c^Tx$ may be made arbitrarily large,
    and such problems are called \textsc{unbounded}.
    Conversely, if no feasible solutions exist,
    then the problem is called \textsc{infeasible}.

    Finally, in most applications of linear programming --
    in particular to the cliques of association schemes --
    the feasible solutions will be further constrained
    to those with all non-negative components (i.e. $x \geq 0$).
    These are called the \textsc{non-negativity constraints},
    in constrast with the \textit{principal constraints}.
    The non-negativity constraints will be required throughout the remainder of
    this report.

    Therefore, for an objective $c^T x$ and constraints $Ax \leq b$,
    the associated linear program will be written in \textsc{standard form}:
    $$
      \max\buildset{
        c^T x
      }{
        A x \leq b,\
        x \geq 0
      }
      \ .
    $$

    \subsection{Duality}

      The most important observation about linear programs
      (for the purposes of this report, at least)
      is that they come in dual pairs.

      Given a linear program $\mathcal{P}$ written in standard form
      $$
        \max\buildset{
          c^T x
        }{
          A x \leq b,\
          x \geq 0
        }
      $$
      its \textsc{dual} program is $\mathcal{P}^*$:
      $$
        \min\buildset{
          b^T y
        }{
          A^T y \geq c,\
          y \geq 0
        }
        \ .
      $$
      Re-writing it in standard form,
      $$
        \max\buildset{
          (-b)^T y
        }{
          -A^T y \leq -c,\
          y \geq 0
        }
      $$
      taking the dual
      $$
        \min\buildset{
          -c^T x
        }{
          -A x \geq -b,\
          x \geq 0
        }
      $$
      and re-writing in standard form
      $$
        \max\buildset{
          c^T x
        }{
          A x \leq b,\
          x \geq 0
        }
      $$
      the original (called \textsc{primal}) linear program is recovered.

      This demonstrates that $\(\mathcal{P}^*\)^* = \mathcal{P}$,
      so that linear programs come in dual pairs.

      \begin{thm}[Weak Duality]\label{weak-duality}
        If $x$ is a feasible solution to a linear program
        $$
          \max\buildset{
            c^T x
          }{
            A x \leq b,\
            x \geq 0
          },
        $$
        and $y$ is a feasible solution to its dual program,
        $$
          \min\buildset{
            b^T y
          }{
            A^T y \geq c,\
            y \geq 0
          },
        $$
        then $c^T x \leq b^T y$.
      \end{thm}

      \begin{proof}
        Let $u, v, w$ be vectors with $u \geq 0$, and $v \leq w$.
        Then for all components $i$,
        $u_i \geq 0$ and $v_i \leq w_i$ implies that $u_i v_i \leq u_i w_i$
        so that
        $$
          u^T v = \sum_i u_i v_i
          \leq \sum_i u_i w_i = u^T w
          \ .
        $$

        In particular, since $y$ is a feasible solution to the dual program,
        and $x \geq 0$,
        $$
          c \leq A^T y 
          \implies x^T c \leq x^T A^T y = y^T A x
          \ .
        $$
        (Here one may take the transpose of the whole expression,
        since the result is a scalar.)
        Similarly, since $x$ is a feasible solution to the primal program,
        and $y \geq 0$,
        $$
          b \geq A x 
          \implies y^T b \geq y^T A x
          \ .
        $$
        By combining the two inequalities,
        $$
          b^T y = y^T b \geq y^T A x \geq x^T c = c^T x
        $$
        which is the desired result.
      \end{proof}

      As a result of the weak duality of linear programs,
      every feasible solution to the dual program
      provides an upper bound on the maximum of the primal,
      and every feasible solution to the primal program
      provides a lower bound on the minimum of the dual.

      In fact the extremal values of dual programs
      (the maximum of the primal, and the minimum of the dual)
      coincide, although this will not be needed for the purposes of this
      report.
      This is referred to as \textit{Strong Duality} of linear programs.

  \section{The LP Bound}

    \begin{defn}
      The \textsc{inner distribution} is TODO.
      I might also put this in the section on association schemes;
      I'm not sure if it belongs better there or here.
    \end{defn}

    \begin{thm}[Delsarte Thm 3.3 \cite{delsarte}]\label{lp-ineq}
      For any inner distribution $y$,
      $$
        Q^T y \geq 0
      $$
      where $Q$ is the matrix of dual eigenvalues.
      (Here, $x \geq 0$ means that each component of
      the vector $x$ is not less than $0$.)
    \end{thm}

    \begin{proof}
      TODO.
      Note that this will require a number of lemmas which I've omitted here for
      brevity, but will include in the final product.
    \end{proof}

    This theorem provides the key inequality that will allow the application of
    linear programming to cliques in association schemes.
    However, because the constraint vector in a primal linear program
    becomes the objective in the dual program,
    this inequality will require some transformation to make it suitable for use
    in linear programming.

    Let $Y$ be an $M$-clique with inner distribution $y$.
    Then $y_i = 0$ for all $i \not\in M$,
    so $Q^T y \geq 0 \iff Q^T \diagM y \geq 0$
    since the action of $\diagM$ acting on the left
    is to zero out the \textit{rows} of $y$
    with index not in $M$.
    Similarly,
    $$
      Q^T \diagM y
      = Q(0)^T y_0 + Q^T \diagMs y
      = \mu + Q^T \diagMs y
    $$
    since the action of $\diagMs$ on the right
    is to zero out the \textit{columns} of $Q^T$
    with index not in $M^*$,
    $y_0 = 1$, and
    $$
      Q^T =
      \begin{bmatrix}
        1 & 1 & \cdots & 1 \\
        \mu_1 & & & \\
        \vdots & & * & \\
        \mu_d & & & \\
      \end{bmatrix}
      \ .
    $$
    Finally, since $y \geq 0$, $Q_0^T y \geq 0$ adds no new constraint,
    so that under the non-negativity constraint
    $Q^T y \geq 0 \iff \diagNs Q^T y \geq 0$.

    Putting all this together,
    $Q^T y \geq 0 \iff \diagNs Q^T \diagMs y \geq -\mu$
    so that Delsarte's LP can be written in standard form:
    \begin{alignat}{2}
      & \max\buildset{
        \vone^T \diagM y
      }{
        Q^T \diagM y \geq 0,\
        y \geq 0,\
        y_0 = 1
      } \label{dlp-primal} \\
      =& \max\buildset{
        \chiMs^T y
      }{
        - \diagNs Q^T \diagMs y \leq \diagNs \mu,\
        y \geq 0
      } + 1 \label{dlp-primal-std}
      \ .
    \end{alignat}

    Taking the dual yields
    \begin{alignat}{2}
      & \min\buildset{
        \mu^T \diagNs z
      }{
        - \diagMs Q \diagNs z \geq \chiMs,\
        z \geq 0
      } + 1 \label{dlp-dual-std} \\
      =& \min\buildset{
        \mu^T z
      }{
        - \diagMs Q \diagNs z \geq \chiMs,\
        z \geq 0,\
        z_0 = 1
      }
      \ .
    \end{alignat}
    Therefore, if $z_0 = 1$ is required,
    recalling that $Q_0 = \vone$ and $\diagMs \vone = \chiMs$, then
    \begin{alignat*}{2}
      & \diagMs Q \diagN z \\
      =& \diagMs \( Q_0 z_0 + Q \diagNs z \) \\
      =& \chiMs + \diagMs Q \diagNs z \\
      \leq& 0
      \ .
    \end{alignat*}
    This equivalence recover's Delsarte's formulation of
    the dual linear program:
    \begin{equation}\label{dlp-dual}
      \min\buildset{
        \mu^T z
      }{
        \diagMs Q z \leq 0,\
        z \geq 0,\
        z_0 = 1
      }
      \ .
    \end{equation}

  \section{The Ratio Bound}
    We will use (\ref{dlp-dual}) frequently.

  \section{The Clique-Coclique Bound}

\chapter{Schrijver's SDP Bound}
  \section{The Terwilliger Algebra of the Hamming Scheme}

  \section{Semi-Definite Programming}

\chapter{Computation}
  I wasn't sure if I ought to mention anything about the code I've written for
  this project (or even if there's anything worth saying that won't be covered
  elsewhere in the report).

  Also, if there are some specific results that would be interesting to show,
  but do not fit naturally into other sections of the report, then perhaps they
  could go here as well.

  \begin{itemize}
    \item Computing the character table of an abelian group
  \end{itemize}

\appendix

\chapter{Linear Algebra}
  \section{The Spectral Theorem}

  \section{Adjacency Matrices}
    Basic results about the spectra of adjacency matrices, which may be used
    elsewhere in the report.  E.g. the sum of eigenvalues with multiplicity, and
    consequences.

  \section{Positive Semi-Definite Matrices}
    Depending on which proof of the clique-coclique bound I use, and how much
    detail I go into Schrijver's SDP bound, I could make some comments about PSD
    matrices.

\chapter{Group Theory}
  \section{Group Actions}
    The material of this section comes primarily from \cite[Section~1.7;
    Chapter~4]{dummit-foote}.

    \begin{defn}[Group Action]\label{group-action}
      Given a group $G$ and a set $X$,
      \textsc{group action} is a homomorphism $G \to \Sym X$,
      where $\Sym X$ is the symmetric group on $X$.
    \end{defn}

    A group action $\phi: G \to \Sym X$ induces a product
    $X \times G \to X$ by mapping $(x, g) \mapsto \phi(g)(x)$.
    When the action is clear from context,
    this will be denoted $x \cdot g$, or simply $xg$.
    This is called a \textsc{right action},
    as $g$ \textit{acts on the right of} $x$
    (the corresponding notion of a \textsc{left action}
    can also be defined.)

    Conversely, given a product $X \times G \to X$,
    the same expression defines a map $G \to \Sym X$.
    If such a product satisfies
    \begin{alignat*}{2}
      &\forall x \in X\ x 1_G = x \\
      \text{and } &\forall x \in X\ \forall g, h \in G\
        (x g) h = x(gh)
    \end{alignat*}
    then the induced map $G \to \Sym X$ will be a homomorphism,
    so that these definitions are equivalent.

    (In \cite{dummit-foote}
    this is taken as the definition of a group action,
    and the homomorphism $G \to \Sym X$ is called its \textsc{permutation
    representation}.  It will be occasionally convenient to adopt each
    perspective.)

    \begin{defn}[Types of Group Actions]\label{group-action-types}
      If if a homomorphism $G \to \Sym X$ is injective,
      then the action is called \textsc{faithful}.
      Note that a group homomorphism is injective
      if and only if it has a trivial kernel.

      Given a group action $G \to \Sym X$,
      $g \in G$ is called \textsc{fixed point-free}
      if $\forall x \in X\ xg \neq x$.
      The group action itself is called \textsc{fixed point-free}
      (or just \textsc{free}) if all its nontrivial elements
      are fixed point-free.

      A group action $G \to \Sym X$ is called \textsc{transitive}
      if $\forall x, y \in X$ there exists some $g \in G$
      such that $xg = y$.

      A group action is called \textsc{regular} if it is simultaneously
      transitive and free.  (This terminology follows \cite{godsil}.)
    \end{defn}

    Note that if $X$ is a structure with automorphisms
    (such as a graph or group), $G$ is a subgroup of $\Aut X$,
    and $G$ acts in the natural way on $X$ (i.e. $xg = g(x)$),
    then this action is faithful.
    That is, $\Aut X \leq \Sym X$,
    so that this action is induced by the identity $G \into \Sym X$,
    which is clearly injective.

    \begin{lem}\label{faithful-transitive-abelian}
      If an abelian group $G$ acts faithfully and transitively on a set $X$,
      then the action is free, and thus also regular. \cite[Section 4.1,
      Exercise 3]{dummit-foote}
    \end{lem}

    \begin{proof}
      Let $g \in G$ be nontrivial, and $x \in X$.
      The goal is to prove that $xg \neq x$.

      Since $g$ is not the identity,
      there exists some $y \in X$ such that $z := yg \neq y$.
      Furthermore, since $G$ acts transitively on $X$,
      there exists some $h \in G$ such that $yh = x \iff y = xh^{-1}$.
      Then,
      \begin{alignat*}{3}
        xg &= (yh) g &\\
        &= y (hg) &\\
        &= y (gh) \quad&\text{ since $G$ is abelian}\\
        &= (yg) h &\\
        &= zh \ .&\\
      \end{alignat*}

      If $zh = x$ then, $z = xh^{-1} = y$,
      but by definition, $z = yg \neq y$,
      so $xg = zh \neq x$.
    \end{proof}

    An alternate characterization of regular actions will be useful in this
    report.  To see this, note that for a pair $x, y \in X$, there exists a $g
    \in G$ such that $xg = y$ by transitivity; for any $g' \in G$ satisfying
    $xg' = y$, 
    $$
      xg = xg' \implies x = x g'g^{-1}
    $$
    so that $g'g^{-1} = 1$ since the action is free, and so $g' = g$.
    Conversely, if for each $x, y \in X$ there existed a unique $g \in G$
    satisfying $xg = y$, then the action would clearly be transitive; since $x 1
    = x$, $1$ is the unique group element fixing any point, so the action must
    be free.

    \begin{lem}\label{regular-unique}
      A group action $G$ on $X$ is regular if and only if
      for all $x, y \in X$, there exists a unique $g \in G$
      such that $xg = y$.
    \end{lem}

  \section{Character Theory}
    \begin{defn}[Characters]\label{character}
      Given a group $G$, a \textsc{character} of the group $G$
      is a homomorphism $G \to \C^\times$,
      the group of non-zero complex numbers under multiplication.
      Then $G^*$ will denote the set of characters of $G$.
      \cite[Chapter 8]{godsil}

      (For abelian groups, this corresponds to irreducible degree $1$ characters
      over $\C$ in \cite[Section 18.3]{dummit-foote}.)
    \end{defn}

    On $G^*$ a product of characters can be defined by setting
    $$
      \phi \psi: g \mapsto \phi(g) \psi(g)
    $$
    under which the character taking each $g \in G$ identically to $1$
    acts as identity.

    Furthermore, for any character $\psi \in G^*$, the map $g \mapsto
    \psi\(g^{-1}\)$ is a homomorphism since
    $$
      gh \longmapsto \psi\((gh)^{-1}\)
      = \psi\( h^{-1} g^{-1} \)
      = \psi\(h^{-1}\) \psi\(g^{-1}\)
      = \psi\(g^{-1}\) \psi\(h^{-1}\)
    $$
    and for any $g \in G$,
    $$
      \psi(g) \psi\(g^{-1}\)
      = \psi\(g^{-1}\) \psi(g)
      = \psi(1) = 1
    $$
    so that this homomorphism is an inverse for $\psi$.

    Therefore, $G^*$ forms a group under the above product of characters.
    \\

    For the remainder of this section (and the rest of this report),
    discussion of characters will be restricted
    to the case of finite abelian groups.
    Throughout this section, $G$ will denote
    a finite abelian group of order $n$.

    In this case, by Lagrange's theorem, $g^n = 1_G$ for every $g \in G$,
    and so for any character $\psi \in G^*$
    $$
      1 = \psi(1) = \psi\(g^n\) = \psi(g)^n
    $$
    -- that is, the image of each character is contained in
    the set of $n^\text{th}$ roots of unity.

    Since the inverse of a complex number with modulus $1$ is also its complex
    conjugate, looking at the inversion in $G^*$,
    $$
      \psi\(g^{-1}\) = \psi(g)^{-1} = \bar{\psi(g)}
    $$
    so that the inverse of $\psi \in G^*$ is $\bar{\psi}: g \mapsto
    \bar{\psi(g)}$.

    \begin{thm}\label{character-duality}
      For all finite abelian groups $G$,
      $$
        G \cong G^*
        \ .
      $$
    \end{thm}

    \begin{proof}
      TODO
    \end{proof}

    While by transitivity this shows that $G^{**} \cong G$,
    this can be seen more directly via the isomorphism
    $$
      g \mapsto \( \psi \mapsto \psi(g) \) \ .
    $$
    \\

    Given an ordering $G = \set{g_1, \ldots, g_n}$,
    the character $\psi \in G^*$ can be identified with the row vector
    \begin{equation}\label{character-vector}
      \psi \rightsquigarrow
      \begin{bmatrix}
        \psi(g_1) & \cdots & \psi(g_n) \\
      \end{bmatrix}
      \ .
    \end{equation}
    With this identification, the product of characters becomes the entrywise
    product of vectors (the \textit{Schur product} of $n \times 1$ matrices),
    and the inverse of a character in $G^*$ is the entrywise inversion of the
    vector.

    Furthermore, given an ordering $G^* = \set{\psi^1, \ldots, \psi^n}$,
    the matrix whose rows consist of the characters of $G^*$ is called the
    \textsc{character table} of $G$:
    \begin{equation}\label{character-table}
      H =
      \begin{bmatrix}
        \horzbar & \psi^1 & \horzbar \\
                 & \vdots &          \\
        \horzbar & \psi^n & \horzbar \\
      \end{bmatrix}
      \ .
    \end{equation}
    Remarkably, this matrix turns out to be (almost) unitary.
    \\

    For any subset $C \subseteq G$, define
    $$
      \psi(C) := \sum_{g \in C} \psi(g)
      = \psi \chi_C
    $$
    where $\chi_C$ is the characteristic vector of $C$ in $G$
    (with the same ordering).
    So, given characters $\psi, \phi$, their inner product can be written
    $$
      \psi \phi^* = \sum_{g \in G} \psi(g) \bar{\phi}(g)
      = \sum_{g \in G} \(\psi \bar{\phi}\) (g)
      = (\psi \bar{\phi}) \chi_G
      = (\psi \bar{\phi}) (G)
      \ .
    $$
    (Note here that $\phi^*$ denotes the conjugate transpose of $\phi$,
    and $\chi_G$ is also the all-ones column vector $\vone$.)

    \begin{lem}\label{characters-orthogonal}
      For any $\psi \in G^*$
      $$
        \psi(G) =
        \begin{cases}
          \abs{G} & \text{ if $\psi$ is the identity of $G^*$} \\
          0 & \text{ else.} \\
        \end{cases}
      $$
    \end{lem}

    \begin{proof}
      For any $h \in G$, since $g \mapsto hg$ is an automorphism of $G$,
      $hG = G$, so that
      $$
        \psi(G) = \sum_{g \in G} \psi(g)
        = \sum_{g \in G} \psi(hg)
        = \sum_{g \in G} \psi(h) \psi(g)
        = \psi(h) \sum_{g \in G} \psi(g)
        = \psi(h) \psi(G)
      $$
      which implies that either $\psi(h) = 1$ or $\psi(G) = 0$.

      But this holds for arbitrary $h \in G$, so that either
      $$
        \forall h \in G\ \psi(h) = 1
        \implies \psi = 1_{G^*}
        \quad\text{and}\quad
        \psi(G) = \sum_{g \in G} 1 = \abs{G}
      $$
      or else
      $$
        \exists h \in G\ \psi(h) \neq 1
        \implies \psi(G) = 0
        \ .
      $$
    \end{proof}

    \begin{cor}\label{character-table-unitary}
      If $H$ is the character table of a finite abelian group $G$ of order $n$,
      then $H H^* = nI$, where $H^*$ is the conjugate transpose of $H$.
    \end{cor}

    \begin{proof}
      If $\psi, \phi$ are characters of $G$, and $\psi \neq \phi$,
      then letting $\theta = \psi \bar{\phi}$, the $(\psi, \phi)$-entry of $HH^*$
      is given by $\psi \phi^* = \theta(G) = 0$, since $\theta$ is not the
      identity of $G^*$.

      However, for the diagonal, $(\psi, \psi)$-entries of $HH^*$,
      $\psi \psi^* = 1_{G^*}(G) = n$, which proves the claim.
    \end{proof}

  \section{The Structure of Finite Abelian Groups}
    TODO I need a citation for this.
    I'm also not sure if this section shouldn't go before the section on
    Character Theory, as it will be used in (\ref{character-duality}).

    \begin{thm}[Structure Theorem]\label{structure-theorem}
      If $G$ is a finitely generated abelian group,
      then $G$ is isomorphic to a direct product of cyclic groups.
      Specifically,
      $$
        G \cong
        \Z_{q_1}^{d_1} \times \cdots \times \Z_{q_t}^{d_t}
        \times \Z^f
      $$
      where the $q_i$ are distinct prime powers.
      Moreover, this decomposition is unique up to the ordering of its factors.
    \end{thm}

    This is a well-known and well-used result.  It comes as a direct result of
    the Structure Theorem for Finitely Generated Modules over PIDs, using the
    language of rings and modules, but this is out of the scope of this report.

\chapter{Notation}
  I've included these sections mostly as an excuse to add the citations to the
  bibliography, though it may be somewhat useful to have a sort of guide to the
  similarities and differences in notation in this report, as well as in the
  references.  (At least, I would find such a thing useful.)

  \section{Godsil}
    See \cite{godsil} Ch. 10.

  \section{Delsarte}
    See \cite{delsarte} Ch. 3.

  \section{Schrijver}
    See \cite{schrijver}.

\printbibliography[heading=bibintoc]

\end{document}
