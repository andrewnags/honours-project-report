\documentclass{report}

\usepackage{preamble}
\usepackage{prealgebra}
\usepackage{premath}
\usepackage{precomb}

\usepackage{biblatex}
\addbibresource{report.bib}

% Remove "chapter" heading
\usepackage{titlesec}
\titleformat{\chapter}{\normalfont\huge\bf}{\thechapter.}{20pt}{\huge\bf}

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\newcommand{\diag}[1]{\operatorname{diag}\( #1 \)}
\newcommand{\chiN}{\chi_{N}}
\newcommand{\chiM}{\chi_{M}}
\newcommand{\chiNs}{\chi_{N^*}}
\newcommand{\chiMs}{\chi_{M^*}}
\newcommand{\diagz}{\diag{\chi_0}}
\newcommand{\diagN}{\diag{\chiN}}
\newcommand{\diagM}{\diag{\chiM}}
\newcommand{\diagNs}{\diag{\chiNs}}
\newcommand{\diagMs}{\diag{\chiMs}}
\newcommand{\diagnu}{\diag{\nu}}
\newcommand{\diagmu}{\diag{\mu}}
\newcommand{\vone}{\mathbf{1}}

\title{Cliques in Association Schemes}
\author{
  Andrew Nagarajah \\
  Supervised by: Prof. Mike Newman
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}

  This report is an exploration of some of the topics within \emph{Delsarte
  theory}, which uses ideas from graph theory, algebra, and optimization to
  address questions in coding theory in particular, and combinatorics more
  broadly.  The centre of study is the \emph{association scheme}, which provides
  a setting in which to view various objects, especially \emph{distance-regular
  graphs}.  This perspective enables the computation of various parameters of
  interest, including the eigenvalues of graphs and upper bounds on codes,
  cliques, and independent sets.

\end{abstract}

\tableofcontents

\chapter{Introduction}
  \section{Coding Theory}
    I'm also not sure if I should include a section like this, but seeing as
    coding theory was the original motivation behind Delsarte's LP bound,
    and it remains (presumably?) a strong motivator for this theory,
    I figured it might be interesting to mention this as an application.

    \begin{itemize}
      \item Basics of Coding Theory
      \item Linear Graphs
      \item Finite Vector Spaces
    \end{itemize}

  \section{Hamming Graphs}

  \section{Distance-Regular Graphs}
    I'm not sure if I should maybe merge this section with $P$-polynomial
    section?

    \begin{itemize}
      \item Definition
      \item Basic parameters
      \item Examples?
    \end{itemize}

\chapter{Association Schemes}
  \section{Association Schemes}
    \begin{itemize}
      \item Definition(s)
      \item Examples?
      \item Basic parameters
    \end{itemize}

    \subsection{The Bose-Mesner Algebra}

    \subsection{Duality}

  \section{$P$-Polynomial Schemes}
    \begin{itemize}
      \item Definitions
      \item ``Equivalence" to DRGs
    \end{itemize}

    If we want to focus on the LP bound and translation schemes, I'm not sure
    that this section is necessary, but it is very interesting and provides an
    important class of examples.

    \subsection{$Q$-Polynomial Schemes}

  \section{Automorphisms}
    This section may be merged with the following section.
    \begin{itemize}
      \item Action of a regular group of automorphisms
      \item Cayley graphs
      \item Eigenspaces from characters
    \end{itemize}

  \section{Translation Schemes}
    \begin{itemize}
      \item Equitable partitions of matrices
      \item Group partitions yielding association schemes
      \item Dual schemes? (Interesting, but not particularly necessary for the
        rest of this report)
    \end{itemize}

  \section{The Eigenvalues of the Hamming Scheme}

\chapter{Delsarte's Linear Programming Bound}
  \section{Linear Programming}
    \begin{itemize}
      \item Basics of Linear Programming -- done
      \item Duality -- done
      \item Algorithms?
    \end{itemize}

    The terminology and results from this section,
    except for the adjective \textit{principal} for
    constraints, follows from \cite{matousek}.

    A \textsc{linear programming problem} (or \textsc{linear program})
    is an optimization problem in which one seeks to maximise or minimize
    a linear function of one or more variables, subject to linear constraints.
    That is, fixing a vector $c$, one tries to maximize or minimize
    the linear combinations of the components of $c$:
    $$
      c_1 x_1 + \cdots + c_n x_n = c^T x
    $$
    for some $x$.
    Note that maximizing $c^T x$ is equivalent to minimizing $(-c)^T x$,
    so that for the theory of linear programming,
    it suffices to consider maximization problems without loss of generality.
    As in other optimization problems,
    the function to be maximized ($c^T x$ in this case)
    is called the \textsc{objective (function)}.

    In most cases, there will be contraints on the inputs to the objective
    function, and for the purposes of linear programming these will also have
    to be linear.
    That is, there will be a matrix $A$ and vector $b$
    such that only inputs $x$ satisfying $Ax \leq b$ will be allowed.
    (Note that for \textit{vectors} $a$ and $b$,
    $a \leq b$ will mean that each component $a_i$
    is less than or equal to the corresponding component $b_i$.)
    These are called the \textsc{(principal) constraints},
    and vectors $x$ which satisfy the constraints will be called
    \textsc{feasible (solutions)}.
    (Note that in \cite{delsarte}, the term \textit{program}
    is used to refer to a feasible solution.)

    If there are no constraints on the problem
    (and even in some cases where there are)
    through appropriate choices of feasible solution $x$,
    the objective $c^Tx$ may be made arbitrarily large,
    and such problems are called \textsc{unbounded}.
    Conversely, if no feasible solutions exist,
    then the problem is called \textsc{infeasible}.

    Finally, in most applications of linear programming --
    in particular to the cliques of association schemes --
    the feasible solutions will be further constrained
    to those with all non-negative components (i.e. $x \geq 0$).
    These are called the \textsc{non-negativity constraints},
    in constrast with the \textit{principal constraints}.
    The non-negativity constraints will be required throughout the remainder of
    this report.

    Therefore, for an objective $c^T x$ and constraints $Ax \leq b$,
    the associated linear program will be written in \textsc{standard form}:
    $$
      \max\buildset{
        c^T x
      }{
        A x \leq b,\
        x \geq 0
      }
      \ .
    $$

    \subsection{Duality}

      The most important observation about linear programs
      (for the purposes of this report, at least)
      is that they come in dual pairs.

      Given a linear program $\mathcal{P}$ written in standard form
      $$
        \max\buildset{
          c^T x
        }{
          A x \leq b,\
          x \geq 0
        }
      $$
      its \textsc{dual} program is $\mathcal{P}^*$:
      $$
        \min\buildset{
          b^T y
        }{
          A^T y \geq c,\
          y \geq 0
        }
        \ .
      $$
      Re-writing it in standard form,
      $$
        \max\buildset{
          (-b)^T y
        }{
          -A^T y \leq -c,\
          y \geq 0
        }
      $$
      taking the dual
      $$
        \min\buildset{
          -c^T x
        }{
          -A x \geq -b,\
          x \geq 0
        }
      $$
      and re-writing in standard form
      $$
        \max\buildset{
          c^T x
        }{
          A x \leq b,\
          x \geq 0
        }
      $$
      the original (called \textsc{primal}) linear program is recovered.

      This demonstrates that $\(\mathcal{P}^*\)^* = \mathcal{P}$,
      so that linear programs come in dual pairs.

      \begin{thm}[Weak Duality]\label{weak-duality}
        If $x$ is a feasible solution to a linear program
        $$
          \max\buildset{
            c^T x
          }{
            A x \leq b,\
            x \geq 0
          },
        $$
        and $y$ is a feasible solution to its dual program,
        $$
          \min\buildset{
            b^T y
          }{
            A^T y \geq c,\
            y \geq 0
          },
        $$
        then $c^T x \leq b^T y$.
      \end{thm}

      \begin{proof}
        Let $u, v, w$ be vectors with $u \geq 0$, and $v \leq w$.
        Then for all components $i$,
        $u_i \geq 0$ and $v_i \leq w_i$ implies that $u_i v_i \leq u_i w_i$
        so that
        $$
          u^T v = \sum_i u_i v_i
          \leq \sum_i u_i w_i = u^T w
          \ .
        $$

        In particular, since $y$ is a feasible solution to the dual program,
        and $x \geq 0$,
        $$
          c \leq A^T y 
          \implies x^T c \leq x^T A^T y = y^T A x
          \ .
        $$
        (Here one may take the transpose of the whole expression,
        since the result is a scalar.)
        Similarly, since $x$ is a feasible solution to the primal program,
        and $y \geq 0$,
        $$
          b \geq A x 
          \implies y^T b \geq y^T A x
          \ .
        $$
        By combining the two inequalities,
        $$
          b^T y = y^T b \geq y^T A x \geq x^T c = c^T x
        $$
        which is the desired result.
      \end{proof}

      As a result of the weak duality of linear programs,
      every feasible solution to the dual program
      provides an upper bound on the maximum of the primal,
      and every feasible solution to the primal program
      provides a lower bound on the minimum of the dual.

      In fact the extremal values of dual programs
      (the maximum of the primal, and the minimum of the dual)
      coincide, although this will not be needed for the purposes of this
      report.
      This is referred to as \textit{Strong Duality} of linear programs.

  \section{The LP Bound}

    \begin{defn}
      The \textsc{inner distribution} is TODO.
      I might also put this in the section on association schemes;
      I'm not sure if it belongs better there or here.
    \end{defn}

    \begin{thm}[Delsarte Thm 3.3 \cite{delsarte}]\label{lp-ineq}
      For any inner distribution $y$,
      $$
        Q^T y \geq 0
      $$
      where $Q$ is the matrix of dual eigenvalues.
      (Here, $x \geq 0$ means that each component of
      the vector $x$ is not less than $0$.)
    \end{thm}

    \begin{proof}
      TODO.
      Note that this will require a number of lemmas which I've omitted here for
      brevity, but will include in the final product.
    \end{proof}

    This theorem provides the key inequality that will allow the application of
    linear programming to cliques in association schemes.
    However, because the constraint vector in a primal linear program
    becomes the objective in the dual program,
    this inequality will require some transformation to make it suitable for use
    in linear programming.

    Let $Y$ be an $M$-clique with inner distribution $y$.
    Then $y_i = 0$ for all $i \not\in M$,
    so $Q^T y \geq 0 \iff Q^T \diagM y \geq 0$
    since the action of $\diagM$ acting on the left
    is to zero out the \textit{rows} of $y$
    with index not in $M$.
    Similarly,
    $$
      Q^T \diagM y
      = Q(0)^T y_0 + Q^T \diagMs y
      = \mu + Q^T \diagMs y
    $$
    since the action of $\diagMs$ on the right
    is to zero out the \textit{columns} of $Q^T$
    with index not in $M^*$,
    $y_0 = 1$, and
    $$
      Q^T =
      \begin{bmatrix}
        1 & 1 & \cdots & 1 \\
        \mu_1 & & & \\
        \vdots & & * & \\
        \mu_d & & & \\
      \end{bmatrix}
      \ .
    $$
    Finally, since $y \geq 0$, $Q_0^T y \geq 0$ adds no new constraint,
    so that under the non-negativity constraint
    $Q^T y \geq 0 \iff \diagNs Q^T y \geq 0$.

    Putting all this together,
    $Q^T y \geq 0 \iff \diagNs Q^T \diagMs y \geq -\mu$
    so that Delsarte's LP can be written in standard form:
    \begin{alignat}{2}
      & \max\buildset{
        \vone^T \diagM y
      }{
        Q^T \diagM y \geq 0,\
        y \geq 0,\
        y_0 = 1
      } \label{dlp-primal} \\
      =& \max\buildset{
        \chiMs^T y
      }{
        - \diagNs Q^T \diagMs y \leq \diagNs \mu,\
        y \geq 0
      } + 1 \label{dlp-primal-std}
      \ .
    \end{alignat}

    Taking the dual yields
    \begin{alignat}{2}
      & \min\buildset{
        \mu^T \diagNs z
      }{
        - \diagMs Q \diagNs z \geq \chiMs,\
        z \geq 0
      } + 1 \label{dlp-dual-std} \\
      =& \min\buildset{
        \mu^T z
      }{
        - \diagMs Q \diagNs z \geq \chiMs,\
        z \geq 0,\
        z_0 = 1
      }
      \ .
    \end{alignat}
    Therefore, if $z_0 = 1$ is required,
    recalling that $Q_0 = \vone$ and $\diagMs \vone = \chiMs$, then
    \begin{alignat*}{2}
      & \diagMs Q \diagN z \\
      =& \diagMs \( Q_0 z_0 + Q \diagNs z \) \\
      =& \chiMs + \diagMs Q \diagNs z \\
      \leq& 0
      \ .
    \end{alignat*}
    This equivalence recover's Delsarte's formulation of
    the dual linear program:
    \begin{equation}\label{dlp-dual}
      \min\buildset{
        \mu^T z
      }{
        \diagMs Q z \leq 0,\
        z \geq 0,\
        z_0 = 1
      }
      \ .
    \end{equation}

  \section{The Ratio Bound}
    We will use (\ref{dlp-dual}) frequently.

  \section{The Clique-Coclique Bound}

\chapter{Schrijver's SDP Bound}
  \section{The Terwilliger Algebra of the Hamming Scheme}

  \section{Semi-Definite Programming}

\chapter{Computation}
  I wasn't sure if I ought to mention anything about the code I've written for
  this project (or even if there's anything worth saying that won't be covered
  elsewhere in the report).

  Also, if there are some specific results that would be interesting to show,
  but do not fit naturally into other sections of the report, then perhaps they
  could go here as well.

\appendix

\chapter{Linear Algebra}
  \section{The Spectral Theorem}

  \section{Adjacency Matrices}
    Basic results about the spectra of adjacency matrices, which may be used
    elsewhere in the report.  E.g. the sum of eigenvalues with multiplicity, and
    consequences.

  \section{Positive Semi-Definite Matrices}
    Depending on which proof of the clique-coclique bound I use, and how much
    detail I go into Schrijver's SDP bound, I could make some comments about PSD
    matrices.

\chapter{Group Theory}
  \section{Group Actions}
    The material of this section comes primarily from \cite[Section~1.7;
    Chapter~4]{dummit-foote}.

    \begin{defn}[Group Action]\label{group-action}
      Given a group $G$ and a set $X$,
      \textsc{group action} is a homomorphism $G \to \Sym X$,
      where $\Sym X$ is the symmetric group on $X$.
    \end{defn}

    A group action $\phi: G \to \Sym X$ induces a product
    $X \times G \to X$ by mapping $(x, g) \mapsto \phi(g)(x)$.
    When the action is clear from context,
    this will be denoted $x \cdot g$, or simply $xg$.
    This is called a \textsc{right action},
    as $g$ \textit{acts on the right of} $x$
    (the corresponding notion of a \textsc{left action}
    can also be defined.)

    Conversely, given a product $X \times G \to X$,
    the same expression defines a map $G \to \Sym X$.
    If such a product satisfies
    \begin{alignat*}{2}
      &\forall x \in X\ x 1_G = x \\
      \text{and } &\forall x \in X\ \forall g, h \in G\
        (x g) h = x(hg)
    \end{alignat*}
    then the induced map $G \to \Sym X$ will be a homomorphism,
    so that these definitions are equivalent.

    (In \cite{dummit-foote}
    this is taken as the definition of a group action,
    and the homomorphism $G \to \Sym X$ is called its \textsc{permutation
    representation}.  It will be occasionally convenient to adopt each
    perspective.)

    \begin{defn}[Types of Group Actions]\label{group-action-types}
      If if a homomorphism $G \to \Sym X$ is injective,
      then the action is called \textsc{faithful}.
      Note that a group homomorphism is injective
      if and only if it has a trivial kernel.

      Given a group action $G \to \Sym X$,
      $g \in G$ is called \textsc{fixed point-free}
      if $\forall x \in X\ xg \neq x$.
      The group action itself is called \textsc{fixed point-free}
      (or just \textsc{free}) if all its nontrivial elements
      are fixed point-free.

      A group action $G \to \Sym X$ is called \textsc{transitive}
      if $\forall x, y \in X$ there exists some $g \in G$
      such that $xg = y$.

      A group action is called \textsc{regular} if it is simultaneously
      transitive and free.  (This terminology follows \cite{godsil}.)
    \end{defn}

    Note that if $X$ is a structure with automorphisms
    (such as a graph or group), $G$ is a subgroup of $\Aut X$,
    and $G$ acts in the natural way on $X$ (i.e. $xg = g(x)$),
    then this action is faithful.
    That is, $\Aut X \leq \Sym X$,
    so that this action is induced by the identity $G \into \Sym X$,
    which is clearly injective.

    \begin{lem}\label{faithful-transitive-abelian}
      If an abelian group $G$ acts faithfully and transitively on a set $X$,
      then the action is free, and thus also regular. \cite[Section 4.1,
      Exercise 3]{dummit-foote}
    \end{lem}

    \begin{proof}
      Let $g \in G$ be nontrivial, and $x \in X$.
      The goal is to prove that $xg \neq x$.

      Since $g$ is not the identity,
      there exists some $y \in X$ such that $z := yg \neq y$.
      Furthermore, since $G$ acts transitively on $X$,
      there exists some $h \in G$ such that $yh = x \iff y = xh^{-1}$.
      Then,
      \begin{alignat*}{3}
        xg &= (yh) g &\\
        &= y (gh) &\\
        &= y (hg) \quad&\text{ since $G$ is abelian}\\
        &= (yg) h &\\
        &= zh \ .&\\
      \end{alignat*}

      If $zh = x$ then, $z = xh^{-1} = y$,
      but by definition, $z = yg \neq y$,
      so $xg = zh \neq x$.
    \end{proof}

  \section{Character Theory}
    \begin{defn}[Characters]\label{character}
      Given a group $G$, a \textsc{character} of the group $G$
      is a homomorphism $G \to \C^\times$,
      the group of non-zero complex numbers under multiplication.
      Then $G^*$ will denote the set of characters of $G$.
      \cite[Chapter 8]{godsil}

      (For abelian groups, this corresponds to irreducible degree $1$ characters
      over $\C$ in \cite[Section 18.3]{dummit-foote}.)
    \end{defn}

    On $G^*$ a product of characters can be defined by setting
    $$
      \phi \psi: g \mapsto \phi(g) \psi(g)
    $$
    under which the character taking each $g \in G$ identically to $1$
    acts as identity.

    Furthermore, for any character $\psi \in G^*$, the map $g \mapsto
    \psi\(g^{-1}\)$ is a homomorphism since
    $$
      gh \longmapsto \psi\((gh)^{-1}\)
      = \psi\( h^{-1} g^{-1} \)
      = \psi\(h^{-1}\) \psi\(g^{-1}\)
      = \psi\(g^{-1}\) \psi\(h^{-1}\)
    $$
    and for any $g \in G$,
    $$
      \psi(g) \psi\(g^{-1}\)
      = \psi\(g^{-1}\) \psi(g)
      = \psi(1) = 1
    $$
    so that this homomorphism is an inverse for $\psi$.

    Therefore, $G^*$ forms a group under the above product of characters.
    \\

    For the remainder of this section (and the rest of this report),
    discussion of characters will be restricted
    to the case of finite abelian groups.
    Throughout this section, $G$ will denote
    a finite abelian group of order $n$.

    In this case, by Lagrange's theorem, $g^n = 1_G$ for every $g \in G$,
    and so for any character $\psi \in G^*$
    $$
      1 = \psi(1) = \psi\(g^n\) = \psi(g)^n
    $$
    -- that is, the image of each character is contained in
    the set of $n^\text{th}$ roots of unity.

    Since the inverse of a complex number with modulus $1$ is also its complex
    conjugate, looking at the inversion in $G^*$,
    $$
      \psi\(g^{-1}\) = \psi(g)^{-1} = \bar{\psi(g)}
    $$
    so that the inverse of $\psi \in G^*$ is $\bar{\psi}: g \mapsto
    \bar{\psi(g)}$.

    \begin{thm}\label{character-duality}
      For all finite abelian groups $G$,
      $$
        G \cong G^*
        \ .
      $$
    \end{thm}

    \begin{proof}
      TODO
    \end{proof}

    While by transitivity this shows that $G^{**} \cong G$,
    this can be seen more directly via the isomorphism
    $$
      g \mapsto \( \psi \mapsto \psi(g) \) \ .
    $$
    \\

    Given an ordering $G = \set{g_1, \ldots, g_n}$,
    the character $\psi \in G^*$ can be identified with the row vector
    $$
      \psi \rightsquigarrow
      \begin{bmatrix}
        \psi(g_1) & \cdots & \psi(g_n) \\
      \end{bmatrix}
      \ .
    $$
    With this identification, the product of characters becomes the entrywise
    product of vectors (the \textit{Schur product} of $n \times 1$ matrices),
    and the inverse of a character in $G^*$ is the entrywise inversion of the
    vector.

    Furthermore, given an ordering $G^* = \set{\psi_1, \ldots, \psi_n}$,
    the matrix whose rows consist of the characters of $G^*$ is called the
    \textsc{character table} of $G$:
    \begin{equation}\label{character-table}
      H =
      \begin{bmatrix}
        \horzbar & \psi_1 & \horzbar \\
                 & \vdots &          \\
        \horzbar & \psi_n & \horzbar \\
      \end{bmatrix}
      \ .
    \end{equation}
    Remarkably, this matrix turns out to be (almost) unitary.
    \\

    For any subset $C \subseteq G$, define
    $$
      \psi(C) := \sum_{g \in C} \psi(g)
      = \psi \chi_C
    $$
    where $\chi_C$ is the characteristic vector of $C$ in $G$
    (with the same ordering).
    So, given characters $\psi, \phi$, their inner product can be written
    $$
      \psi \phi^* = \sum_{g \in G} \psi(g) \bar{\phi}(g)
      = \sum_{g \in G} \(\psi \bar{\phi}\) (g)
      = (\psi \bar{\phi}) \chi_G
      = (\psi \bar{\phi}) (G)
      \ .
    $$
    (Note here that $\phi^*$ denotes the conjugate transpose of $\phi$,
    and $\chi_G$ is also the all-ones column vector $\vone$.)

    \begin{lem}\label{characters-orthogonal}
      For any $\psi \in G^*$
      $$
        \psi(G) =
        \begin{cases}
          \abs{G} & \text{ if $\psi$ is the identity of $G^*$} \\
          0 & \text{ else.} \\
        \end{cases}
      $$
    \end{lem}

    \begin{proof}
      For any $h \in G$, since $g \mapsto hg$ is an automorphism of $G$,
      $hG = G$, so that
      $$
        \psi(G) = \sum_{g \in G} \psi(g)
        = \sum_{g \in G} \psi(hg)
        = \sum_{g \in G} \psi(h) \psi(g)
        = \psi(h) \sum_{g \in G} \psi(g)
        = \psi(h) \psi(G)
      $$
      which implies that either $\psi(h) = 1$ or $\psi(G) = 0$.

      But this holds for arbitrary $h \in G$, so that either
      $$
        \forall h \in G\ \psi(h) = 1
        \implies \psi = 1_{G^*}
        \quad\text{and}\quad
        \psi(G) = \sum_{g \in G} 1 = \abs{G}
      $$
      or else
      $$
        \exists h \in G\ \psi(h) \neq 1
        \implies \psi(G) = 0
        \ .
      $$
    \end{proof}

    \begin{cor}\label{character-table-unitary}
      If $H$ is the character table of a finite abelian group $G$ of order $n$,
      then $H H^* = nI$, where $H^*$ is the conjugate transpose of $H$.
    \end{cor}

    \begin{proof}
      If $\psi, \phi$ are characters of $G$, and $\psi \neq \phi$,
      then letting $\theta = \psi \bar{\phi}$, the $(\psi, \phi)$-entry of $HH^*$
      is given by $\psi \phi^* = \theta(G) = 0$, since $\theta$ is not the
      identity of $G^*$.

      However, for the diagonal, $(\psi, \psi)$-entries of $HH^*$,
      $\psi \psi^* = 1_{G^*}(G) = n$, which proves the claim.
    \end{proof}

\chapter{Notation}
  I've included these sections mostly as an excuse to add the citations to the
  bibliography, though it may be somewhat useful to have a sort of guide to the
  similarities and differences in notation in this report, as well as in the
  references.  (At least, I would find such a thing useful.)

  \section{Godsil}
    See \cite{godsil} Ch. 10.

  \section{Delsarte}
    See \cite{delsarte} Ch. 3.

  \section{Schrijver}
    See \cite{schrijver}.

\printbibliography[heading=bibintoc]

\end{document}
